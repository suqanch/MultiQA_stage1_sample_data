# sample_data/latex_paper

- `parse_result/`  
  Extracted images and related data from `*.pdf`

- `merged.tex`  
  A LaTeX file generated by merging all `.tex` files in the path

- `output.txt`  
  Preprocessed text file used to generate QA

- `prompt_response.json`  
  Raw output returned by the LLM

- `processed_response.json`  
  Processed version of `prompt_response.json`, containing extracted QA pairs


# Preprocessing

1. Merge all .tex files. Remove all comments (% ...).
	
2. \ref{fig:xxx} -> [Ref id="fig: xxx"].

3. Tables
- Add [Table] at the beginning.
- Keep header row -> [TableHeader] ...
- Convert \caption{...} -> [Caption] ...
- Convert \label{tab:xxx} -> [Label id="tab:xxx"]
- Remove table body content.
	
4. Images
- Convert \includegraphics{figure/xxx.pdf} -> [Graphic src="figure/xxx.pdf"]
- Convert \caption{...} -> [Caption] ...
- Convert \label{fig:xxx} -> [Label id="fig:xxx"]



# Analysis (Rouge_L_evidence2.py)

#### ROUGE-L score calculation

- Goal: For each evidence section, compare its content with the corresponding original document section and compute ROUGE-L.

#### Split original document (tex_analysis.py)

Input original tex(merged.tex) and get:
{section: contont}


#### Fuzzy matching of evidence section names

- Target categories:
  1. Introduction/Background
  2. Proposed Method
  3. Experimental Results
  4. Conclusion
  5. Others

- Anchors:
```python
ANCHOR_SECTIONS = {
    "intro_background": ["abstract", "introduction", "related work", "background", "preliminaries", "literature review"],
    "experiments":       ["experiment", "evaluation", "results", "analysis", "experiment & analysis"],
    "conclusion":        ["conclusion", "discussion", "summary", "closing remarks", "limitations"],
}
```

- Algorithm:
  - Step 1: Locate the end of Introduction/Background (`intro_end`), the start of Experiments (`exp_start`), and the start of Conclusion (`concl_start`) using fuzzy title matching against the anchor lists.
  - Step 2: For each section `i` in document order, assign a category:
    - `i <= intro_end`: Ignore (belongs to Intro/Background).
    - `intro_end < i < exp_start`: Proposed Method.
    - `exp_start <= i < concl_start`: Ignore (belongs to Experiments).
    - `i >= concl_start`: Ignore (belongs to Conclusion).
    - Otherwise: Others.

#### Analysis to conduct

- Target category counting.
- Misclassification count: number of sections where predicted â‰  true category.
- ROUGE-L < 0.95 count: number of evidences with score below 0.95.
- Intent counting.
- Max/min average evidence length per question.
- Evidence count statistics: max/min/avg number of evidences per question.


#### Output file

```json
[
  {
    "question": "Summarize the full ChuLo pipeline:....",
    "intent": [
        "Descriptive",
        "Procedural"
    ],
    "num_sections": 2,
    "avg_evidence_len": 743.5,
    "evidences": [
        {
            "section_name": "related work",
            "true_matched_section": "related work",
            "true_category": "Introduction or background",
            "content": "The Overall ChuLo Framework proposed...",
            "gold_paragraph": "\u00a7 RELATED WORK \u00a7.\u00a7 ...",
            "max_rougeL_score": 1.0,
            "evidence_length": 144
    },]
  }
]
```