[
    {
        "question": "Summarize the full ChuLo pipeline: how documents are chunked, how keyphrases are extracted and prioritized, how chunk embeddings are computed (including the weighting of keyphrase vs. non-keyphrase tokens), and how the chunk attention module is trained. Use the framework diagram and related descriptions.",
        "intent": [
            "Descriptive",
            "Procedural"
        ],
        "num_sections": 2,
        "avg_evidence_len": 743.5,
        "evidences": [
            {
                "section_name": "related work",
                "fuzzy_matched_section": "related work",
                "true_category": "Introduction or background",
                "content": "The Overall ChuLo Framework proposed in this paper. Each chunk is surrounded by a pink box. $C_1$ ... $C_n$ represents the chunk representation.",
                "gold_paragraph": "\u00a7 RELATED WORK \u00a7.\u00a7 Long Document UnderstandingDocument understanding involves global understanding (e.g., classification) and token-level tasks (e.g., named entity recognition). Transformer-based models face performance issues with long inputs, addressed through input processing and architecture optimization. Input processing methods include truncating tokens beyond the input limit <cit.> and chunking, as seen in Hierarchical Transformer <cit.> and RoR <cit.>, though these often neglect full document context. Architecture optimizations improve efficiency using sparse attention <cit.> or approximations <cit.>. Other approaches incorporate RNN concepts, such as cache memory <cit.>. These methods balance performance and efficiency, highlighting the need to reduce input length effectively.For document NER, text length is less studied, with recent work addressing low-resource languages <cit.>, complex domains <cit.>, prompt-based methods <cit.>, and multimodal data <cit.>. Our work tackles these challenges[The summary of Long Document Understanding related works can be found in Appendix [Ref id=\"app:related_works\"]] with a novel chunk representation that preserves semantic information while reducing input length, enhancing both classification and token-level tasks. \u00a7.\u00a7 Unsupervised Keyphrase ExtractionUnsupervised keyphrase extraction identifies representative phrases to summarize content without labelled data <cit.>. Methods include statistics-based (e.g., TfIdf <cit.>, co-occurrence <cit.>, and context statistics <cit.>), graph-based (e.g., TextRank <cit.> and its variants <cit.>), and embedding-based approaches (e.g., EmbedRank <cit.>, SIFRank <cit.>, and PromptRank <cit.>). While effective, these methods prioritize phrase extraction and ranking over improving downstream tasks. Our work integrates keyphrase extraction with chunk representation to enhance long document understanding.[Graphic src=\"images/framework_2.png\"][Caption]{The Overall ChuLo Framework proposed in this paper. Each chunk is surrounded by a pink box. $C_1$ ... $C_n$ represents the chunk representation.}[Label id=\"fig:framework\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 144
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "We propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 0.9901960784313726,
                "evidence_length": 786
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences.",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 1.0,
                "evidence_length": 875
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints.",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 1.0,
                "evidence_length": 1514
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 1.0,
                "evidence_length": 470
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 1.0,
                "evidence_length": 672
            }
        ]
    },
    {
        "question": "How does ChuLo compare with Longformer, BERT variants, CogLTX, and LLMs (GPT-4o, Gemini 1.5 Pro) in document classification across datasets and especially for longer documents (>1024 and >2048 tokens)? Summarize using the results tables and accompanying analysis.",
        "intent": [
            "Comparative",
            "Evaluative",
            "Causal"
        ],
        "num_sections": 2,
        "avg_evidence_len": 869.0,
        "evidences": [
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "We evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 0.9919354838709677,
                "evidence_length": 819
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 583
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 557
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 982
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 352
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 1126
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 1664
            }
        ]
    },
    {
        "question": "What are the NER (token classification) results of ChuLo versus Longformer, BigBird, GPT-4o, and Gemini 1.5 Pro, and how does performance change with document length? Include the architectural note on how ChuLo predicts token labels and use the length-interval tables/figures.",
        "intent": [
            "Comparative",
            "Evaluative",
            "Descriptive",
            "Causal"
        ],
        "num_sections": 2,
        "avg_evidence_len": 538.125,
        "evidences": [
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 137
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "To further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 395
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 634
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 359
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 951
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 738
            },
            {
                "section_name": "experiments setup",
                "fuzzy_matched_section": "experiments setup",
                "true_category": "Experimental Results",
                "content": "2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model\u2019s ability to process extended text sequences for token classification tasks.",
                "gold_paragraph": "\u00a7 EXPERIMENTS SET-UPWe evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[<https://openai.com/index/hello-gpt-4o/>] and Gemini1.5pro[<https://deepmind.google/technologies/gemini/pro/>]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"].Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"]. 1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the \u2018byarticle\u2019 version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted.2) LUN uses for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxes <cit.>. Our goal is to predict the source type for each article.3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information.For token classification, we use GUM and CoNLL-2012 for Named Entity Recognition (NER) tasks:1) GUM (Georgetown University Multilayer) is a richly annotated collection of 235 documents across various genres such as academic texts, news, fiction, and interviews <cit.>. GUM\u2019s various linguistic styles and structures make it an excellent benchmark for assessing token-level understanding in lengthy documents, ensuring that the model captures complex entity relationships over extended contexts.2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model\u2019s ability to process extended text sequences for token classification tasks.Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison.Regarding implementation, we provide key details here, with the complete setup in Appendix [Ref id=\"app:imp. details\"]. We use CrossEntropy loss for training on the Hyperpartisan, LUN, CoNLL and GUM datasets, and Binary CrossEntropy loss for the Eurlex57k and Inverted Eurlex57k datasets. All models are optimized using the AdamW optimizer, and training employs early stopping based on the respective validation metric, with a patience threshold set to 10 epochs. A learning rate search is conducted for each experiment to ensure optimal model performance for comparison. Top-n value is set to 15[We tested with different n values, but 15 was generally better in most datasets].",
                "max_rougeL_score": 1.0,
                "evidence_length": 304
            },
            {
                "section_name": "results",
                "fuzzy_matched_section": "results",
                "true_category": "Experimental Results",
                "content": "Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.",
                "gold_paragraph": "\u00a7 RESULTS \u00a7.\u00a7 Document ClassificationWe evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples. [Table][TableHeader] {height 0.8pt}[Caption]{Document classification Result. Following previous work, we use accuracy for HP and LUN, and micro F1 for other datasets. Results for LUN are obtained by our own experiment based on provided baseline codes and methods, while baseline results for the other datasets are from previous work<cit.>. $^\u2020$ are the BERT variants proposed by <cit.>. The best performance for each dataset is bolded while the second best is underscored, and we can see that our final model, a BERT-based backbone, generally outperforms other baselines across all datasets by achieving the best or second-best.}[Label id=\"tab:overal_performance_comparison\"][Table][TableHeader] {height 0.8pt}[Caption]{The usage of the input content in the experiments.\u201cF-512\u201c and \u201cF-4096\u201c means the first 512 tokens and the first 4096 tokens, \u201cS-512\u201c means the selected 512 tokens.}[Label id=\"tab:usage_of_input\"]Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted. Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges. \u00a7.\u00a7 Longer Document ClassificationTo further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set. We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation\u2019s emphasis on keyphrases, which preserves crucial semantic content even when document length increases.On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs.[Table][TableHeader] {height 0.8pt}    [Caption]{LUN dataset} [TableHeader] {height 0.8pt}    [Caption]{HP dataset}[Caption]{Document classification results for comparison on documents of different lengths: all documents in the test set, the subset of documents longer than 1024 tokens, and longer than 2048 tokens respectively. Values in brackets indicate the number of documents for each specific document set. The best performance (Accuracy) for each document set is bolded.}[Label id=\"tab:Accuracy length intervals\"]We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo\u2019s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths.[Table][TableHeader] {height 0.8pt}[Caption]{Results on token classification tasks. The best performance for each dataset is bolded, and our model achieves the best on both datasets.}[Label id=\"tab:overal_performance_token_cls_comparison\"] \u00a7.\u00a7 Token ClassificationTo further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task\u2014specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method\u2019s ability to encode the entire document\u2019s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels.Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, validating its application in downstream tasks that require a detailed and comprehensive understanding of long document tokens.[Table]    [TableHeader] {height 0.8pt}    [Caption]{Results on CoNLL dataset.}    [Label id=\"tab:Conll Microf1 length intervals\"]    [TableHeader] {height 0.8pt}    [Caption]{Results on GUM dataset.    }    [Label id=\"tab:Gum Microf1 length intervals\"]    [Caption]{NER results for comparison on documents of different lengths. >number represents the documents longer than the number, with the values in brackets indicating the corresponding counts for the documents. The best performance (Micro F1) is bolded and the second best is underscored, and our model consistently outperforms all the baselines for each document set.}    [b]{0.47}           [Graphic src=\"images/output_conll.png\"]         [Caption]{CoNLL Performance (Range: 1798 to 9778)}        [Label id=\"fig:conll_results\"]        [b]{0.47}          [Graphic src=\"images/output_gum.png\"]         [Caption]{GUM Performance (Range: 628 to 1281)}        [Label id=\"fig:gum_results\"]        [Caption]{Comparison of performance in different length ranges for CoNLL and GUM datasets. Values of brackets includes the min and max length of each dataset.}    [Label id=\"fig:combined_results\"]     \u00a7.\u00a7 Token Classification in Longer DocumentsWe further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model\u2019s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach\u2019s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines.These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks\u2014a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding.  \u00a7.\u00a7 Prompt Method[Label id=\"sec:prompt\"]We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:[Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_doc_c\"][Table][TableHeader] {height 0.8pt}[Caption]{The prompt we used for each dataset in our experiments.}[Label id=\"tab:app_prompts_token_c\"]Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, \u201cO\u201d. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all \u201cO\u201d labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area.    [b]{1.0}           [Graphic src=\"images/case9_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case9_ans.png\"]        [Caption]{Prompt and output for a sample document of length 3038 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case9\"] \u00a7.\u00a7 Ablation StudiesWe conducted more ablation studies, including 1) keyphrase extraction methods, 2) sentence embedding approaches, and 3) backbone model analysis, shown in Appendix [Ref id=\"app:ablation study\"]. \u00a7.\u00a7 Qualitative AnalysisWe performed a qualitative analysis by visualizing each sample document from different datasets, comparing the outputs of Longformer, GPT-4o, Gemini 1.5 Pro, and our ChuLo. ChuLo captures the context and semantic patterns of the document, providing accurate predictions, whereas the other models struggle to maintain coherence and consistency. We have more examples in Appendix [Ref id=\"app:cases\"].",
                "max_rougeL_score": 1.0,
                "evidence_length": 787
            }
        ]
    },
    {
        "question": "What experimental setup did the authors use (tasks, datasets, metrics, training losses/optimizers, early stopping), and what dataset statistics and hardware were reported? Summarize across the setup text and appendix tables.",
        "intent": [
            "Descriptive"
        ],
        "num_sections": 2,
        "avg_evidence_len": 639.1666666666666,
        "evidences": [
            {
                "section_name": "experiments setup",
                "fuzzy_matched_section": "experiments setup",
                "true_category": "Experimental Results",
                "content": "We evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[https://openai.com/index/hello-gpt-4o/] and Gemini1.5pro[https://deepmind.google/technologies/gemini/pro/]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"].",
                "gold_paragraph": "\u00a7 EXPERIMENTS SET-UPWe evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[<https://openai.com/index/hello-gpt-4o/>] and Gemini1.5pro[<https://deepmind.google/technologies/gemini/pro/>]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"].Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"]. 1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the \u2018byarticle\u2019 version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted.2) LUN uses for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxes <cit.>. Our goal is to predict the source type for each article.3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information.For token classification, we use GUM and CoNLL-2012 for Named Entity Recognition (NER) tasks:1) GUM (Georgetown University Multilayer) is a richly annotated collection of 235 documents across various genres such as academic texts, news, fiction, and interviews <cit.>. GUM\u2019s various linguistic styles and structures make it an excellent benchmark for assessing token-level understanding in lengthy documents, ensuring that the model captures complex entity relationships over extended contexts.2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model\u2019s ability to process extended text sequences for token classification tasks.Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison.Regarding implementation, we provide key details here, with the complete setup in Appendix [Ref id=\"app:imp. details\"]. We use CrossEntropy loss for training on the Hyperpartisan, LUN, CoNLL and GUM datasets, and Binary CrossEntropy loss for the Eurlex57k and Inverted Eurlex57k datasets. All models are optimized using the AdamW optimizer, and training employs early stopping based on the respective validation metric, with a patience threshold set to 10 epochs. A learning rate search is conducted for each experiment to ensure optimal model performance for comparison. Top-n value is set to 15[We tested with different n values, but 15 was generally better in most datasets].",
                "max_rougeL_score": 0.9888888888888889,
                "evidence_length": 640
            },
            {
                "section_name": "experiments setup",
                "fuzzy_matched_section": "experiments setup",
                "true_category": "Experimental Results",
                "content": "Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"].",
                "gold_paragraph": "\u00a7 EXPERIMENTS SET-UPWe evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[<https://openai.com/index/hello-gpt-4o/>] and Gemini1.5pro[<https://deepmind.google/technologies/gemini/pro/>]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"].Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"]. 1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the \u2018byarticle\u2019 version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted.2) LUN uses for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxes <cit.>. Our goal is to predict the source type for each article.3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information.For token classification, we use GUM and CoNLL-2012 for Named Entity Recognition (NER) tasks:1) GUM (Georgetown University Multilayer) is a richly annotated collection of 235 documents across various genres such as academic texts, news, fiction, and interviews <cit.>. GUM\u2019s various linguistic styles and structures make it an excellent benchmark for assessing token-level understanding in lengthy documents, ensuring that the model captures complex entity relationships over extended contexts.2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model\u2019s ability to process extended text sequences for token classification tasks.Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison.Regarding implementation, we provide key details here, with the complete setup in Appendix [Ref id=\"app:imp. details\"]. We use CrossEntropy loss for training on the Hyperpartisan, LUN, CoNLL and GUM datasets, and Binary CrossEntropy loss for the Eurlex57k and Inverted Eurlex57k datasets. All models are optimized using the AdamW optimizer, and training employs early stopping based on the respective validation metric, with a patience threshold set to 10 epochs. A learning rate search is conducted for each experiment to ensure optimal model performance for comparison. Top-n value is set to 15[We tested with different n values, but 15 was generally better in most datasets].",
                "max_rougeL_score": 1.0,
                "evidence_length": 498
            },
            {
                "section_name": "experiments setup",
                "fuzzy_matched_section": "experiments setup",
                "true_category": "Experimental Results",
                "content": "Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison.Regarding implementation, we provide key details here, with the complete setup in Appendix [Ref id=\"app:imp. details\"]. We use CrossEntropy loss for training on the Hyperpartisan, LUN, CoNLL and GUM datasets, and Binary CrossEntropy loss for the Eurlex57k and Inverted Eurlex57k datasets. All models are optimized using the AdamW optimizer, and training employs early stopping based on the respective validation metric, with a patience threshold set to 10 epochs. A learning rate search is conducted for each experiment to ensure optimal model performance for comparison. Top-n value is set to 15[We tested with different n values, but 15 was generally better in most datasets].",
                "gold_paragraph": "\u00a7 EXPERIMENTS SET-UPWe evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[<https://openai.com/index/hello-gpt-4o/>] and Gemini1.5pro[<https://deepmind.google/technologies/gemini/pro/>]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"].Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"]. 1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the \u2018byarticle\u2019 version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted.2) LUN uses for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxes <cit.>. Our goal is to predict the source type for each article.3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information.For token classification, we use GUM and CoNLL-2012 for Named Entity Recognition (NER) tasks:1) GUM (Georgetown University Multilayer) is a richly annotated collection of 235 documents across various genres such as academic texts, news, fiction, and interviews <cit.>. GUM\u2019s various linguistic styles and structures make it an excellent benchmark for assessing token-level understanding in lengthy documents, ensuring that the model captures complex entity relationships over extended contexts.2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model\u2019s ability to process extended text sequences for token classification tasks.Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison.Regarding implementation, we provide key details here, with the complete setup in Appendix [Ref id=\"app:imp. details\"]. We use CrossEntropy loss for training on the Hyperpartisan, LUN, CoNLL and GUM datasets, and Binary CrossEntropy loss for the Eurlex57k and Inverted Eurlex57k datasets. All models are optimized using the AdamW optimizer, and training employs early stopping based on the respective validation metric, with a patience threshold set to 10 epochs. A learning rate search is conducted for each experiment to ensure optimal model performance for comparison. Top-n value is set to 15[We tested with different n values, but 15 was generally better in most datasets].",
                "max_rougeL_score": 1.0,
                "evidence_length": 957
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples.",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 1302
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"].",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 284
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "Our experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G.",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 154
            }
        ]
    },
    {
        "question": "What limitations and risks do the authors acknowledge for ChuLo, and which ablation results support their design choices (keyphrase method, backbone selection)?",
        "intent": [
            "Evaluative",
            "Verificative",
            "Causal"
        ],
        "num_sections": 3,
        "avg_evidence_len": 288.5,
        "evidences": [
            {
                "section_name": "limitation",
                "fuzzy_matched_section": "limitation",
                "true_category": "Conclusion",
                "content": "There are several opportunities for future work, including extending the chunk representation to generative tasks such as long text generation, where chunk representation may extend the LLM's context range limitation and enhance generation quality. However, the performance of the keyphrase extraction method poses a potential risk, as its quality directly affects the overall effectiveness of the approach. We believe this work offers valuable insights into long text understanding and lays a foundation for advancements in related tasks.",
                "gold_paragraph": "\u00a7 LIMITATIONThere are several opportunities for future work, including extending the chunk representation to generative tasks such as long text generation, where chunk representation may extend the LLM's context range limitation and enhance generation quality. However, the performance of the keyphrase extraction method poses a potential risk, as its quality directly affects the overall effectiveness of the approach. We believe this work offers valuable insights into long text understanding and lays a foundation for advancements in related tasks.",
                "max_rougeL_score": 0.9871794871794872,
                "evidence_length": 539
            },
            {
                "section_name": "chulo",
                "fuzzy_matched_section": "chulo",
                "true_category": "Proposed Method",
                "content": "In this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.",
                "gold_paragraph": "\u00a7 CHULOWe propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]. \u00a7.\u00a7 Document Input ChunkingTo effectively manage long document inputs, we employ a chunking strategy that reduces input length while preserving all relevant information.Common approaches to long document processing, such as truncation and sparse attention, either disregard parts of the document<cit.> or restrict the receptive field of individual tokens <cit.>, resulting in potential information loss.Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, \u2026, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, \u2026, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= \u2308{l_D/n}\u2309$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences. \u00a7.\u00a7 Semantic Key Information ExtractionThe fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding.Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer\u2019s attention mechanism to prioritize the most informative parts of the text, enhancing the model\u2019s ability to capture the document\u2019s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding.To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document\u2019s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints. \u00a7.\u00a7 Chunk Representation ProductionAfter extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing.To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:\\begin{equation}w_t =         a, t is $T_k$b, t is $T_{nk}$c = \u2211{w_t*t}/\u2211{w_t}[Label id=\"equ:chunkemb\"]\\end{equation}Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$$t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks. \u00a7.\u00a7 Chunk Representation TrainingIn this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context.We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure\u00a0[Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions.Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing.",
                "max_rougeL_score": 1.0,
                "evidence_length": 477
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "Effect of keyphrase extraction methods; Average: Average Chunk Representations",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 78
            },
            {
                "section_name": "appendix",
                "fuzzy_matched_section": "appendix",
                "true_category": "Others",
                "content": "Effect of different backbone models for the chunk attention.",
                "gold_paragraph": "\u00a7 APPENDIX \u00a7.\u00a7 Related Works[Label id=\"app:related_works\"]As shown in Table\u00a0[Ref id=\"tab:related_work\"], most of the previous works addressing the problem of processing long documents cannot fully utilize all the content. Those methods either reduce input length via truncation or focus on local context learning to improve efficiency by applying sparse attention, approximated attention or RNN integration. Such approaches will lead to a certain level of information loss, unlike our chunking approach which can take all the content into consideration.Hierarchical Transformer <cit.> splits documents into non-overlapping chunks and computes chunk representations. RoR <cit.> generates regional answers from chunks, which are combined for the final answer. However, neither considers the entire document context when chunking.In addition, previous works applying the chunking method for processing long document context only focus on a single task, either document classification or token classification, while our framework can be applied to both tasks to guarantee both document-level and token-level understanding. \u00a7.\u00a7 Keyphrase Extraction[Label id=\"app:algorithm\"]We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"].While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document.[htb]    [Caption]{Semantic Keyphrase Prioritization (SKP) Algorithm}    [Label id=\"alg:algorithm\"]    2        Input: A tokenized document $D$, an encoder-decoder pretrained model represented by $\u2131_E$ and $\u2131_D$, a POS tagger $\u2131_{POS}$, a regular expression $\u2131_{REG} = \u27e8NN.\u2217 |JJ\u27e9\u2217\u27e8NN.\u2217\u27e9$    Parameter: Experimentally determined $\u03b1$, $\u03b3$      Output: Sorted keyphrases set $K_s$        [1]             Let $S=\u2205$, $K_s=\u2205$, $i=0$, $j=0$.            Get the candidate phrases set:  $K=\u2131_{REG}(\u2131_{POS}(D))={k_0, k_1, \u2026, k_{n-1}}$            Split $D$ into segments $S={D_0, D_1, \u2026, D_{m-1}}$ to meet the input requirement of $\u2131_E$            { $i < n$}            Calculate the position penalty $r_i=L_c/l_d + \u03b3/(l_d)^3$            where $L_c$ is the first occurrence position of $k_i$ in the document, $l_d$ is the length of the document             Construct the prompt $P$ \u201cThe * mainly discusses $k_i$\u201d and tokenize, * is the category of the document.            { $j < m$}            Calculate the probability $p_{ij}$ of the phrase $k_i$:             $p_{ij} = 1/(l_P)^\u03b1\u2211_{g=h}^{h+l_k-1}log p(t_g | t_{<g})$,             $p(t_g | t_{<g})=\u2131_D(\u2131_E(D_j), t_{<g})$             where $l_P$ is the length of the tokenized $P$, $h$ is the start index of $k_i$ in the prompt, $l_k$ is the length of $k_i$, $t$ is the token of the prompt.            Calculate the final score of $k_i$: $s_i=r_i\u00d7\u2211_{j=0}^{j<m}p_{ij}$            return $K_s=Sort(K, s)$                [Table]        [TableHeader] Model     Year     Task     Lengthy Document Solution     Core Architecture    [Caption]{Summary of Related Works. D, T, G represent tasks of document classification, token classification, and text generation, respectively. }    [Label id=\"tab:related_work\"] \u00a7.\u00a7 Baselines[Label id=\"app:baseline models\"]We use BERT <cit.> as our backbone model, comparing it with ToBERT <cit.>, CogLTX <cit.>, Longformer <cit.>, various BERT variants <cit.> and ChunkBERT <cit.> for the document classification task. For the NER task, we compare against Longformer, BigBird <cit.>, and two large language models, GPT4o and Gemini1.5pro. Below are brief descriptions of the baseline models:   * BERT: A transformer model pre-trained on masked language modeling (MLM) and next-sentence prediction (NSP). We fine-tuned the BERT-base variant on each dataset.    * ToBERT: A BERT variant designed for long document classification, utilizing an additional transformer layer to learn inter-chunk relationships.   * CogLTX: A framework for applying BERT to long documents by training a key sentence identification model to assist in document understanding   * Longformer: Optimized for long sequences using sparse attention, combining dilated sliding window and global attention patterns   * BigBird: Utilizes block sparse attention, integrating sliding window, global, and random attention patterns across token blocks.   * BERT+TextRank and BERT+Random: Proposed to select other tokens randomly or with the help of TextRank<cit.> to feed into the BERT model as the supplementation for long document classification.       * ChunkBERT: A BERT variant for long document classification that processes self-attention within document chunks and adds a TextCNN module for classification using the chunk representation.   * GPT-4o: A transformer-based multi-modal large language model developed by OpenAI, which leverages large-scale pretraining data to process diverse language tasks via instruction prompts.   * Gemini 1.5 Pro: an advanced multi-modal AI model from Google, leveraging a Sparse Mixture-of-Experts (MoE) Transformer architecture, with a context window of up to 2 million tokens. This architecture allows for the efficient handling of long documents.  \u00a7.\u00a7 Baseline Input[Label id=\"app: baseline input\"]We selected these baseline models because they represent diverse methods for processing long documents. As summarized in Table [Ref id=\"tab:baseline_input\"], BERT truncates the input to 512 tokens. Longformer and BigBird utilize sparse attention mechanisms, allowing them to process up to 4096 tokens while conserving computational resources. ToBERT divides the input into 200-token chunks, feeds them to BERT for chunk representations, and uses a transformer layer for downstream tasks. However, it cannot capture dependencies across the entire input sequence. CogLTX selects key sentences to form a 512-token input, limiting its input size to that constraint. BERT variants like BERT+TextRank and BERT+Random select up to 512 tokens using TextRank or random selection. They concatenate the [CLS] representation of the initial 512 tokens with the selected tokens, creating an augmented input for a fully connected classification layer, with a maximum input length of 1024 tokens. ChunkBERT splits the input into chunks, computes self-attention, and feeds chunk representations into a TextCNN module for classification. The original implementation processes up to 4096 tokens per document. It has the same limitation as the ToBERT. For GPT4o and Gemini1.5pro, we input all tokens together with our instruction in the prompt due to the large input size supported by these large language model. In contrast to these baseline models, our approach flexibly segments the entire input into chunks of varying sizes, using semantic keyphrases to minimize information loss. Additionally, we compute chunk-level attention to capture long-range dependencies more effectively.[Table][TableHeader] {height 0.8pt}[Caption]{The input of the baseline models}[Label id=\"tab:baseline_input\"] \u00a7.\u00a7 Details of datasets[Label id=\"sec:Appendix:Dataset statistics\"][Table][TableHeader] {height 0.8pt}[Caption]{The split and statistics of the datasets, including document classification task (HP, LUN, EURLEX57K, and Inverted EURLEX57K) and token classification task (GUM, CoNLL)}[Label id=\"datasettable\"]We analyzed the data distribution across the datasets used in this paper. Of these, the CoNLL dataset has the highest average number of tokens per document at 5,065. In contrast, LUN has the shortest average length, with 480 tokens per document. Both HP and EURLEX57K have similar average document lengths, measuring 705 and 707 tokens respectively. GUM presents a relatively higher token count, averaging 972 tokens per document.Regarding the number of classes, EURLEX57K is the most complex dataset, containing 4,271 unique labels. In comparison, HP and LUN are more limited, with only 2 and 3 classes respectively. GUM and CoNLL are more diverse, with 21 and 37 different classes. Beyond label diversity, EURLEX57K also has the largest sample size, comprising 45,000 training samples, 6,000 development samples, and 6,000 test samples. LUN is the second-largest dataset, with 12,003 training samples, 2,992 development samples, and 2,250 test samples. Due to our selection strategy, CoNLL has the longest average document length, with the fewest samples. It has a total of 160 documents split into 120/20/20 for training, development, and test sets. GUM follows a similar distribution with 179/26/26 samples. The HP dataset includes 516 training samples, 64 development samples, and 65 test samples. \u00a7.\u00a7 Implementation details[Label id=\"app:imp. details\"]  \u00a7.\u00a7.\u00a7 Experiment hyperparameters We performed extensive experiments to select the hyperparameters, including chunk size, token weights, learning rates, and warm-up strategies and steps. The optimal hyperparameters for each dataset for our proposed ChuLo model are presented in Table [Ref id=\"tap:app_hp_imp_details\"]. [Table][TableHeader] {height 0.8pt}    [Caption]{The optimal hyperparameters used in our experiments.}    [Label id=\"tap:app_hp_imp_details\"]  \u00a7.\u00a7.\u00a7 Hardware InformationOur experiments are run on the Linux platform with an A6000 Nvidia graphic card and an AMD Ryzen Threadripper PRO 5955WX 16-core CPU, and the RAM is 128G. \u00a7.\u00a7 Ablation Studies[Label id=\"app:ablation study\"]We performed a few ablation studies on the HP and LUN  to assess the impact of various components within our model. First, we analyzed the effectiveness of different keyphrase extraction methods and the effect of using average chunk representations. As shown in Table [Ref id=\"tab:keyphrase ablation\"], the PromptRank-based method yields the highest performance across both datasets, outperforming alternatives like YAKE-based. This improvement can be attributed to PromptRank\u2019s ability to extract higher-quality keyphrases by considering semantic relationships within the document, whereas YAKE relies primarily on statistical features such as phrase frequency, resulting in less semantically rich keyphrases. Then, we explored the effect of incorporating sentence embeddings into the chunk representations to introduce global sentence-level context. Surprisingly, as shown in Table [Ref id=\"tab:sent emb ablation\"], the results indicate a performance drop when sentence embeddings are included. We hypothesize that adding sentence-level information at the initial representation stage may cause chunk embeddings within the same sentence to become too similar, hindering the model\u2019s ability to learn distinctive patterns and reducing overall classification performance.Then, we evaluated the performance of different backbone models for the chunk attention module while keeping the keyphrase extraction and chunk representation settings consistent. Table [Ref id=\"tab:backbone ablation\"] shows that BERT outperforms Longformer as the backbone. This result suggests that, after document chunking, the input sequences become relatively short, making it difficult for Longformer to leverage its long-range attention capabilities fully. Consequently, Longformer may suffer underutilization during training, resulting in suboptimal performance compared to BERT. [Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of keyphrase extraction methods; Average: Average Chunk Representations}    [Label id=\"tab:keyphrase ablation\"]    [Table]        [TableHeader] {height 0.8pt}    [Caption]{Effect of sentence embedding, adding the sentence-level information to the chunk representations.}    [Label id=\"tab:sent emb ablation\"][Table]            [TableHeader] {height 0.8pt}    [Caption]{Effect of different backbone models for the chunk attention. }    [Label id=\"tab:backbone ablation\"]                 \u00a7.\u00a7 More Case Studies[Label id=\"app:cases\"]In this section, we will present several prompt and output samples for the long documents from the LUN (Figures\u00a0[Ref id=\"fig:case1\"]) and \u00a0[Ref id=\"fig:case2\"]) and Hyperpartisan (Figures\u00a0[Ref id=\"fig:case3\"] and \u00a0[Ref id=\"fig:case4\"]) datasets for document classification, as well as GUM (Figures\u00a0[Ref id=\"case5\"], [Ref id=\"case6\"] and \u00a0[Ref id=\"case7\"]) and CoNLL (Figures\u00a0[Ref id=\"case8\"], \u00a0[Ref id=\"case9\"], \u00a0[Ref id=\"case10\"] and \u00a0[Ref id=\"case11\"]) datasets for NER task. Documents with various lengths are randomly selected to see the comparison of our model against GPT-4, Gemini1.5pro and Longformer. While there is always at least one baseline which predicts wrongly for the difficult cases presented for the document classification task, we can observe that our model consistently classifies those documents well. For the token classification task, our model can also correctly classify more tokens than each baseline across the shown cases.        [Graphic src=\"images/case1.png\"]    [Caption]{Prompt and output for a sample document of length 3928 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Hoax.}    [Label id=\"fig:case1\"]    [Graphic src=\"images/case2.png\"]    [Caption]{Prompt and output for a sample document of length 2410 in LUN dataset, where the correct prediction is highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o, Gemini1.5pro and Longformer, our model can correctly classify the given document as Propaganda.}    [Label id=\"fig:case2\"]        [Graphic src=\"images/case3.png\"]    [Caption]{Prompt and output for a sample document of length 6800 in Hyperpartisan dataset, where correct predictions are highlighted in green and the wrong prediction is highlighted in red. Compared to Gemini1.5pro, our model, GPT4o and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case3\"]    [Graphic src=\"images/case4.png\"]    [Caption]{Prompt and output for a sample document of length 2445 in Hyperpartisan dataset, where correct predictions are highlighted in green and wrong predictions are highlighted in red. Compared to GPT4o and Gemini1.5pro, our model and Longformer can correctly classify the given document as False.}    [Label id=\"fig:case4\"]    [b]{1.0}           [Graphic src=\"images/case5_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case5_ans.png\"]        [Caption]{Prompt and output for a sample document of length 895 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case5\"]    [b]{1.0}           [Graphic src=\"images/case6_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case6_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1029 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case6\"]    [b]{1.0}           [Graphic src=\"images/case7_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case7_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1281 in GUM dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case7\"]    [b]{1.0}           [Graphic src=\"images/case8_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case8_ans.png\"]        [Caption]{Prompt and output for a sample document of length 1798 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case8\"]    [b]{1.0}           [Graphic src=\"images/case10_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case10_ans.png\"]        [Caption]{Prompt and output for a sample document of length 7474 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case10\"]    [b]{1.0}           [Graphic src=\"images/case11_prompt.png\"]             [b]{1.0}          [Graphic src=\"images/case11_ans.png\"]        [Caption]{Prompt and output for a sample document of length 9778 in CoNLL dataset for NER task, where correct predictions are highlighted in green and wrong predictions are highlighted in red.}    [Label id=\"case11\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 60
            }
        ]
    },
    {
        "question": "How and why does Docopilot reduce inference latency compared with RAG on long multimodal documents, and what do the paper\u2019s latency analysis and the MM-NIAH teaser figure jointly show about the accuracy\u2013latency trade-off?",
        "intent": [
            "Causal",
            "Verificative",
            "Evaluative",
            "Descriptive"
        ],
        "num_sections": 3,
        "avg_evidence_len": 487.42857142857144,
        "evidences": [
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "\u2026Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks. (1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks; (3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.",
                "gold_paragraph": "\u00a7 INTRODUCTION[Label id=\"sec:intro\"]In recent years, multimodal large language models (MLLMs)\u00a0<cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks\u00a0<cit.>, particularly image-level tasks, such as image captioning\u00a0<cit.>, optical character recognition (OCR)\u00a0<cit.>, and visual question answering (VQA)\u00a0<cit.>.Despite these advances, current MLLMs still face significant challenges in document-level understanding\u00a0<cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.    {[Graphic src=\"figure/teaser.pdf\"]}    [Caption]{Accuracy v.s inference latency on MM-NIAH.     The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9    [Label id=\"fig:teaser\"]    Current research on long-content understanding primarily focuses on text-only models\u00a0<cit.>, targeting specific retrieval tasks such as \u201cNeedle in a Haystack\u201d (NIAH)\u00a0<cit.>.However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.To address these problems, two primary challenges need to be considered.(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot.Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.The main contributions are summarized as follows:(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. This approach greatly improves its ability to integrate and comprehend information across multi-page documents.(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.}",
                "max_rougeL_score": 1.0,
                "evidence_length": 686
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "\u2026Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>. Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support. As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.",
                "gold_paragraph": "\u00a7 INTRODUCTION[Label id=\"sec:intro\"]In recent years, multimodal large language models (MLLMs)\u00a0<cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks\u00a0<cit.>, particularly image-level tasks, such as image captioning\u00a0<cit.>, optical character recognition (OCR)\u00a0<cit.>, and visual question answering (VQA)\u00a0<cit.>.Despite these advances, current MLLMs still face significant challenges in document-level understanding\u00a0<cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.    {[Graphic src=\"figure/teaser.pdf\"]}    [Caption]{Accuracy v.s inference latency on MM-NIAH.     The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9    [Label id=\"fig:teaser\"]    Current research on long-content understanding primarily focuses on text-only models\u00a0<cit.>, targeting specific retrieval tasks such as \u201cNeedle in a Haystack\u201d (NIAH)\u00a0<cit.>.However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.To address these problems, two primary challenges need to be considered.(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot.Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.The main contributions are summarized as follows:(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. This approach greatly improves its ability to integrate and comprehend information across multi-page documents.(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.}",
                "max_rougeL_score": 1.0,
                "evidence_length": 719
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "\u2026[Caption]{Accuracy v.s inference latency on MM-NIAH. The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9",
                "gold_paragraph": "\u00a7 INTRODUCTION[Label id=\"sec:intro\"]In recent years, multimodal large language models (MLLMs)\u00a0<cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks\u00a0<cit.>, particularly image-level tasks, such as image captioning\u00a0<cit.>, optical character recognition (OCR)\u00a0<cit.>, and visual question answering (VQA)\u00a0<cit.>.Despite these advances, current MLLMs still face significant challenges in document-level understanding\u00a0<cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.    {[Graphic src=\"figure/teaser.pdf\"]}    [Caption]{Accuracy v.s inference latency on MM-NIAH.     The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9    [Label id=\"fig:teaser\"]    Current research on long-content understanding primarily focuses on text-only models\u00a0<cit.>, targeting specific retrieval tasks such as \u201cNeedle in a Haystack\u201d (NIAH)\u00a0<cit.>.However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.To address these problems, two primary challenges need to be considered.(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot.Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.The main contributions are summarized as follows:(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. This approach greatly improves its ability to integrate and comprehend information across multi-page documents.(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.}",
                "max_rougeL_score": 1.0,
                "evidence_length": 154
            },
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Latency Analysis. To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 1.0,
                "evidence_length": 336
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026Optimizing Training Efficiency The training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 0.9672131147540983,
                "evidence_length": 441
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026(2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 1.0,
                "evidence_length": 579
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 1.0,
                "evidence_length": 497
            }
        ]
    },
    {
        "question": "Summarize the Doc-750K data engine end-to-end: how are documents processed into interleaved text-image and multi-image formats, how are QA pairs constructed, and how are these formats used in downstream evaluations? Please reference the dataset pipeline and example/format figures.",
        "intent": [
            "Descriptive",
            "Procedural"
        ],
        "num_sections": 4,
        "avg_evidence_len": 316.9,
        "evidences": [
            {
                "section_name": "related work",
                "fuzzy_matched_section": "related work",
                "true_category": "Introduction or background",
                "content": "\u2026[Caption]{ Multimodal document dataset generation pipeline. This pipeline involves three main stages: (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats. (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format. (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o. }",
                "gold_paragraph": "\u00a7 RELATED WORK \u00a7.\u00a7 Multimodal Large Language ModelsMultimodal large language models (MLLMs) have demonstrated impressive capabilities in processing image and text information, opening up new directions for applications such as visual question answering and image captioning.Early models\u00a0<cit.> trained with contrastive learning methods excelled in recognizing and understanding open-world semantics within an image-text matching framework. However, their limited generative abilities restricted their applicability.To leverage the powerful generation abilities of large language models (LLMs), subsequent works\u00a0<cit.> introduced a connector to align the embedding spaces of vision encoders and LLMs, allowing encoded image embeddings to serve as soft prompts for LLMs. Another series of works\u00a0<cit.> extended LLMs by integrating additional visual experts, reducing reliance on standalone vision encoders.More recently, models capable of both understanding and generating images have also made notable progress\u00a0<cit.>, leveraging the insight that image generation can enhance image understanding.Despite these advancements, current MLLMs still face challenges with long-context multimodal inputs. For instance, InternVL 2.0\u00a0<cit.> performs optimally within a token range of up to 8192, constraining its effectiveness in document-level applications.    {[Graphic src=\"figure/data_engine.pdf\"]}    [Caption]{    Multimodal document dataset generation pipeline.     This pipeline involves three main stages:     (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats.     (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format.     (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o.    }    [Label id=\"fig:data_pipeline\"]     \u00a7.\u00a7 Document Understanding ModelsExtracting key information from documents is crucial for industries and academic research.OCR-model-driven methods\u00a0<cit.> represent one of the primary technical approaches. These methods extract text, layout, and bounding box information from external systems and integrate it with another model. However, they are prone to error propagation and high processing times due to their reliance on multiple components.Benefitting from the rapid advancements in LLMs, OCR-free methods have also achieved great progress. Donut\u00a0<cit.> is the first end-to-end training framework based on a Transformer without requiring OCR engines or APIs. Subsequent works <cit.> propose diverse modifications in model architectures and training algorithms.However, these models are designed for specific tasks and lack general abilities. \u00a7.\u00a7 Long-Context Large Language ModelsWith advancements in engineering, architecture, and algorithms, long-context large language models have made substantial progress. Techniques such as Flash Attention <cit.> and Ring Attention <cit.> have notably reduced GPU memory usage for training on extended contexts. Additionally, various sparse attention mechanisms\u2014including Shifted Sparse Attention\u00a0<cit.>, Dilated Attention\u00a0<cit.>, and Attention Sinks\u00a0<cit.>\u2014have enabled efficient scaling to handle larger contexts. New positional embedding methods, like ALiBi\u00a0<cit.>, xPOS\u00a0<cit.>, and RoPE\u00a0<cit.>, further enhance the models\u2019 generalization capabilities in length extrapolation.However, these advancements remain largely confined to natural language processing, and methods to extend the context size of MLLMs are still under-explored.Another research approach aims to reduce context size by leveraging retrieval augmented generation (RAG)\u00a0<cit.>, where only the most relevant passages are retrieved and fed into the generation model. However, this retrieval-based approach can disrupt the coherence of the semantic chain, particularly in complex reasoning tasks, due to fragmented information flow.In this work, we integrate the above engineering techniques into MLLMs and demonstrate that a model fine-tuned on a high-quality, long-context training corpus is a strong baseline, achieving superior performance compared to its RAG counterpart.    {[Graphic src=\"figure/example.pdf\"]}    [Caption]{Visualization of an example from {Doc-750K}. The left side presents the interleaved text-image format data obtained through Document Content Extraction, while the right side showcases the annotations generated via Question-Answer Pairs Construction.}    [Label id=\"fig:data_example\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 496
            },
            {
                "section_name": "multimodal document dataset generation",
                "fuzzy_matched_section": "multimodal document dataset generation",
                "true_category": "Proposed Method",
                "content": "\u2026The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction. Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data.",
                "gold_paragraph": "\u00a7 MULTIMODAL DOCUMENT DATASET GENERATIONIn this section, we begin by introducing the details of the data engine.Following this, we provide a comprehensive overview of the dataset\u2014Doc-750K. \u00a7.\u00a7 Data Engine [Label id=\"sec:data_engine\"]The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: {{0.95}{}}Here, the , , and  are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.Document Content Extraction.In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.In this work, we process each document into two formats as follows:(1) Interleaved Text-Image Format. Using the document content extractor MinerU\u00a0<cit.>, we segment the document content into interleaved text and image annotations, for example, \u201c\u201dThis format captures the document\u2019s textual content, making it easier to construct question-answer pairs. (2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern \u201c\u201d. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.Question-Answer Pairs Construction.In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:(1) For documents with reliable QA annotations, like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.    {[Graphic src=\"figure/data.pdf\"]}    [Caption]{        Data distribution of our dataset.        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.        Left: Data distribution of {Doc-750K}.        Right: Data distribution of our complete SFT training dataset.         Note that the number reported in the figure represents the number of samples. \u201cMT\u201d is short for multi-turn.    }    [Label id=\"fig:data_distribution\"]    (2) For documents with a clear textual structure, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table\u00a0[Ref id=\"tab:task_format\"]. (3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o\u00a0<cit.>. Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8[Table][TableHeader] Tasks                 1c{Questions}[Caption]{Questions format for different tasks.For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.}[Label id=\"tab:task_format\"] \u00a7.\u00a7 Multimodal Document DatasetData Source. The composition and distribution of our training data are detailed in Figure\u00a0[Ref id=\"fig:data_distribution\"]. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6Dataset Statistics.In our {Doc-750K} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table\u00a0[Ref id=\"tab:dataset_analysis\"]. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table\u00a0[Ref id=\"tab:dataset_comparison\"]. \u00a7.\u00a7 Data Recipe for Supervised Fine-TuningAlthough Doc-750K effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.As shown in Figure [Ref id=\"fig:data_distribution\"](b), these datasets are organized into 4 categories as follows: (1) For multi-page document QA, Doc-750K serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix\u00a0<cit.>, MP-DocVQA\u00a0<cit.>, DUDE\u00a0<cit.>, and Taesiri-ArxivQA\u00a0<cit.>offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences.  (2) For multi-image general QA, MMDU-45K <cit.> offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.(3) For single-page document QA, We introduce DocVQA\u00a0<cit.>, DocReason <cit.>, InfoVQA <cit.>, and ChartQA <cit.> to further enhance the diversity of the SFT dataset.These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.(4) For pure-text QA, we add datasets including LongAlpaca <cit.>, LongAlpaca-16K-Length <cit.>, LongQLoRA <cit.>, LongCite <cit.>, LongAlign <cit.>, and LongReward <cit.> to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.",
                "max_rougeL_score": 1.0,
                "evidence_length": 409
            },
            {
                "section_name": "multimodal document dataset generation",
                "fuzzy_matched_section": "multimodal document dataset generation",
                "true_category": "Proposed Method",
                "content": "\u2026(1) Interleaved Text-Image Format. Using the document content extractor MinerU\u00a0<cit.>, we segment the document content into interleaved text and image annotations, for example, \u201c\u201d This format captures the document\u2019s textual content, making it easier to construct question-answer pairs.",
                "gold_paragraph": "\u00a7 MULTIMODAL DOCUMENT DATASET GENERATIONIn this section, we begin by introducing the details of the data engine.Following this, we provide a comprehensive overview of the dataset\u2014Doc-750K. \u00a7.\u00a7 Data Engine [Label id=\"sec:data_engine\"]The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: {{0.95}{}}Here, the , , and  are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.Document Content Extraction.In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.In this work, we process each document into two formats as follows:(1) Interleaved Text-Image Format. Using the document content extractor MinerU\u00a0<cit.>, we segment the document content into interleaved text and image annotations, for example, \u201c\u201dThis format captures the document\u2019s textual content, making it easier to construct question-answer pairs. (2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern \u201c\u201d. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.Question-Answer Pairs Construction.In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:(1) For documents with reliable QA annotations, like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.    {[Graphic src=\"figure/data.pdf\"]}    [Caption]{        Data distribution of our dataset.        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.        Left: Data distribution of {Doc-750K}.        Right: Data distribution of our complete SFT training dataset.         Note that the number reported in the figure represents the number of samples. \u201cMT\u201d is short for multi-turn.    }    [Label id=\"fig:data_distribution\"]    (2) For documents with a clear textual structure, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table\u00a0[Ref id=\"tab:task_format\"]. (3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o\u00a0<cit.>. Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8[Table][TableHeader] Tasks                 1c{Questions}[Caption]{Questions format for different tasks.For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.}[Label id=\"tab:task_format\"] \u00a7.\u00a7 Multimodal Document DatasetData Source. The composition and distribution of our training data are detailed in Figure\u00a0[Ref id=\"fig:data_distribution\"]. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6Dataset Statistics.In our {Doc-750K} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table\u00a0[Ref id=\"tab:dataset_analysis\"]. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table\u00a0[Ref id=\"tab:dataset_comparison\"]. \u00a7.\u00a7 Data Recipe for Supervised Fine-TuningAlthough Doc-750K effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.As shown in Figure [Ref id=\"fig:data_distribution\"](b), these datasets are organized into 4 categories as follows: (1) For multi-page document QA, Doc-750K serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix\u00a0<cit.>, MP-DocVQA\u00a0<cit.>, DUDE\u00a0<cit.>, and Taesiri-ArxivQA\u00a0<cit.>offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences.  (2) For multi-image general QA, MMDU-45K <cit.> offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.(3) For single-page document QA, We introduce DocVQA\u00a0<cit.>, DocReason <cit.>, InfoVQA <cit.>, and ChartQA <cit.> to further enhance the diversity of the SFT dataset.These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.(4) For pure-text QA, we add datasets including LongAlpaca <cit.>, LongAlpaca-16K-Length <cit.>, LongQLoRA <cit.>, LongCite <cit.>, LongAlign <cit.>, and LongReward <cit.> to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.",
                "max_rougeL_score": 1.0,
                "evidence_length": 286
            },
            {
                "section_name": "multimodal document dataset generation",
                "fuzzy_matched_section": "multimodal document dataset generation",
                "true_category": "Proposed Method",
                "content": "\u2026(2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern \u201c\u201d. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.",
                "gold_paragraph": "\u00a7 MULTIMODAL DOCUMENT DATASET GENERATIONIn this section, we begin by introducing the details of the data engine.Following this, we provide a comprehensive overview of the dataset\u2014Doc-750K. \u00a7.\u00a7 Data Engine [Label id=\"sec:data_engine\"]The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: {{0.95}{}}Here, the , , and  are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.Document Content Extraction.In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.In this work, we process each document into two formats as follows:(1) Interleaved Text-Image Format. Using the document content extractor MinerU\u00a0<cit.>, we segment the document content into interleaved text and image annotations, for example, \u201c\u201dThis format captures the document\u2019s textual content, making it easier to construct question-answer pairs. (2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern \u201c\u201d. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.Question-Answer Pairs Construction.In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:(1) For documents with reliable QA annotations, like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.    {[Graphic src=\"figure/data.pdf\"]}    [Caption]{        Data distribution of our dataset.        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.        Left: Data distribution of {Doc-750K}.        Right: Data distribution of our complete SFT training dataset.         Note that the number reported in the figure represents the number of samples. \u201cMT\u201d is short for multi-turn.    }    [Label id=\"fig:data_distribution\"]    (2) For documents with a clear textual structure, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table\u00a0[Ref id=\"tab:task_format\"]. (3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o\u00a0<cit.>. Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8[Table][TableHeader] Tasks                 1c{Questions}[Caption]{Questions format for different tasks.For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.}[Label id=\"tab:task_format\"] \u00a7.\u00a7 Multimodal Document DatasetData Source. The composition and distribution of our training data are detailed in Figure\u00a0[Ref id=\"fig:data_distribution\"]. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6Dataset Statistics.In our {Doc-750K} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table\u00a0[Ref id=\"tab:dataset_analysis\"]. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table\u00a0[Ref id=\"tab:dataset_comparison\"]. \u00a7.\u00a7 Data Recipe for Supervised Fine-TuningAlthough Doc-750K effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.As shown in Figure [Ref id=\"fig:data_distribution\"](b), these datasets are organized into 4 categories as follows: (1) For multi-page document QA, Doc-750K serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix\u00a0<cit.>, MP-DocVQA\u00a0<cit.>, DUDE\u00a0<cit.>, and Taesiri-ArxivQA\u00a0<cit.>offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences.  (2) For multi-image general QA, MMDU-45K <cit.> offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.(3) For single-page document QA, We introduce DocVQA\u00a0<cit.>, DocReason <cit.>, InfoVQA <cit.>, and ChartQA <cit.> to further enhance the diversity of the SFT dataset.These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.(4) For pure-text QA, we add datasets including LongAlpaca <cit.>, LongAlpaca-16K-Length <cit.>, LongQLoRA <cit.>, LongCite <cit.>, LongAlign <cit.>, and LongReward <cit.> to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.",
                "max_rougeL_score": 1.0,
                "evidence_length": 308
            },
            {
                "section_name": "details of data construction",
                "fuzzy_matched_section": "details of data construction",
                "true_category": "Others",
                "content": "\u2026As stated in Section\u00a0[Ref id=\"sec:data_engine\"], documents in Doc-750K can be extracted in two formats: Interleaved Text-Image Format and rendered Multi-Image Format. The interleaved format utilizes an external PDF parser to extract text directly, avoiding OCR errors. However, this approach sacrifices some layout information. The multi-image format, commonly used in screenshot-based QA scenarios, preserves the complete layout but relies on the model's built-in OCR capabilities, which may introduce errors. Each format has its advantages and is suitable for different applications. By leveraging both formats, the model can develop complementary capabilities, enhancing its robustness across diverse input types.",
                "gold_paragraph": "\u00a7 DETAILS OF DATA CONSTRUCTION \u00a7.\u00a7 Document Data FormatAs stated in Section\u00a0[Ref id=\"sec:data_engine\"], documents in Doc-750K can be extracted in two formats: Interleaved Text-Image Format and rendered Multi-Image Format. The interleaved format utilizes an external PDF parser to extract text directly, avoiding OCR errors. However, this approach sacrifices some layout information. The multi-image format, commonly used in screenshot-based QA scenarios, preserves the complete layout but relies on the model's built-in OCR capabilities, which may introduce errors. Each format has its advantages and is suitable for different applications. By leveraging both formats, the model can develop complementary capabilities, enhancing its robustness across diverse input types. Examples of each format are shown in Figure\u00a0[Ref id=\"fig:multi-image\"] and Figure\u00a0[Ref id=\"fig:interleaved-image_text\"], respectively.    {[Graphic src=\"figure/multi-image_format.pdf\"]}    [Caption]{    An example of multi-image format document. Each page of the document is rendered as an image.    }    [Label id=\"fig:multi-image\"]    {[Graphic src=\"figure/interleaved_example.pdf\"]}    [Caption]{    An example of interleaved text-image format document. We capture the documents' textual content and construct them into interleaved images and text.     }    [Label id=\"fig:interleaved-image_text\"] \u00a7.\u00a7 Image Types and Tasks CoverageWe provide additional comparisons of Doc-750K with various document-level datasets in terms of image types and task coverage, as shown in Table\u00a0[Ref id=\"tab:addi_dataset_comparison\"]. Compared to these datasets, Doc-750K exhibits greater diversity in proxy tasks and ranks among the largest datasets in terms of QA pair count. \u00a7.\u00a7 Question-Answer Pairs GenerationPrompt. The prompt used to generate question-answer pairs from GPT-4o is shown below.{Please read the paper and first check if this is an English paper. If it is not an English paper, don't do any other things. If it is an English paper, please design about 3 to 5 question-answer pairs based on the paper. All questions should require as much text as possible to answer and it is better to ask about the images in the papers. All images in the questions should be represented as the mentioned title in the paper like Figure 1/Figure 2 or Table 1/ Table 2 and they must be mentioned in the questions. The question should be specific enough that it can only be answered with the paper. The question should also be interesting and intellectual enough that a curious reader of the paper would ask about it. The answer of the QA pair must start with \"According to the original text ......\u201d, first give the relevant original text in the reference content, and then answer the question in detail. Please try to analyze the asked image in the answer. Please directly output a list of the QA pairs without any other outputs. Here is the paper: <paper>}Examples. In Section\u00a0[Ref id=\"sec:data_engine\"], we propose diverse question-answer formats tailored to data from different sources. To fully utilize the webpage structure of OpenReview, we develop tasks focused on review writing and replies within its review-reply framework. For Sci-Hub and Arxiv, we use their well-defined writing structures to create tasks such as writing and translating various sections. We provide examples of these various QA formats in Figure\u00a0[Ref id=\"fig:qa_example\"].  Quality Evaluation. The quality of Doc-750K is ensured through the following measures: (1) Human-Originated Data: QA pairs from OpenReview are derived from human-written discussions, providing high contextual quality.(2) Structured Tasks: Tasks like abstract writing and paper titling are constructed based on document metadata, following deterministic rules to ensure reliability.(3) Synthetic QA: We randomly sample and manually review 500 training QA pairs across tasks and 498 of 500 (over 99  {    {.95}  }  [Caption]{The examples of different formats of the QA pairs. All tasks leverage the inherent structure of the documents.}  [Label id=\"fig:qa_example\"]",
                "max_rougeL_score": 0.9906542056074766,
                "evidence_length": 717
            },
            {
                "section_name": "details of data construction",
                "fuzzy_matched_section": "details of data construction",
                "true_category": "Others",
                "content": "\u2026[Caption]{ An example of multi-image format document. Each page of the document is rendered as an image. }",
                "gold_paragraph": "\u00a7 DETAILS OF DATA CONSTRUCTION \u00a7.\u00a7 Document Data FormatAs stated in Section\u00a0[Ref id=\"sec:data_engine\"], documents in Doc-750K can be extracted in two formats: Interleaved Text-Image Format and rendered Multi-Image Format. The interleaved format utilizes an external PDF parser to extract text directly, avoiding OCR errors. However, this approach sacrifices some layout information. The multi-image format, commonly used in screenshot-based QA scenarios, preserves the complete layout but relies on the model's built-in OCR capabilities, which may introduce errors. Each format has its advantages and is suitable for different applications. By leveraging both formats, the model can develop complementary capabilities, enhancing its robustness across diverse input types. Examples of each format are shown in Figure\u00a0[Ref id=\"fig:multi-image\"] and Figure\u00a0[Ref id=\"fig:interleaved-image_text\"], respectively.    {[Graphic src=\"figure/multi-image_format.pdf\"]}    [Caption]{    An example of multi-image format document. Each page of the document is rendered as an image.    }    [Label id=\"fig:multi-image\"]    {[Graphic src=\"figure/interleaved_example.pdf\"]}    [Caption]{    An example of interleaved text-image format document. We capture the documents' textual content and construct them into interleaved images and text.     }    [Label id=\"fig:interleaved-image_text\"] \u00a7.\u00a7 Image Types and Tasks CoverageWe provide additional comparisons of Doc-750K with various document-level datasets in terms of image types and task coverage, as shown in Table\u00a0[Ref id=\"tab:addi_dataset_comparison\"]. Compared to these datasets, Doc-750K exhibits greater diversity in proxy tasks and ranks among the largest datasets in terms of QA pair count. \u00a7.\u00a7 Question-Answer Pairs GenerationPrompt. The prompt used to generate question-answer pairs from GPT-4o is shown below.{Please read the paper and first check if this is an English paper. If it is not an English paper, don't do any other things. If it is an English paper, please design about 3 to 5 question-answer pairs based on the paper. All questions should require as much text as possible to answer and it is better to ask about the images in the papers. All images in the questions should be represented as the mentioned title in the paper like Figure 1/Figure 2 or Table 1/ Table 2 and they must be mentioned in the questions. The question should be specific enough that it can only be answered with the paper. The question should also be interesting and intellectual enough that a curious reader of the paper would ask about it. The answer of the QA pair must start with \"According to the original text ......\u201d, first give the relevant original text in the reference content, and then answer the question in detail. Please try to analyze the asked image in the answer. Please directly output a list of the QA pairs without any other outputs. Here is the paper: <paper>}Examples. In Section\u00a0[Ref id=\"sec:data_engine\"], we propose diverse question-answer formats tailored to data from different sources. To fully utilize the webpage structure of OpenReview, we develop tasks focused on review writing and replies within its review-reply framework. For Sci-Hub and Arxiv, we use their well-defined writing structures to create tasks such as writing and translating various sections. We provide examples of these various QA formats in Figure\u00a0[Ref id=\"fig:qa_example\"].  Quality Evaluation. The quality of Doc-750K is ensured through the following measures: (1) Human-Originated Data: QA pairs from OpenReview are derived from human-written discussions, providing high contextual quality.(2) Structured Tasks: Tasks like abstract writing and paper titling are constructed based on document metadata, following deterministic rules to ensure reliability.(3) Synthetic QA: We randomly sample and manually review 500 training QA pairs across tasks and 498 of 500 (over 99  {    {.95}  }  [Caption]{The examples of different formats of the QA pairs. All tasks leverage the inherent structure of the documents.}  [Label id=\"fig:qa_example\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 107
            },
            {
                "section_name": "details of data construction",
                "fuzzy_matched_section": "details of data construction",
                "true_category": "Others",
                "content": "\u2026[Caption]{ An example of interleaved text-image format document. We capture the documents' textual content and construct them into interleaved images and text. }",
                "gold_paragraph": "\u00a7 DETAILS OF DATA CONSTRUCTION \u00a7.\u00a7 Document Data FormatAs stated in Section\u00a0[Ref id=\"sec:data_engine\"], documents in Doc-750K can be extracted in two formats: Interleaved Text-Image Format and rendered Multi-Image Format. The interleaved format utilizes an external PDF parser to extract text directly, avoiding OCR errors. However, this approach sacrifices some layout information. The multi-image format, commonly used in screenshot-based QA scenarios, preserves the complete layout but relies on the model's built-in OCR capabilities, which may introduce errors. Each format has its advantages and is suitable for different applications. By leveraging both formats, the model can develop complementary capabilities, enhancing its robustness across diverse input types. Examples of each format are shown in Figure\u00a0[Ref id=\"fig:multi-image\"] and Figure\u00a0[Ref id=\"fig:interleaved-image_text\"], respectively.    {[Graphic src=\"figure/multi-image_format.pdf\"]}    [Caption]{    An example of multi-image format document. Each page of the document is rendered as an image.    }    [Label id=\"fig:multi-image\"]    {[Graphic src=\"figure/interleaved_example.pdf\"]}    [Caption]{    An example of interleaved text-image format document. We capture the documents' textual content and construct them into interleaved images and text.     }    [Label id=\"fig:interleaved-image_text\"] \u00a7.\u00a7 Image Types and Tasks CoverageWe provide additional comparisons of Doc-750K with various document-level datasets in terms of image types and task coverage, as shown in Table\u00a0[Ref id=\"tab:addi_dataset_comparison\"]. Compared to these datasets, Doc-750K exhibits greater diversity in proxy tasks and ranks among the largest datasets in terms of QA pair count. \u00a7.\u00a7 Question-Answer Pairs GenerationPrompt. The prompt used to generate question-answer pairs from GPT-4o is shown below.{Please read the paper and first check if this is an English paper. If it is not an English paper, don't do any other things. If it is an English paper, please design about 3 to 5 question-answer pairs based on the paper. All questions should require as much text as possible to answer and it is better to ask about the images in the papers. All images in the questions should be represented as the mentioned title in the paper like Figure 1/Figure 2 or Table 1/ Table 2 and they must be mentioned in the questions. The question should be specific enough that it can only be answered with the paper. The question should also be interesting and intellectual enough that a curious reader of the paper would ask about it. The answer of the QA pair must start with \"According to the original text ......\u201d, first give the relevant original text in the reference content, and then answer the question in detail. Please try to analyze the asked image in the answer. Please directly output a list of the QA pairs without any other outputs. Here is the paper: <paper>}Examples. In Section\u00a0[Ref id=\"sec:data_engine\"], we propose diverse question-answer formats tailored to data from different sources. To fully utilize the webpage structure of OpenReview, we develop tasks focused on review writing and replies within its review-reply framework. For Sci-Hub and Arxiv, we use their well-defined writing structures to create tasks such as writing and translating various sections. We provide examples of these various QA formats in Figure\u00a0[Ref id=\"fig:qa_example\"].  Quality Evaluation. The quality of Doc-750K is ensured through the following measures: (1) Human-Originated Data: QA pairs from OpenReview are derived from human-written discussions, providing high contextual quality.(2) Structured Tasks: Tasks like abstract writing and paper titling are constructed based on document metadata, following deterministic rules to ensure reliability.(3) Synthetic QA: We randomly sample and manually review 500 training QA pairs across tasks and 498 of 500 (over 99  {    {.95}  }  [Caption]{The examples of different formats of the QA pairs. All tasks leverage the inherent structure of the documents.}  [Label id=\"fig:qa_example\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 162
            },
            {
                "section_name": "multimodal document dataset generation",
                "fuzzy_matched_section": "multimodal document dataset generation",
                "true_category": "Proposed Method",
                "content": "\u2026(3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o\u00a0<cit.>.",
                "gold_paragraph": "\u00a7 MULTIMODAL DOCUMENT DATASET GENERATIONIn this section, we begin by introducing the details of the data engine.Following this, we provide a comprehensive overview of the dataset\u2014Doc-750K. \u00a7.\u00a7 Data Engine [Label id=\"sec:data_engine\"]The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: {{0.95}{}}Here, the , , and  are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.Document Content Extraction.In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.In this work, we process each document into two formats as follows:(1) Interleaved Text-Image Format. Using the document content extractor MinerU\u00a0<cit.>, we segment the document content into interleaved text and image annotations, for example, \u201c\u201dThis format captures the document\u2019s textual content, making it easier to construct question-answer pairs. (2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern \u201c\u201d. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.Question-Answer Pairs Construction.In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:(1) For documents with reliable QA annotations, like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.    {[Graphic src=\"figure/data.pdf\"]}    [Caption]{        Data distribution of our dataset.        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.        Left: Data distribution of {Doc-750K}.        Right: Data distribution of our complete SFT training dataset.         Note that the number reported in the figure represents the number of samples. \u201cMT\u201d is short for multi-turn.    }    [Label id=\"fig:data_distribution\"]    (2) For documents with a clear textual structure, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table\u00a0[Ref id=\"tab:task_format\"]. (3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o\u00a0<cit.>. Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8[Table][TableHeader] Tasks                 1c{Questions}[Caption]{Questions format for different tasks.For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.}[Label id=\"tab:task_format\"] \u00a7.\u00a7 Multimodal Document DatasetData Source. The composition and distribution of our training data are detailed in Figure\u00a0[Ref id=\"fig:data_distribution\"]. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6Dataset Statistics.In our {Doc-750K} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table\u00a0[Ref id=\"tab:dataset_analysis\"]. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table\u00a0[Ref id=\"tab:dataset_comparison\"]. \u00a7.\u00a7 Data Recipe for Supervised Fine-TuningAlthough Doc-750K effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.As shown in Figure [Ref id=\"fig:data_distribution\"](b), these datasets are organized into 4 categories as follows: (1) For multi-page document QA, Doc-750K serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix\u00a0<cit.>, MP-DocVQA\u00a0<cit.>, DUDE\u00a0<cit.>, and Taesiri-ArxivQA\u00a0<cit.>offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences.  (2) For multi-image general QA, MMDU-45K <cit.> offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.(3) For single-page document QA, We introduce DocVQA\u00a0<cit.>, DocReason <cit.>, InfoVQA <cit.>, and ChartQA <cit.> to further enhance the diversity of the SFT dataset.These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.(4) For pure-text QA, we add datasets including LongAlpaca <cit.>, LongAlpaca-16K-Length <cit.>, LongQLoRA <cit.>, LongCite <cit.>, LongAlign <cit.>, and LongReward <cit.> to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.",
                "max_rougeL_score": 1.0,
                "evidence_length": 243
            },
            {
                "section_name": "evaluation details",
                "fuzzy_matched_section": "evaluation details",
                "true_category": "Experimental Results",
                "content": "\u2026For multi-page benchmarks, we discuss them case by case. We employed image concatenation for multi-page VQA benchmarks like MP-DocVQA, MMLongBench-Doc, and DocGenome to reduce the excessive input patches. Adjacent pages were vertically concatenated into a single image, with a maximum total image count limit of 18.",
                "gold_paragraph": "\u00a7 EVALUATION DETAILS \u00a7.\u00a7 Benchmark MetricsWe report the metrics of benchmarks used in the evaluation in Table\u00a0[Ref id=\"tab:benchmark_metrics\"]. For DocVQA\u00a0<cit.>, InfoVQA\u00a0<cit.>, and MP-DocVQA\u00a0<cit.>, we employ ANLS to evaluate the similarity between model responses and ground truth answers, while ChartQA\u00a0<cit.> uses Relaxed Exact Match (Relaxed EM). For open-ended QA tasks in MMLongbench-Doc\u00a0<cit.> and DocGenome\u00a0<cit.>, we utilize GPT-4o to assess the correctness of answers and calculate GPT Accuracy. Other tasks in DocGenome follow their official evaluation metrics. \u00a7.\u00a7 Evaluation SettingsFor single-page benchmarks such as DocVQA, ChartQA, and InfoVQA, we conduct the evaluation using a unified prompt as follows:{<image>  <question>  Answer the question using a single word or phrase.}For multi-page benchmarks, we discuss them case by case. We employed image concatenation for multi-page VQA benchmarks like MP-DocVQA, MMLongBench-Doc, and DocGenome to reduce the excessive input patches. Adjacent pages were vertically concatenated into a single image, with a maximum total image count limit of 18.(1) For MPDocVQA, we use the prompt for a $N$ concatenated page document as follows:{Image-1: <concat-page 1>  Image-2: <concat-page 2>  ...  Image-N: <concat-page N>  <question> Answer the question using a single word or phrase.}(2) For MMLongBench-Doc and DocGenome, we use the official prompt in their open-sourced code base for response generation and extract the correctness of the answer using GPT-4o.(3) For MM-NIAH, we use the original interleaved data format and calculate the accuracy by their official judgment function.Evaluation of LLMs on Multimodal Document Benchmarks.We utilized the InternVL2-8B as the OCR model to extract text from each image of the document, followed by post-processing to remove redundant responses. The text extraction prompt is as follows:{Image-1: <image> Please extract the text from Image-1, while retaining as much of the original formatting and structured information (such as headings, paragraphs, lists, tables, charts, etc.) as possible. If the document is not in PDF format, provide a caption for Image-1. Present the extracted information directly without additional explanations.}We concatenated the extracted texts to replace the original document images for the language models:{Page 1: <text 1>  Page 2: <text 2> ... Page N: <text N>}For the image-text interleaved data, we replaced the images with the captions as the input.Implementation Details of Multimodel RAG.VisRAG\u00a0<cit.> uses their proposed retrieval model VisRet to calculate scores for each image and text segment based on the query. InternVL-RAG\u00a0<cit.> utilizes InternVL-14B, a CLIP-like model, to compute the similarity between images and text. For multi-paged VQA benchmarks, we select the top-3 retrieved documents for generation. For interleaved VQA, we choose up to 8K tokens for the generation.",
                "max_rougeL_score": 1.0,
                "evidence_length": 316
            },
            {
                "section_name": "evaluation details",
                "fuzzy_matched_section": "evaluation details",
                "true_category": "Experimental Results",
                "content": "\u2026(3) For MM-NIAH, we use the original interleaved data format and calculate the accuracy by their official judgment function.",
                "gold_paragraph": "\u00a7 EVALUATION DETAILS \u00a7.\u00a7 Benchmark MetricsWe report the metrics of benchmarks used in the evaluation in Table\u00a0[Ref id=\"tab:benchmark_metrics\"]. For DocVQA\u00a0<cit.>, InfoVQA\u00a0<cit.>, and MP-DocVQA\u00a0<cit.>, we employ ANLS to evaluate the similarity between model responses and ground truth answers, while ChartQA\u00a0<cit.> uses Relaxed Exact Match (Relaxed EM). For open-ended QA tasks in MMLongbench-Doc\u00a0<cit.> and DocGenome\u00a0<cit.>, we utilize GPT-4o to assess the correctness of answers and calculate GPT Accuracy. Other tasks in DocGenome follow their official evaluation metrics. \u00a7.\u00a7 Evaluation SettingsFor single-page benchmarks such as DocVQA, ChartQA, and InfoVQA, we conduct the evaluation using a unified prompt as follows:{<image>  <question>  Answer the question using a single word or phrase.}For multi-page benchmarks, we discuss them case by case. We employed image concatenation for multi-page VQA benchmarks like MP-DocVQA, MMLongBench-Doc, and DocGenome to reduce the excessive input patches. Adjacent pages were vertically concatenated into a single image, with a maximum total image count limit of 18.(1) For MPDocVQA, we use the prompt for a $N$ concatenated page document as follows:{Image-1: <concat-page 1>  Image-2: <concat-page 2>  ...  Image-N: <concat-page N>  <question> Answer the question using a single word or phrase.}(2) For MMLongBench-Doc and DocGenome, we use the official prompt in their open-sourced code base for response generation and extract the correctness of the answer using GPT-4o.(3) For MM-NIAH, we use the original interleaved data format and calculate the accuracy by their official judgment function.Evaluation of LLMs on Multimodal Document Benchmarks.We utilized the InternVL2-8B as the OCR model to extract text from each image of the document, followed by post-processing to remove redundant responses. The text extraction prompt is as follows:{Image-1: <image> Please extract the text from Image-1, while retaining as much of the original formatting and structured information (such as headings, paragraphs, lists, tables, charts, etc.) as possible. If the document is not in PDF format, provide a caption for Image-1. Present the extracted information directly without additional explanations.}We concatenated the extracted texts to replace the original document images for the language models:{Page 1: <text 1>  Page 2: <text 2> ... Page N: <text N>}For the image-text interleaved data, we replaced the images with the captions as the input.Implementation Details of Multimodel RAG.VisRAG\u00a0<cit.> uses their proposed retrieval model VisRet to calculate scores for each image and text segment based on the query. InternVL-RAG\u00a0<cit.> utilizes InternVL-14B, a CLIP-like model, to compute the similarity between images and text. For multi-paged VQA benchmarks, we select the top-3 retrieved documents for generation. For interleaved VQA, we choose up to 8K tokens for the generation.",
                "max_rougeL_score": 1.0,
                "evidence_length": 125
            }
        ]
    },
    {
        "question": "Compare Docopilot\u2019s performance on multi-page, interleaved long-context, and single-page benchmarks, and explain what the ablation reveals about the contribution of the Doc-750K dataset.",
        "intent": [
            "Comparative",
            "Evaluative",
            "Verificative",
            "Descriptive"
        ],
        "num_sections": 3,
        "avg_evidence_len": 437.14285714285717,
        "evidences": [
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Results. As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs. Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios. For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 1.0,
                "evidence_length": 701
            },
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Results. The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 1.0,
                "evidence_length": 551
            },
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Results. As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents.",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 1.0,
                "evidence_length": 443
            },
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Effect of Doc-750K. We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 0.9896907216494846,
                "evidence_length": 558
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "abstract",
                "true_category": "Introduction or background",
                "content": "\u2026Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding.",
                "gold_paragraph": "{CVPR}2025{2025}{Docopilot}-750K{Doc-750K}{UTF8}{gbsn} {Docopilot: Improving Multimodal Models for Document-Level Understanding}    {{0.91}{Yuchen Duan$^{1,2*}$, Zhe Chen$^{3, 1*}$, Yusong Hu$^{4,1*}$, Weiyun Wang$^{*5,1}$, Shenglong Ye$^1$, Botian Shi$^1$,} {0.91}{\u00a0Lewei Lu$^7$, Qibin Hou$^4$, Tong Lu$^{3,1}$, Hongsheng Li$^{2,1}$, Jifeng Dai$^{6,1}$, Wenhai Wang$^{2,1}$} {0.91}{\u00a0$^1$Shanghai AI Laboratory,\u00a0$^2$The Chinese University of Hong Kong, $^3$Nanjing University,} {0.91}{\u00a0$^4$Nankai University, $^5$Fudan University, $^6$Tsinghua University, $^7$SenseTime Research}}    September 3, 2025==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================={* Equal contribution;  \u00a0 Corresponding author:\u00a0wangwenhai@pjlab.org.cn}Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets.While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents.This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents.Building on the dataset, we develop a native multimodal model\u2014Docopilot, which can accurately handle document-level dependencies without relying on RAG.Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. Data, code, and models are released at <https://github.com/OpenGVLab/Docopilot>.",
                "max_rougeL_score": 1.0,
                "evidence_length": 222
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "\u2026(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31",
                "gold_paragraph": "\u00a7 INTRODUCTION[Label id=\"sec:intro\"]In recent years, multimodal large language models (MLLMs)\u00a0<cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks\u00a0<cit.>, particularly image-level tasks, such as image captioning\u00a0<cit.>, optical character recognition (OCR)\u00a0<cit.>, and visual question answering (VQA)\u00a0<cit.>.Despite these advances, current MLLMs still face significant challenges in document-level understanding\u00a0<cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.    {[Graphic src=\"figure/teaser.pdf\"]}    [Caption]{Accuracy v.s inference latency on MM-NIAH.     The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9    [Label id=\"fig:teaser\"]    Current research on long-content understanding primarily focuses on text-only models\u00a0<cit.>, targeting specific retrieval tasks such as \u201cNeedle in a Haystack\u201d (NIAH)\u00a0<cit.>.However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.To address these problems, two primary challenges need to be considered.(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot.Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.The main contributions are summarized as follows:(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. This approach greatly improves its ability to integrate and comprehend information across multi-page documents.(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.}",
                "max_rougeL_score": 0.9827586206896551,
                "evidence_length": 386
            },
            {
                "section_name": "conclusions",
                "fuzzy_matched_section": "conclusions",
                "true_category": "Conclusion",
                "content": "\u2026Experimental results show that our model achieves state-of-the-art performance across several document-level QA benchmarks, underscoring its strength in multi-page integration and complex reasoning.",
                "gold_paragraph": "\u00a7 CONCLUSIONSThis work introduced a diverse document-level question-answering dataset that covers complex structures and cross-page dependencies, providing a robust foundation for training and evaluating document understanding models. We also proposed a retrieval-free long-document understanding model that effectively integrates multi-page information, reducing reliance on external retrieval systems. Experimental results show that our model achieves state-of-the-art performance across several document-level QA benchmarks, underscoring its strength in multi-page integration and complex reasoning. Future work will focus on improving computational efficiency, extending the model to larger multimodal tasks, and adapting it to broader applications for enhanced practicality and generalization.",
                "max_rougeL_score": 1.0,
                "evidence_length": 199
            }
        ]
    },
    {
        "question": "Detail the training-efficiency techniques Docopilot employs\u2014multimodal data packing, Ring Attention, and Liger Kernel\u2014and explain how the packed-dataset algorithm enforces attention boundaries and supports long-context training under the reported setup.",
        "intent": [
            "Procedural",
            "Descriptive",
            "Causal"
        ],
        "num_sections": 5,
        "avg_evidence_len": 569.375,
        "evidences": [
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026Optimizing Training Efficiency The training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 0.9672131147540983,
                "evidence_length": 441
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample (s) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If (s) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit (M), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials.",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 1.0,
                "evidence_length": 1251
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026(2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 1.0,
                "evidence_length": 579
            },
            {
                "section_name": "enhanced baseline for documentlevel multimodal understanding",
                "fuzzy_matched_section": "enhanced baseline for documentlevel multimodal understanding",
                "true_category": "Proposed Method",
                "content": "\u2026(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "gold_paragraph": "\u00a7 ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING \u00a7.\u00a7 Model ArchitectureOur model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework. \u00a7.\u00a7 Optimizing Training EfficiencyThe training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \\(s\\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \\(s\\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \\(M\\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. (2) Ring Attention. We implement the Ring Attention mechanism\u00a0\u00a0<cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel\u00a0<cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.",
                "max_rougeL_score": 1.0,
                "evidence_length": 497
            },
            {
                "section_name": "training details",
                "fuzzy_matched_section": "training details",
                "true_category": "Others",
                "content": "\u2026(3) Pack Samples: The given sample and the selected buffer are packed together. Notably, during the training process, each token can only attend to other tokens within the same original sample. Tokens from other samples packed together remain inaccessible.",
                "gold_paragraph": "\u00a7 TRAINING DETAILS \u00a7.\u00a7 HyperparametersWe report the models and training hyperparameters of Docopilot-2B and Docopilot-8B in Table\u00a0[Ref id=\"tab:hyperparam\"]. \u00a7.\u00a7 Multimodal Packed DatasetIn this section, we provide a detailed description of our packing algorithm. The main workflow is outlined in Algorithm [Ref id=\"alg:packed_dataset\"]. Specifically, our algorithm constructs the packed dataset by combining individual samples drawn from the original dataset. The packing operation involves four steps:(1) Check Sample: Given an individual sample, we first verify whether the number of images exceeds the image threshold $T_i$ or the number of tokens exceeds the token threshold $T_t$. If either condition is met, the sample is truncated into $N$ parts. The first $N-1$ parts contain exactly $T_i$ images or $T_t$ tokens and are immediately added to the output. The remaining part is passed to the subsequent steps for further processing.(2) Find Buffer: For the remaining part of the sample, we attempt to find a suitable buffer from the buffer list to pack it together with the sample. The combined result must not exceed the thresholds $T_i$ for images or $T_t$ for tokens, while maximizing the total number of images and tokens in the packed sample. To speed up this process, the buffer list is organized as a priority queue.(3) Pack Samples: The given sample and the selected buffer are packed together. Notably, during the training process, each token can only attend to other tokens within the same original sample. Tokens from other samples packed together remain inaccessible.(4) Maintain Buffer List: After generating a packed sample, we check if its number of images or tokens meets the specified thresholds. If so, the sample is added to the output; otherwise, it is reinserted into the buffer list for potential future packing.Note that we omit numerous edge cases for brevity.\t[Caption]{Multimodal Packed Dataset} \t[Label id=\"alg:packed_dataset\"]     {Dataset $\ud835\udc9f$, buffer list $\u212c$, Token Threshold $T_t$, Image Threshold $T_i$}    {Packed Dataset $\ud835\udc9f_packed$}    {data_sample $d$ in $\ud835\udc9f$}{        $b  (d, B)$         $b_p(d, b)$        {$b_p$ contains more than $T_i$ images or $T_t$ tokens}{             $b_{packed}$        }        {            $(b_{packed}, B)$        }    }",
                "max_rougeL_score": 1.0,
                "evidence_length": 257
            },
            {
                "section_name": "experiments",
                "fuzzy_matched_section": "experiments",
                "true_category": "Experimental Results",
                "content": "\u2026Training Details. Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule. To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.",
                "gold_paragraph": "\u00a7 EXPERIMENTS \u00a7.\u00a7 Experimental SetupTraining Details.Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2\u00a0<cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy\u00a0<cit.> to enhance OCR for document understanding. The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.Baselines.We compare our Docopilot with a series of open-source document-level MLLMs\u00a0<cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro\u00a0<cit.>, GPT-4o\u00a0<cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG\u00a0<cit.>, InternVL + RAG\u00a0<cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs\u00a0<cit.>. \u00a7.\u00a7 Multi-Page VQABenchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:(1) MP-DocVQA <cit.>, which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.(2) MMLongbench-Doc <cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.(3) DocGenome <cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.Results.As illustrated in Table\u00a0[Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. In comparison to RAG-based methods\u00a0<cit.>, our model demonstrates advantages in multi-page scenarios.For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6 \u00a7.\u00a7 Interleaved Long-Context QABenchmarks.For the interleaved long-context QA task, we evaluate our models on MM-NIAH\u00a0<cit.>, a benchmark designed for long multimodal document comprehension.Results.The right side of Table\u00a0[Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into \"Short,\" \"Medium,\" and \"Long\" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110 \u00a7.\u00a7 Single-Page VQABenchmarks.For single-page VQA tasks, we evaluate our model on three benchmarks:(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.Results.As shown in Table\u00a0[Ref id=\"tab:single_page_qa\"], our model achieves comparable performance to baseline models. Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents. \u00a7.\u00a7 Ablation StudyEffect of Doc-750K.We conducted ablation studies on MMLongBench-Doc\u00a0<cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table\u00a0[Ref id=\"tab:data_effect\"]. We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.Latency Analysis.To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc\u00a0<cit.>, as reported in Table\u00a0[Ref id=\"tab:latency\"]. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130",
                "max_rougeL_score": 0.992,
                "evidence_length": 694
            },
            {
                "section_name": "related work",
                "fuzzy_matched_section": "related work",
                "true_category": "Introduction or background",
                "content": "\u2026With advancements in engineering, architecture, and algorithms, long-context large language models have made substantial progress. Techniques such as Flash Attention <cit.> and Ring Attention <cit.> have notably reduced GPU memory usage for training on extended contexts. Additionally, various sparse attention mechanisms\u2014including Shifted Sparse Attention\u00a0<cit.>, Dilated Attention\u00a0<cit.>, and Attention Sinks\u00a0<cit.>\u2014have enabled efficient scaling to handle larger contexts.",
                "gold_paragraph": "\u00a7 RELATED WORK \u00a7.\u00a7 Multimodal Large Language ModelsMultimodal large language models (MLLMs) have demonstrated impressive capabilities in processing image and text information, opening up new directions for applications such as visual question answering and image captioning.Early models\u00a0<cit.> trained with contrastive learning methods excelled in recognizing and understanding open-world semantics within an image-text matching framework. However, their limited generative abilities restricted their applicability.To leverage the powerful generation abilities of large language models (LLMs), subsequent works\u00a0<cit.> introduced a connector to align the embedding spaces of vision encoders and LLMs, allowing encoded image embeddings to serve as soft prompts for LLMs. Another series of works\u00a0<cit.> extended LLMs by integrating additional visual experts, reducing reliance on standalone vision encoders.More recently, models capable of both understanding and generating images have also made notable progress\u00a0<cit.>, leveraging the insight that image generation can enhance image understanding.Despite these advancements, current MLLMs still face challenges with long-context multimodal inputs. For instance, InternVL 2.0\u00a0<cit.> performs optimally within a token range of up to 8192, constraining its effectiveness in document-level applications.    {[Graphic src=\"figure/data_engine.pdf\"]}    [Caption]{    Multimodal document dataset generation pipeline.     This pipeline involves three main stages:     (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats.     (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format.     (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o.    }    [Label id=\"fig:data_pipeline\"]     \u00a7.\u00a7 Document Understanding ModelsExtracting key information from documents is crucial for industries and academic research.OCR-model-driven methods\u00a0<cit.> represent one of the primary technical approaches. These methods extract text, layout, and bounding box information from external systems and integrate it with another model. However, they are prone to error propagation and high processing times due to their reliance on multiple components.Benefitting from the rapid advancements in LLMs, OCR-free methods have also achieved great progress. Donut\u00a0<cit.> is the first end-to-end training framework based on a Transformer without requiring OCR engines or APIs. Subsequent works <cit.> propose diverse modifications in model architectures and training algorithms.However, these models are designed for specific tasks and lack general abilities. \u00a7.\u00a7 Long-Context Large Language ModelsWith advancements in engineering, architecture, and algorithms, long-context large language models have made substantial progress. Techniques such as Flash Attention <cit.> and Ring Attention <cit.> have notably reduced GPU memory usage for training on extended contexts. Additionally, various sparse attention mechanisms\u2014including Shifted Sparse Attention\u00a0<cit.>, Dilated Attention\u00a0<cit.>, and Attention Sinks\u00a0<cit.>\u2014have enabled efficient scaling to handle larger contexts. New positional embedding methods, like ALiBi\u00a0<cit.>, xPOS\u00a0<cit.>, and RoPE\u00a0<cit.>, further enhance the models\u2019 generalization capabilities in length extrapolation.However, these advancements remain largely confined to natural language processing, and methods to extend the context size of MLLMs are still under-explored.Another research approach aims to reduce context size by leveraging retrieval augmented generation (RAG)\u00a0<cit.>, where only the most relevant passages are retrieved and fed into the generation model. However, this retrieval-based approach can disrupt the coherence of the semantic chain, particularly in complex reasoning tasks, due to fragmented information flow.In this work, we integrate the above engineering techniques into MLLMs and demonstrate that a model fine-tuned on a high-quality, long-context training corpus is a strong baseline, achieving superior performance compared to its RAG counterpart.    {[Graphic src=\"figure/example.pdf\"]}    [Caption]{Visualization of an example from {Doc-750K}. The left side presents the interleaved text-image format data obtained through Document Content Extraction, while the right side showcases the annotations generated via Question-Answer Pairs Construction.}    [Label id=\"fig:data_example\"]",
                "max_rougeL_score": 1.0,
                "evidence_length": 476
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "\u2026Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot. Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.",
                "gold_paragraph": "\u00a7 INTRODUCTION[Label id=\"sec:intro\"]In recent years, multimodal large language models (MLLMs)\u00a0<cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks\u00a0<cit.>, particularly image-level tasks, such as image captioning\u00a0<cit.>, optical character recognition (OCR)\u00a0<cit.>, and visual question answering (VQA)\u00a0<cit.>.Despite these advances, current MLLMs still face significant challenges in document-level understanding\u00a0<cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.    {[Graphic src=\"figure/teaser.pdf\"]}    [Caption]{Accuracy v.s inference latency on MM-NIAH.     The proposed Docopilot-8B shows a notable improvement over baseline models\u00a0<cit.>, achieving a +19.9    [Label id=\"fig:teaser\"]    Current research on long-content understanding primarily focuses on text-only models\u00a0<cit.>, targeting specific retrieval tasks such as \u201cNeedle in a Haystack\u201d (NIAH)\u00a0<cit.>.However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.Retrieval-augmented generation (RAG) methods\u00a0<cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.To address these problems, two primary challenges need to be considered.(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.Building upon this dataset, we developed a native baseline model for document-level multimodal understanding\u2013Docopilot.Unlike existing approaches\u00a0<cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.As shown in Figure [Ref id=\"fig:teaser\"], this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.The main contributions are summarized as follows:(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. This approach greatly improves its ability to integrate and comprehend information across multi-page documents.(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. As shown in Figure [Ref id=\"fig:teaser\"], Docopilot-8B achieves a score of 61.8 on MM-NIAH\u00a0<cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.}",
                "max_rougeL_score": 1.0,
                "evidence_length": 360
            }
        ]
    },
    {
        "question": "Summarize the M-DocEval framework: What dimensions does it evaluate, how are TS and IS computed (including their components), and how are IF, TS, and IS combined into the final Total score? Reference any cited figures/tables that contextualize these metrics.",
        "intent": [
            "Descriptive",
            "Procedural",
            "Verificative"
        ],
        "num_sections": 1,
        "avg_evidence_len": 606.4,
        "evidences": [
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities. Specific details regarding text and image reference evaluation can be found in Appendices Figure [Ref id=\"fig:case_en_2\"] and [Ref id=\"fig:case_en_1\"].",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 476
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "Textual Content Evaluation. The text summary evaluation focuses on two primary aspects: (1) Completeness (Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy (Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score (TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality. Specific prompt details can be found in Appendix Figure [Ref id=\"fig:prompt_com\"] and  [Ref id=\"fig:prompt_acc\"].",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 795
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy (NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy (ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate (OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity (JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score (IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 1009
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability (IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 370
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 382
            }
        ]
    },
    {
        "question": "Provide an interleaved summary of the M-DocSum task and benchmark design: inputs, interleaved output structure (four paragraphs and image referencing rules), document selection (domains and time), authoritative images, and the two key advantages over prior work, citing any referenced figures/tables.",
        "intent": [
            "Descriptive",
            "Procedural",
            "Comparative"
        ],
        "num_sections": 2,
        "avg_evidence_len": 407.1111111111111,
        "evidences": [
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark (M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents.",
                "gold_paragraph": "\u00a7 INTRODUCTIONThe ability to generate concise, interleaved image-text summaries is crucial for a wide range of real-world applications\u00a0<cit.>. Imagine automatically generating accessible presentations from complex scientific papers, producing interactive reports from data-heavy analyses, or even crafting engaging social media posts summarizing lengthy articles. These scenarios demand a deep understanding of not only the textual content but also the visual information and the intricate relationships between them. However, despite the importance of this capability, the field of interleaved image-text document summarization remains largely unexplored\u00a0<cit.>. There is a critical lack of suitable benchmarks to evaluate the performance of Large Vision-Language Models (LVLMs) in this challenging setting, and consequently, a dearth of effective methodologies tailored for this task.Therefore, a crucial question remains: Do LVLMs genuinely comprehend interleaved image-text in documents? Addressing this question is vital for advancing document understanding towards more complex and practical applications.Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering\u00a0(VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing\u00a0<cit.>.While some recent efforts have explored multi-page document understanding\u00a0<cit.>, their datasets often lack sufficient complexity in terms of cross-page and cross-element reasoning, whereas the recent MMLongBench-Doc\u00a0<cit.> proves excessively difficult, resulting in universally poor performance across models.To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark\u00a0(M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents. M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents.Each paragraph has at most one accompanying image, and each image appears only once. Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure\u00a0[Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.To facilitate this Benchmark, we develop an automated framework for constructing interleaved summaries which is subjected to a rigorous development process including cross-validation of human experts. Subsequently, we propose a fine-grained evaluation method namely M-DocEval. This method centers on three primary dimensions:(1)Textual Content: M-DocEval evaluates the completeness and accuracy of the textual summary. (2)Visual Reference: This dimension assesses the precision and overall effectiveness of image referencing.(3)Instruction Following: This metric evaluates how well the model follows instructions.Furthermore, M-DocEval ensures reliability and consistency through a rigorous process, leveraging a powerful LLM such as GPT-4o.We conduct experiments on M-DocSum-Bench with several leading closed-source models (Gemini Pro\u00a0<cit.>, GPT-4o\u00a0<cit.>, Claude-3.5-Sonnet\u00a0<cit.>, etc.) and powerful open-source models (Qwen2.5-VL-72B\u00a0<cit.>, Qwen2-VL-72B\u00a0<cit.>, InternVL2.5-8B\u00a0<cit.>, etc.).The evaluation results reveal a significant gap between the document summarization capabilities of LVLMs and humans.We also find that while LVLMs can process individual images and text segments, they struggle to maintain coherence and accurately integrate information within long interleaved contexts, particularly in image understanding, and often exhibiting confusion between similar images and a lack of robustness.To explore the performance boundary, we further develop a robust summarization baseline, i.e., M-DocSum-7B based Qwen2-vl-7B,Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization\u00a0(DPO)\u00a0<cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.This demonstrates the potential of LVLMs for enhancing interleaved image-text understanding and provides new insights for the document understanding community.Our contributions are summarized as follows:  * We introduce a novel and challenging M-DocSum-Bench, addressing a critical gap in the current landscape.  * We propose an automated evaluation method M-DocEval, providing a realistic and objective assessment.  * We develop a robust summarization baseline M-DocSum-7B, providing new insights for document understanding.",
                "max_rougeL_score": 1.0,
                "evidence_length": 306
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.",
                "gold_paragraph": "\u00a7 INTRODUCTIONThe ability to generate concise, interleaved image-text summaries is crucial for a wide range of real-world applications\u00a0<cit.>. Imagine automatically generating accessible presentations from complex scientific papers, producing interactive reports from data-heavy analyses, or even crafting engaging social media posts summarizing lengthy articles. These scenarios demand a deep understanding of not only the textual content but also the visual information and the intricate relationships between them. However, despite the importance of this capability, the field of interleaved image-text document summarization remains largely unexplored\u00a0<cit.>. There is a critical lack of suitable benchmarks to evaluate the performance of Large Vision-Language Models (LVLMs) in this challenging setting, and consequently, a dearth of effective methodologies tailored for this task.Therefore, a crucial question remains: Do LVLMs genuinely comprehend interleaved image-text in documents? Addressing this question is vital for advancing document understanding towards more complex and practical applications.Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering\u00a0(VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing\u00a0<cit.>.While some recent efforts have explored multi-page document understanding\u00a0<cit.>, their datasets often lack sufficient complexity in terms of cross-page and cross-element reasoning, whereas the recent MMLongBench-Doc\u00a0<cit.> proves excessively difficult, resulting in universally poor performance across models.To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark\u00a0(M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents. M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents.Each paragraph has at most one accompanying image, and each image appears only once. Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure\u00a0[Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.To facilitate this Benchmark, we develop an automated framework for constructing interleaved summaries which is subjected to a rigorous development process including cross-validation of human experts. Subsequently, we propose a fine-grained evaluation method namely M-DocEval. This method centers on three primary dimensions:(1)Textual Content: M-DocEval evaluates the completeness and accuracy of the textual summary. (2)Visual Reference: This dimension assesses the precision and overall effectiveness of image referencing.(3)Instruction Following: This metric evaluates how well the model follows instructions.Furthermore, M-DocEval ensures reliability and consistency through a rigorous process, leveraging a powerful LLM such as GPT-4o.We conduct experiments on M-DocSum-Bench with several leading closed-source models (Gemini Pro\u00a0<cit.>, GPT-4o\u00a0<cit.>, Claude-3.5-Sonnet\u00a0<cit.>, etc.) and powerful open-source models (Qwen2.5-VL-72B\u00a0<cit.>, Qwen2-VL-72B\u00a0<cit.>, InternVL2.5-8B\u00a0<cit.>, etc.).The evaluation results reveal a significant gap between the document summarization capabilities of LVLMs and humans.We also find that while LVLMs can process individual images and text segments, they struggle to maintain coherence and accurately integrate information within long interleaved contexts, particularly in image understanding, and often exhibiting confusion between similar images and a lack of robustness.To explore the performance boundary, we further develop a robust summarization baseline, i.e., M-DocSum-7B based Qwen2-vl-7B,Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization\u00a0(DPO)\u00a0<cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.This demonstrates the potential of LVLMs for enhancing interleaved image-text understanding and provides new insights for the document understanding community.Our contributions are summarized as follows:  * We introduce a novel and challenging M-DocSum-Bench, addressing a critical gap in the current landscape.  * We propose an automated evaluation method M-DocEval, providing a realistic and objective assessment.  * We develop a robust summarization baseline M-DocSum-7B, providing new insights for document understanding.",
                "max_rougeL_score": 1.0,
                "evidence_length": 200
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents. Each paragraph has at most one accompanying image, and each image appears only once.",
                "gold_paragraph": "\u00a7 INTRODUCTIONThe ability to generate concise, interleaved image-text summaries is crucial for a wide range of real-world applications\u00a0<cit.>. Imagine automatically generating accessible presentations from complex scientific papers, producing interactive reports from data-heavy analyses, or even crafting engaging social media posts summarizing lengthy articles. These scenarios demand a deep understanding of not only the textual content but also the visual information and the intricate relationships between them. However, despite the importance of this capability, the field of interleaved image-text document summarization remains largely unexplored\u00a0<cit.>. There is a critical lack of suitable benchmarks to evaluate the performance of Large Vision-Language Models (LVLMs) in this challenging setting, and consequently, a dearth of effective methodologies tailored for this task.Therefore, a crucial question remains: Do LVLMs genuinely comprehend interleaved image-text in documents? Addressing this question is vital for advancing document understanding towards more complex and practical applications.Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering\u00a0(VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing\u00a0<cit.>.While some recent efforts have explored multi-page document understanding\u00a0<cit.>, their datasets often lack sufficient complexity in terms of cross-page and cross-element reasoning, whereas the recent MMLongBench-Doc\u00a0<cit.> proves excessively difficult, resulting in universally poor performance across models.To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark\u00a0(M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents. M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents.Each paragraph has at most one accompanying image, and each image appears only once. Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure\u00a0[Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.To facilitate this Benchmark, we develop an automated framework for constructing interleaved summaries which is subjected to a rigorous development process including cross-validation of human experts. Subsequently, we propose a fine-grained evaluation method namely M-DocEval. This method centers on three primary dimensions:(1)Textual Content: M-DocEval evaluates the completeness and accuracy of the textual summary. (2)Visual Reference: This dimension assesses the precision and overall effectiveness of image referencing.(3)Instruction Following: This metric evaluates how well the model follows instructions.Furthermore, M-DocEval ensures reliability and consistency through a rigorous process, leveraging a powerful LLM such as GPT-4o.We conduct experiments on M-DocSum-Bench with several leading closed-source models (Gemini Pro\u00a0<cit.>, GPT-4o\u00a0<cit.>, Claude-3.5-Sonnet\u00a0<cit.>, etc.) and powerful open-source models (Qwen2.5-VL-72B\u00a0<cit.>, Qwen2-VL-72B\u00a0<cit.>, InternVL2.5-8B\u00a0<cit.>, etc.).The evaluation results reveal a significant gap between the document summarization capabilities of LVLMs and humans.We also find that while LVLMs can process individual images and text segments, they struggle to maintain coherence and accurately integrate information within long interleaved contexts, particularly in image understanding, and often exhibiting confusion between similar images and a lack of robustness.To explore the performance boundary, we further develop a robust summarization baseline, i.e., M-DocSum-7B based Qwen2-vl-7B,Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization\u00a0(DPO)\u00a0<cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.This demonstrates the potential of LVLMs for enhancing interleaved image-text understanding and provides new insights for the document understanding community.Our contributions are summarized as follows:  * We introduce a novel and challenging M-DocSum-Bench, addressing a critical gap in the current landscape.  * We propose an automated evaluation method M-DocEval, providing a realistic and objective assessment.  * We develop a robust summarization baseline M-DocSum-7B, providing new insights for document understanding.",
                "max_rougeL_score": 1.0,
                "evidence_length": 295
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure [Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.",
                "gold_paragraph": "\u00a7 INTRODUCTIONThe ability to generate concise, interleaved image-text summaries is crucial for a wide range of real-world applications\u00a0<cit.>. Imagine automatically generating accessible presentations from complex scientific papers, producing interactive reports from data-heavy analyses, or even crafting engaging social media posts summarizing lengthy articles. These scenarios demand a deep understanding of not only the textual content but also the visual information and the intricate relationships between them. However, despite the importance of this capability, the field of interleaved image-text document summarization remains largely unexplored\u00a0<cit.>. There is a critical lack of suitable benchmarks to evaluate the performance of Large Vision-Language Models (LVLMs) in this challenging setting, and consequently, a dearth of effective methodologies tailored for this task.Therefore, a crucial question remains: Do LVLMs genuinely comprehend interleaved image-text in documents? Addressing this question is vital for advancing document understanding towards more complex and practical applications.Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering\u00a0(VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing\u00a0<cit.>.While some recent efforts have explored multi-page document understanding\u00a0<cit.>, their datasets often lack sufficient complexity in terms of cross-page and cross-element reasoning, whereas the recent MMLongBench-Doc\u00a0<cit.> proves excessively difficult, resulting in universally poor performance across models.To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark\u00a0(M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents. M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents.Each paragraph has at most one accompanying image, and each image appears only once. Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure\u00a0[Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.To facilitate this Benchmark, we develop an automated framework for constructing interleaved summaries which is subjected to a rigorous development process including cross-validation of human experts. Subsequently, we propose a fine-grained evaluation method namely M-DocEval. This method centers on three primary dimensions:(1)Textual Content: M-DocEval evaluates the completeness and accuracy of the textual summary. (2)Visual Reference: This dimension assesses the precision and overall effectiveness of image referencing.(3)Instruction Following: This metric evaluates how well the model follows instructions.Furthermore, M-DocEval ensures reliability and consistency through a rigorous process, leveraging a powerful LLM such as GPT-4o.We conduct experiments on M-DocSum-Bench with several leading closed-source models (Gemini Pro\u00a0<cit.>, GPT-4o\u00a0<cit.>, Claude-3.5-Sonnet\u00a0<cit.>, etc.) and powerful open-source models (Qwen2.5-VL-72B\u00a0<cit.>, Qwen2-VL-72B\u00a0<cit.>, InternVL2.5-8B\u00a0<cit.>, etc.).The evaluation results reveal a significant gap between the document summarization capabilities of LVLMs and humans.We also find that while LVLMs can process individual images and text segments, they struggle to maintain coherence and accurately integrate information within long interleaved contexts, particularly in image understanding, and often exhibiting confusion between similar images and a lack of robustness.To explore the performance boundary, we further develop a robust summarization baseline, i.e., M-DocSum-7B based Qwen2-vl-7B,Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization\u00a0(DPO)\u00a0<cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.This demonstrates the potential of LVLMs for enhancing interleaved image-text understanding and provides new insights for the document understanding community.Our contributions are summarized as follows:  * We introduce a novel and challenging M-DocSum-Bench, addressing a critical gap in the current landscape.  * We propose an automated evaluation method M-DocEval, providing a realistic and objective assessment.  * We develop a robust summarization baseline M-DocSum-7B, providing new insights for document understanding.",
                "max_rougeL_score": 1.0,
                "evidence_length": 919
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 552
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection. By matching these indices with the original image sequence, we parse the output into the final interleaved summary format.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 586
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website [https://arxiv.org/] <cit.>, primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 374
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 133
            },
            {
                "section_name": "mdocsumbench",
                "fuzzy_matched_section": "mdocsumbench",
                "true_category": "Proposed Method",
                "content": "For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative. As shown in Table [Ref id=\"tab:statis\"] and Figure [Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject.",
                "gold_paragraph": "\u00a7 M-DOCSUM-BENCH[Graphic src=\"figs/statis-shu\"][Caption]{The quantitative indicators of M-DocSum-Bench display fundamental information such as token length, image count, and document topics.}[Label id=\"fig:1_statisticians\"][Graphic src=\"figs/fig_main\"][Caption]{The illustration of automated data construction pipeline, multi-roll data verification, and two-stage training.}[Label id=\"fig-main\"] \u00a7.\u00a7 Task DefineWe introduce the M-DocSum task, given the extensive demand for comprehending and summarizing lengthy documents such as scientific articles and technical reports, we have selected scientific articles from arXiv.The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information. These two modalities are then fed into the model in an interleaved manner, simulating the natural alternating text-image experience of human readers when engaging with scientific articles. Following model processing, a structured output is generated, including: 1) Summary text: a concise summary of the article's core content divided into four fixed paragraphs, covering essential information such as background, main arguments, experimental results or analysis, and conclusions. 2) Referenced images: the model generates indices for referenced images along with their corresponding captions. The captions show the reasoning when making the selection.By matching these indices with the original image sequence, we parse the output into the final interleaved summary format. This output format not only demands strong text summarization skills from the model but also requires accurate identification and referencing of key images, demonstrating a comprehensive understanding of multimodal information. Additionally, the fixed paragraph structure provides clear criteria for evaluating the generation, facilitating both automated and human assessments. \u00a7.\u00a7 Dataset Collection[Label id=\"sec:3.2\"]To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website\u00a0[<https://arxiv.org/>]\u00a0<cit.>,primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine, given the specialized knowledge required for academic articles. To mitigate the risk of data contamination or memorization by existing models, we limit the publication dates to April 2024 or later. We filter out documents without images and those exceeding 32k text tokens, ultimately collecting 4.2k high-quality academic articles. We carefully select 500 articles for the M-DocSum-Bench for broader benchmark coverage, reserving the remaining 3.7k documents for the training set. We have taken steps to ensure the integrity and accuracy of the multimodal data within this benchmark.  For the evaluation, the images contained within the M-DocSum-Bench are considered authoritative.As shown in Table\u00a0[Ref id=\"tab:statis\"] and Figure\u00a0[Ref id=\"fig:1_statisticians\"], we compile detailed basic information, including document length, number of images, and the proportion of each subject. The collection of this data lays the foundation for subsequent exploration of the performance of LVLMs. \u00a7.\u00a7 Interleaved Summary Generation[Label id=\"sec:3.3\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](a), to create a high-quality benchmark, we design a meticulous automated framework for generating summary texts and selecting referenced images. Initially, we extract key points for each summary paragraph from the original text, adhering to specific rules: ensuring no repetition of information, maintaining atomicity so each point is an independent, indivisible unit, verifying that each point is traceable back to the original text, and limiting the number of key points per paragraph to no more than 10. Once these key points are confirmed, we utilize advanced LLM like GPT-4o to synthesize them into coherent and comprehensive paragraph summaries.For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM.The model first determines if an image is necessary to aid understanding of the summary paragraph. If deemed necessary, it selects the most suitable image based on criteria such as high relevance to the paragraph's content, providing crucial visual support that complements textual information, and ensuring that only one image is referenced per paragraph and each image is used only once. Detailed descriptions of our selection and generation processes are available in Appendix Figure\u00a0[Ref id=\"fig:prompt1\"],\u00a0[Ref id=\"fig:prompt2\"], and\u00a0[Ref id=\"fig:prompt3\"]. \u00a7.\u00a7 Quality Control[Label id=\"sec:3.4\"]As shown in Figure\u00a0[Ref id=\"fig-main\"](right), we introduce Multi-roll Data Verification, a semi-automatic quality control process combining LLMs and human expertise to ensure high benchmark quality. Initially, LLMs perform two rounds of validation: Round 1 checks for necessary key fields and at least one image reference in the summary format, while Round 2 ensures semantic integrity by identifying repetitive or invalid content. Summaries failing these checks are iteratively regenerated. Following LLM validation, domain experts conduct thorough reviews, spending at least 20 minutes on each document to correct any hallucinations or semantic distortions. They also refine image references to align with human reading habits. Additionally, annotators cross-check each other\u2019s work, with primary reviewer resolving any disagreements to maintain consistency. This rigorous process ensures a high-quality, reliable benchmark for advancing multimodal document understanding. \u00a7.\u00a7 M-DocEval[Label id=\"sec:3.5\"]To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.Specific details regarding text and image reference evaluation can be found in Appendices Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Textual Content Evaluation. The text summary evaluation focuses on two primary aspects:(1) Completeness\u00a0(Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy\u00a0(Acc) assesses the correctness of the summary by comparing each sentence to the original content, rewarding matches and penalizing hallucinations, repetitions, or semantic distortions. We compute the Text Score\u00a0(TS) as the F1 score of completeness and accuracy:\\begin{equation}TS = 2 * Com * Acc/Com + Acc.\\end{equation}This balanced score ensures a comprehensive assessment of the textual quality.Specific prompt details can be found in Appendix Figure\u00a0[Ref id=\"fig:prompt_com\"] and \u00a0[Ref id=\"fig:prompt_acc\"].Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy\u00a0(NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy\u00a0(ImgAcc) measures precise image matching for paragraphs requiring images. (3) Overall Matching Rate\u00a0(OMatch) provides an overall assessment of correct image decisions across all paragraphs. (4) Jaccard Similarity\u00a0(JacSim) evaluates the similarity between the model's referenced images and the correct set, excluding \u201cNone\" cases, offering insight into image selection accuracy regardless of placement. The Image Score\u00a0(IS) is computed as the weighted average of Overall Matching Rate and Jaccard Similarity:\\begin{equation}IS = OMatch + JacSim/2.\\end{equation}This score reflects both the correctness of image selection and the overall appropriateness of image references in the summary, ensuring a robust and objective evaluation framework.Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability\u00a0(IF) metric. This metric evaluates how well the model follows instructions by assessing whether it produces appropriate summary texts and image references as specified.To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average:\\begin{equation}Total = \u03b1 * IF + \u03b2 * TS + \u03b3 * IS,\\end{equation}where $\u03b1, \u03b2, \u03b3$ are $0.1, 0.45, 0.45$, respectively. This weighting scheme ensures that while instruction adherence is important, the quality of the textual and visual content remains the primary focus.",
                "max_rougeL_score": 1.0,
                "evidence_length": 299
            }
        ]
    },
    {
        "question": "Explain the two-stage training strategy for M-DocSum-7B, including initialization, Stage-1 instruction-tuning setup, Stage-2 DPO preference construction and filtering, optimization objective, and training resources/hyperparameters. Summarize how this strategy impacts performance.",
        "intent": [
            "Procedural",
            "Descriptive",
            "Evaluative"
        ],
        "num_sections": 2,
        "avg_evidence_len": 441.57142857142856,
        "evidences": [
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "After establishing the InterSum-Bench, as shown in Figure [Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 0.96875,
                "evidence_length": 179
            },
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "During this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured. To achieve this, we utilize the training set generated by the automated framework described in Section [Ref id=\"sec:3.3\"]. We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels. The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 0.991304347826087,
                "evidence_length": 657
            },
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "In this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices. (2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection. (3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 1.0,
                "evidence_length": 753
            },
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section [Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data. A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 1.0,
                "evidence_length": 597
            },
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 1.0,
                "evidence_length": 279
            },
            {
                "section_name": "training",
                "fuzzy_matched_section": "training",
                "true_category": "Proposed Method",
                "content": "We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "gold_paragraph": "\u00a7 TRAININGAfter establishing the InterSum-Bench, as shown in Figure\u00a0[Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.  \u00a7.\u00a7 Stage 1: Instruction-TuningDuring this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.To achieve this, we utilize the training set generated by the automated framework described in Section\u00a0[Ref id=\"sec:3.3\"].We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. For each image, we set the maximum pixels equal to the minimum pixels.The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5. \u00a7.\u00a7 Stage 2: Direct Preference OptimizationIn this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. We utilize the model $\u03c0_\u03b8$ initialized from the first stage. Given an original prompt $x$, $\u03c0_\u03b8$ generates an interleaved summary $y$. We then modify the original prompt to obtain a degraded output $y$ as a negative sample. The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.These perturbations increase task complexity while reducing input information. The corrupted prompt $x$ is entered into $\u03c0_\u03b8$ to generate a new interleaved summary, forming a preference data pair ${x=x, y_w=y, y_l=y}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.To prevent the output $y$ from not being strictly worse than $y$ after being given the prompt $x$, an effective filtering mechanism is necessary. We use M-DocEval proposed in Section\u00a0[Ref id=\"sec:3.5\"], employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.A preference data pair is considered standard if it satisfies the following formula:\\begin{equation}\u0394 TS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 IS > 0\u00a0\u00a0and\u00a0\u00a0\u0394 TS + \u0394 IS > \u03b4,\\end{equation}where the $\u0394 TS$ and $\u0394 IS$ represent the differences in TS and IS between $y$ and $y$, and $\u03b4$ is an adjustable threshold that controls the margin.We generate preference data using the same articles from the training set as in the first stage. The final policy model is optimized using the following loss function:\\begin{equation}\u2112_{DPO} = -   \ud835\udd3c_{(x, y_w, y_l) \u223c\ud835\udc9f}[    log\u03c3( \u03b2log\u03c0_\u03b8(y_w|x)/\u03c0_{ref}(y_w|x) - \u03b2log\u03c0_\u03b8(y_l|x)/\u03c0_{ref}(y_l|x)) ],\\end{equation}where $\u03c0_{ref}(y_w|x)$ denotes the model obtained during the stage 1.We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. Global batch size is set to 64, the learning rate is set to 1e-6, and $\u03b2$ is set to 0.2. All training processes utilize the DeepSpeed acceleration framework for efficiency.",
                "max_rougeL_score": 1.0,
                "evidence_length": 275
            },
            {
                "section_name": "introduction",
                "fuzzy_matched_section": "introduction",
                "true_category": "Introduction or background",
                "content": "Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization (DPO) <cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.",
                "gold_paragraph": "\u00a7 INTRODUCTIONThe ability to generate concise, interleaved image-text summaries is crucial for a wide range of real-world applications\u00a0<cit.>. Imagine automatically generating accessible presentations from complex scientific papers, producing interactive reports from data-heavy analyses, or even crafting engaging social media posts summarizing lengthy articles. These scenarios demand a deep understanding of not only the textual content but also the visual information and the intricate relationships between them. However, despite the importance of this capability, the field of interleaved image-text document summarization remains largely unexplored\u00a0<cit.>. There is a critical lack of suitable benchmarks to evaluate the performance of Large Vision-Language Models (LVLMs) in this challenging setting, and consequently, a dearth of effective methodologies tailored for this task.Therefore, a crucial question remains: Do LVLMs genuinely comprehend interleaved image-text in documents? Addressing this question is vital for advancing document understanding towards more complex and practical applications.Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering\u00a0(VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing\u00a0<cit.>.While some recent efforts have explored multi-page document understanding\u00a0<cit.>, their datasets often lack sufficient complexity in terms of cross-page and cross-element reasoning, whereas the recent MMLongBench-Doc\u00a0<cit.> proves excessively difficult, resulting in universally poor performance across models.To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark\u00a0(M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents. M-DocSum-Bench comprises 500 high-quality paper documents from arXiv primarily across six domains, created in or after April 2024, along with human-preference-aligned multimodal interleaved summaries.To facilitate evaluation and align with real-world application scenarios, these summaries are divided into four paragraphs, each optionally accompanied by a reference image based on the original image contents.Each paragraph has at most one accompanying image, and each image appears only once. Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. As illustrated in Figure\u00a0[Ref id=\"fig:intro\"], the interleaved summarization format enables a clear observation of both image referencing and summarization results, thus providing a foundation for the investigation of model capabilities. (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.To facilitate this Benchmark, we develop an automated framework for constructing interleaved summaries which is subjected to a rigorous development process including cross-validation of human experts. Subsequently, we propose a fine-grained evaluation method namely M-DocEval. This method centers on three primary dimensions:(1)Textual Content: M-DocEval evaluates the completeness and accuracy of the textual summary. (2)Visual Reference: This dimension assesses the precision and overall effectiveness of image referencing.(3)Instruction Following: This metric evaluates how well the model follows instructions.Furthermore, M-DocEval ensures reliability and consistency through a rigorous process, leveraging a powerful LLM such as GPT-4o.We conduct experiments on M-DocSum-Bench with several leading closed-source models (Gemini Pro\u00a0<cit.>, GPT-4o\u00a0<cit.>, Claude-3.5-Sonnet\u00a0<cit.>, etc.) and powerful open-source models (Qwen2.5-VL-72B\u00a0<cit.>, Qwen2-VL-72B\u00a0<cit.>, InternVL2.5-8B\u00a0<cit.>, etc.).The evaluation results reveal a significant gap between the document summarization capabilities of LVLMs and humans.We also find that while LVLMs can process individual images and text segments, they struggle to maintain coherence and accurately integrate information within long interleaved contexts, particularly in image understanding, and often exhibiting confusion between similar images and a lack of robustness.To explore the performance boundary, we further develop a robust summarization baseline, i.e., M-DocSum-7B based Qwen2-vl-7B,Notably, leveraging a progressive two-stage training approach encompassing instruction-tuning and Direct Preference Optimization\u00a0(DPO)\u00a0<cit.>, along with the diverse instruction and preference data generated by our automated framework, the M-DocSum-7B achieves state-of-the-art performance among a range of closed-source and larger open-source models.This demonstrates the potential of LVLMs for enhancing interleaved image-text understanding and provides new insights for the document understanding community.Our contributions are summarized as follows:  * We introduce a novel and challenging M-DocSum-Bench, addressing a critical gap in the current landscape.  * We propose an automated evaluation method M-DocEval, providing a realistic and objective assessment.  * We develop a robust summarization baseline M-DocSum-7B, providing new insights for document understanding.",
                "max_rougeL_score": 1.0,
                "evidence_length": 351
            }
        ]
    },
    {
        "question": "What are the key empirical findings on paragraph-wise performance, context-length sensitivity, and image-quantity effects across models on M-DocSum-Bench? Compare open-source and closed-source trends and reference any figures/tables cited.",
        "intent": [
            "Comparative",
            "Evaluative",
            "Causal"
        ],
        "num_sections": 1,
        "avg_evidence_len": 518.2857142857143,
        "evidences": [
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Figure [Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections. This might be because earlier images are more directly related to the text, while later images could involve more complexity.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 278
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Figure [Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature. Paragraph 2 follows, indicating robust understanding and summarization of specific methods. Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization. The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 784
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Figures [Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 0.9922480620155039,
                "evidence_length": 928
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Figure [Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 444
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Figure [Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 540
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 417
            },
            {
                "section_name": "experiments  analysis",
                "fuzzy_matched_section": "experiments  analysis",
                "true_category": "Experimental Results",
                "content": "Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.",
                "gold_paragraph": "\u00a7 EXPERIMENTS & ANALYSISIn this section, we first evaluate the performance of various models on M-DocSum-Bench using the inference prompt which can be found in the Appendix Figure\u00a0[Ref id=\"fig:prompt_infer\"] and compare M-DocSum-7B with several baselines. Then, we conduct an ablation study to analyze our key training steps and data. \u00a7.\u00a7 Main ResultsAs shown in Table\u00a0[Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24It is also the only model where both Overall Matching and Jaccard Similarity scores exceed 60Some notable results include the exceptional performance in text generation completeness and accuracy for Gemini Pro.Open-source models show that overall scores increase with model parameter size, with the Qwen2.5 series outperforming the Qwen2 series. For InternVL, the results indicate that the model is better at generating high-quality text but neglects the understanding and referencing of the images. Its None Accuracy score is nearly 1, suggesting that almost all predictions are \u201cNone\" which means that no images are selected. \u00a7.\u00a7 Analysis  \u00a7.\u00a7.\u00a7 Paragraph-wise Performance VariationsFigure\u00a0[Ref id=\"fig:analysis\"](a) reveals that models more accurately reference images in earlier paragraphs, with accuracy declining in later sections.This might be because earlier images are more directly related to the text, while later images could involve more complexity. Figure\u00a0[Ref id=\"fig:analysis\"](b) indicates that the task difficulty is well-calibrated with an average score around 0.55, showing distinct difficulty levels across paragraphs. Paragraph 4 scores the highest, reflecting strong summarization capabilities, likely due to its structured and key information-rich nature.Paragraph 2 follows, indicating robust understanding and summarization of specific methods.Paragraphs 1 and 3 (Background and Experimental Design) score lower, possibly because of their extensive detail and background information, challenging models in extraction and summarization.The consistent trend observed across all models indicates that our paragraph and difficulty settings are reasonable and our evaluation is stable, leading to highly consistent results.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Paragraph-wise Performance Variations\"].  \u00a7.\u00a7.\u00a7 Influence of Context Length on PerformanceFigures\u00a0[Ref id=\"fig:analysis\"](c) and (d) illustrate a clear trend: as the context length increases, both text and image scores decrease. Notably, the decline in image scores is more precipitous than that of text scores. This disparity suggests that longer contexts disproportionately challenge the models' ability to accurately reference and integrate relevant images, likely stemming from the increased difficulty of localizing the correct visual information within extensive, multi-page documents. Furthermore, a performance gap is observed between open-source and closed-source models. Open-source models exhibit a steeper performance degradation with increasing context length, while closed-source models demonstrate greater robustness. This difference likely reflects the advantages of closed-source models in terms of larger-scale, higher-quality training data, and potentially more sophisticated training methodologies.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Influence of Context Length on Performance\"].  \u00a7.\u00a7.\u00a7 Impact of Image Quantity on PerformanceFigure\u00a0[Ref id=\"fig:analysis\"](e) reveals a significant decrease in image referencing scores across all models as the number of images per document increases. This highlights the challenge models face in selecting pertinent images for summarization in image-rich contexts. The highest scores are observed with documents containing only 1-3 images, suggesting that correct image selection is facilitated in these less visually complex scenarios.Figure\u00a0[Ref id=\"fig:analysis\"](f) illustrates that leading closed-source models sustain stable text scores irrespective of the image count. This consistency in text generation likely reflects their advanced architectural designs and optimization strategies. Conversely, open-source models, along with certain closed-source models, show a deterioration in text quality as the number of images grows, suggesting limitations in their multimodal integration capabilities, potentially attributable to constraints in model size and training data.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Impact of Image Quantity on Performance\"].[Graphic src=\"figs/fig_analysis\"][Caption]{Quantitative analysis of the performance trends of different models as data characteristics vary, including the different paragraphs in the interleaved summarization, the token length of the original text, and the number of input images.}[Label id=\"fig:analysis\"]  \u00a7.\u00a7.\u00a7 Image Reference BiasNotably, as shown in Table\u00a0[Ref id=\"tab:bias\"], when assuming all output referenced images are \u201cNone\", the OMatch score is 0.325, indicating that only 32.5[Graphic src=\"figs/ablation\"][Caption]{Blue bars represent original image scores, orange bars represent scores after modification, and the green line indicates the decline rate.}[Label id=\"fig:ablation\"] \u00a7.\u00a7 Ablation StudyIn our ablation study, we test the robustness of LVLMs by shuffling the order of images and removing the original abstract, observing the performance under this out-of-distribution (OOD) condition.A fundamental hypothesis is that models should primarily rely on the semantic association between images and text rather than the positional arrangement of images. If a model accurately captures these semantic relationships, its performance should remain stable even when the image order is changed; conversely, it indicates the model may merely have learned specific patterns.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](a), overall, closed-source models or larger models exhibit lower sensitivity to image position, with performance decline rates all below 17.7In contrast, smaller open-source models, such as Qwen2-VL-7B, demonstrate the poorest robustness. After the first stage of training, while overall performance improves, decline rates are reduced, indicating an enhanced understanding of interleaved image-text information, but not eliminating the dependence on image order. With the second stage of training, M-DocSum-7B exhibits even greater robustness. Furthermore, experimental results demonstrate that effective first-stage training is essential, if second-stage training is conducted directly, the model's decline rates actually increase.Another question is whether the model completes the summarization by directly copying the original abstract. To investigate this, we conduct experiments on models of the same size with strong textual capabilities.As shown in Figure\u00a0[Ref id=\"fig:ablation\"](b), the results show that as we progressively train the model, the text score gradually increases while the decline rates decrease.This effectively demonstrates the effectiveness of the training and indicates the continuous improvement of the robustness.Furthermore, compared to the disturbance caused by image changes, the absence of the abstract has a weaker impact on the summarization ability, which indirectly confirms the model's low dependence on the original abstract.Detailed results can be found in the Appendix Section\u00a0[Ref id=\"Ablation\"]. \u00a7.\u00a7 Case StudyWe provide two detailed case studies in the Appendix, Figure\u00a0[Ref id=\"fig:case_en_2\"] and\u00a0[Ref id=\"fig:case_en_1\"].Notably, the order of images presented in the case studies may differ from the order in the abstract. This discrepancy underscores a common challenge in current vision-language tasks: models often struggle to accurately capture fine-grained correspondences between images and text. Furthermore, the case studies clearly illustrate that existing models are prone to confusion when dealing with images that are semantically similar but possess distinct visual details, leading to inaccurate reasoning results.",
                "max_rougeL_score": 1.0,
                "evidence_length": 237
            }
        ]
    }
]