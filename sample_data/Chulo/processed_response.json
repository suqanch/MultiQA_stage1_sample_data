[
    {
        "question": "Summarize the ChuLo pipeline end-to-end: how documents are chunked, how keyphrases are extracted and prioritized, how chunk embeddings are weighted and constructed (referencing the chunk-embedding formula), and how the chunk attention module is trained as illustrated in the framework figure.",
        "intent": [
            "Descriptive",
            "Procedural"
        ],
        "evidence": [
            {
                "section": "§ CHULO",
                "type": "paragraph",
                "content": "We propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies. The method segments documents into non-overlapping chunks, integrates key semantic information using unsupervised keyphrase extraction, and assigns higher weights to keyphrase tokens in chunk representations. These enriched chunks train a Transformer-based chunk attention module, enabling efficient processing of long documents while retaining global and local context. Further details are provided in subsequent subsections, with the framework illustrated in Figure [Ref id=\"fig:framework\"]."
            },
            {
                "section": "§.§ Document Input Chunking",
                "type": "paragraph",
                "content": "Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context. Specifically, we first tokenize the document $D = (t_0, t_1, …, t_{l_D-1})$ and divide it into fixed-length chunks $C_D = (C_0, C_1, …, C_{m-1})$, where $l_D$ is the number of the tokens, each chunk $C$ consists of $n$ tokens, and $m= ⌈{l_D/n}⌉$ is the number of chunks. The incomplete chunks will be padded with the [PAD] tokens. The chunk size $n$ is a hyper-parameter controlling the degree of input length reduction. By grouping tokens this way, we maintain the integrity of the input content while alleviating the computational burden associated with processing long sequences."
            },
            {
                "section": "§.§ Semantic Key Information Extraction",
                "type": "paragraph",
                "content": "The fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding."
            },
            {
                "section": "§.§ Semantic Key Information Extraction",
                "type": "paragraph",
                "content": "To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document’s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints."
            },
            {
                "section": "§.§ Chunk Representation Production",
                "type": "paragraph",
                "content": "After extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. While chunking helps reduce the input length, it may also result in an uneven distribution of meaningful content across chunks. Thus, it is crucial to re-emphasize the importance of these keyphrases within the chunk to maintain semantic integrity. Our approach dynamically adjusts the representation of each chunk by assigning greater importance to keyphrase tokens, enabling the model to focus on the most relevant content during downstream processing."
            },
            {
                "section": "§.§ Chunk Representation Production",
                "type": "paragraph",
                "content": "To achieve this, we label the tokens corresponding to the extracted keyphrases in the original text as keyphrase tokens $T_k$, while other tokens are labelled as non-keyphrase tokens $T_{nk}$. Then, we feed these chunked tokens $t$ into the embedding layer to obtain their embeddings. The chunk embedding $c$ is then computed using a weighted average of these token embeddings, as defined in Formula [Ref id=\"equ:chunkemb\"]:"
            },
            {
                "section": "§.§ Chunk Representation Production",
                "type": "paragraph",
                "content": "Here, $w_t$ represents the weight assigned to each token $t$ in the chunk, where $a$ and $b$ are hyperparameters with $a > b.$ $t$ denotes the embedding of token $t$, and $c$ is the resulting chunk embedding that captures the weighted importance of keyphrase and non-keyphrase tokens. By assigning a higher weight $a$ to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length."
            },
            {
                "section": "§.§ Chunk Representation Production",
                "type": "paragraph",
                "content": "Finally, the chunk embeddings are fed into the Transformer-based model, allowing it to effectively leverage the enhanced chunk representations during long document classification or token-level classification tasks. This method not only preserves the semantic coherence of the document but also allows the model to retain meaningful context and relationships, ultimately enhancing its performance on long document tasks."
            },
            {
                "section": "§.§ Chunk Representation Training",
                "type": "paragraph",
                "content": "In this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. We selected BERT-base according to Table [Ref id=\"tab:backbone ablation\"]. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text, thereby improving its ability to handle long document inputs without losing critical context."
            },
            {
                "section": "§.§ Chunk Representation Training",
                "type": "paragraph",
                "content": "We leverage a Transformer-based backbone model, which is used to initialize the weights of the chunk attention module, as illustrated in Figure [Ref id=\"fig:framework\"]. This chunk attention module is designed to capture the intricate contextual relationships among chunks while retaining the influence of keyphrases. By doing so, the module can better understand local and global semantic patterns, enabling the model to perform robustly across various long document tasks. The chunk embeddings are processed through the chunk attention module to produce refined contextual representations, which are then fed into a classification head to generate the final predictions. Our framework, ChuLo, is adaptable to any transformer-based architecture, from pretrained to large language models, making it versatile for tasks involving long document understanding. Through integrating keyphrase-enhanced chunk representations, the model achieves superior performance in both document classification and token-level tasks, highlighting the effectiveness of our approach in leveraging semantic information to tackle the challenges associated with long document processing."
            }
        ]
    },
    {
        "question": "How does ChuLo compare with Longformer, BERT variants, and large language models on document classification overall and for longer documents (>1024 and >2048 tokens), and what explanations are given for these differences?",
        "intent": [
            "Comparative",
            "Evaluative",
            "Causal"
        ],
        "evidence": [
            {
                "section": "§.§ Document Classification",
                "type": "paragraph",
                "content": "We evaluate the effectiveness of our ChuLo by comparing it with fine-tuned PLMs and previously published baselines <cit.> on several benchmark datasets: HP, LUN, EURLEX57K, and Inverted EURLEX57K. The comparative results are summarized in Table [Ref id=\"tab:overal_performance_comparison\"], with input configurations provided in Table [Ref id=\"tab:usage_of_input\"] and detailed descriptions available in Appendix [Ref id=\"app: baseline input\"]. Our method demonstrates clear superiority on three of the four datasets, achieving a significant improvement of 6.43While our model delivers competitive results on the HP dataset, it trails behind Longformer by a slight margin of 0.0031 in accuracy. This marginal difference corresponds to only one additional correctly classified instance out of a total of 65 test samples."
            },
            {
                "section": "§.§ Document Classification",
                "type": "paragraph",
                "content": "Interestingly, for the other datasets, Longformer underperforms compared to models like BERT variants or CogLTX, which use the first 512 tokens and focus on selecting key sentences. This observation indicates that unfiltered additional content can introduce noise, negatively impacting classification accuracy. In contrast, ChuLo expands the input content and strategically emphasizes key semantic elements during chunk representation. This approach mitigates noise interference, ensuring that only the most relevant information is retained and highlighted."
            },
            {
                "section": "§.§ Document Classification",
                "type": "paragraph",
                "content": "Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges."
            },
            {
                "section": "§.§ Longer Document Classification",
                "type": "paragraph",
                "content": "To further validate the robustness of our model, we evaluate its classification performance across various document length ranges, with a particular focus on longer documents. For this analysis, we consider the documents with more than 1024 tokens and more than 2048 tokens in the test set."
            },
            {
                "section": "§.§ Longer Document Classification",
                "type": "paragraph",
                "content": "We use Longformer and off-the-shelf LLMs, GPT4o and Gemini1.5 pro for comparison. As shown in Table [Ref id=\"tab:Accuracy length intervals\"], our model consistently outperforms others on longer documents in the LUN dataset. Specifically, for documents exceeding 2,048 tokens, ChuLo maintains a higher accuracy compared to all baselines, demonstrating its capacity to handle lengthy inputs effectively. This performance gain can be attributed to our chunk representation’s emphasis on keyphrases, which preserves crucial semantic content even when document length increases."
            },
            {
                "section": "§.§ Longer Document Classification",
                "type": "paragraph",
                "content": "On the HP dataset, ChuLo and Longformer achieve perfect accuracy (1.0) for documents longer than 2,048 tokens. However, for shorter documents (more than 1,024 tokens), ChuLo surpasses Longformer. This improvement is likely due to our chunk representation strategy, which selectively highlights key content rather than averaging information across the entire document. As a result, ChuLo maintains high semantic fidelity, leading to better overall performance even with condensed text inputs."
            },
            {
                "section": "§.§ Longer Document Classification",
                "type": "paragraph",
                "content": "We also benchmarked against newly released LLMs, GPT-4o and Gemini 1.5 Pro, using longer document inputs for both the LUN and HP datasets. On LUN, GPT-4o achieved an accuracy of 0.7143 and Gemini 1.5 Pro scored 0.6531, both surpassing Longformer. However, ChuLo achieved the highest accuracy of 0.7959, showcasing its superiority in handling long documents with diverse content. On the HP dataset, GPT-4o (0.8889) and Gemini 1.5 Pro (0.7778) performed worse than Longformer and ChuLo, both of which achieved a perfect accuracy of 1.0 on the longer documents. This highlights ChuLo’s robustness and consistency in classifying documents with varying length, even compared to advanced language models. The prompt and response samples are in Section [Ref id=\"sec:prompt\"] and [Ref id=\"app:cases\"]. Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths."
            },
            {
                "section": "§.§ Longer Document Classification",
                "type": "paragraph",
                "content": "Overall, these results demonstrate that ChuLo not only outperforms standard PLM baselines and chunking methods on long documents but also remains competitive against the latest large language models. By prioritizing key semantic elements and managing document length, ChuLo maintains stable performance across varying input lengths."
            }
        ]
    },
    {
        "question": "What datasets and evaluation metrics were used for ChuLo, and how were zero-shot prompts applied to large language models during experiments?",
        "intent": [
            "Descriptive",
            "Procedural",
            "Verificative"
        ],
        "evidence": [
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "We evaluate ChuLo on document and token classification tasks to highlight our motivation. While document classification primarily requires global contextual understanding, token classification tasks test the model's ability to retain and utilize detailed token-level information within long documents. We compare it with BERT <cit.> and BERT variants <cit.>, Longformer <cit.>, ToBERT <cit.>, CogLTX <cit.>, ChunkBERT <cit.>, and instructions with LLMs, GPT4o[<https://openai.com/index/hello-gpt-4o/>] and Gemini1.5pro[<https://deepmind.google/technologies/gemini/pro/>]. Baselines Details are listed in Appendix [Ref id=\"app:baseline models\"]."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "Datasets:  We conduct experiments using three(3) datasets for long document classification and two(2) for long document token classification. For document classification, we use HP<cit.>, LUN <cit.>, and Eurlex57k <cit.>. These datasets vary in average document length from 707 to 1147 tokens, enabling us to assess performance on documents of different lengths and complexities. Further details on dataset statistics and splits are available in Appendix [Ref id=\"sec:Appendix:Dataset statistics\"]."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the ‘byarticle’ version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "2) LUN uses for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxes <cit.>. Our goal is to predict the source type for each article."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "For token classification, we use GUM and CoNLL-2012 for Named Entity Recognition (NER) tasks:"
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "1) GUM (Georgetown University Multilayer) is a richly annotated collection of 235 documents across various genres such as academic texts, news, fiction, and interviews <cit.>. GUM’s various linguistic styles and structures make it an excellent benchmark for assessing token-level understanding in lengthy documents, ensuring that the model captures complex entity relationships over extended contexts."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "2) CoNLL-2012 originally designed for coreference resolution, and is adapted for NER in our work <cit.>. We convert the data to a document-level format and select the top-k longest documents in each split, emphasizing the model’s ability to process extended text sequences for token classification tasks."
            },
            {
                "section": "§ EXPERIMENTS SET-UP",
                "type": "paragraph",
                "content": "Metrics and Implementation:  For the HP and LUN datasets, we use accuracy as the evaluation metric, while for Eurlex57k, Inverted Eurlex57k, GUM, and CoNLL-2012, we adopt micro F1. These metrics are chosen to maintain consistency with prior work and facilitate direct comparison."
            },
            {
                "section": "§.§ Prompt Method",
                "type": "paragraph",
                "content": "We employed zero-shot prompting with large language models (LLMs), specifically Gemini 1.5 Pro and GPT4o, in our experiments. The prompts used for each dataset are detailed in Table [Ref id=\"tab:app_prompts_doc_c\"] and [Ref id=\"tab:app_prompts_token_c\"]:"
            }
        ]
    },
    {
        "question": "Why is keyphrase extraction central to ChuLo’s design, and what is the role of the Semantic Keyphrase Prioritization (SKP) algorithm in selecting and scoring keyphrases?",
        "intent": [
            "Causal",
            "Descriptive",
            "Procedural"
        ],
        "evidence": [
            {
                "section": "§.§ Semantic Key Information Extraction",
                "type": "paragraph",
                "content": "The fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. During chunking, the document is divided into smaller segments, which can inadvertently distribute important semantic information unevenly across chunks or even cause it to be diluted. Simply treating each chunk equally may lead to overlooking critical context essential for accurate document classification and token-level understanding."
            },
            {
                "section": "§.§ Semantic Key Information Extraction",
                "type": "paragraph",
                "content": "Identifying and highlighting critical phrases within these chunks ensures that the most relevant information is preserved and emphasized, allowing the model to focus on the core content even within a limited input space. This compensates for the information fragmentation caused by chunking and guides the Transformer’s attention mechanism to prioritize the most informative parts of the text, enhancing the model’s ability to capture the document’s overall meaning and relationships. Thus, extracting keyphrases from chunks is crucial for bridging the gap between document segmentation and semantic coherence, ultimately improving the effectiveness of the chunk-based representation for long document understanding."
            },
            {
                "section": "§.§ Semantic Key Information Extraction",
                "type": "paragraph",
                "content": "To achieve this, we extract semantically important keyphrases to identify the core content of the entire document. Since document understanding, such as document classification or token classification, inherently involves semantic understanding, it is crucial to highlight the most informative parts of the text to create meaningful chunk representations. By making the extracted keyphrases more salient, we can effectively emphasize the content that contributes most to the document’s overall meaning. Hence, we employ unsupervised keyphrase extraction methods, ensuring our approach remains adaptable across diverse domains without requiring annotated data. Building on the principles of PromptRank <cit.>, we adapt and enhance its template-based approach to prioritize keyphrases that are contextually significant across the entire document. Our modified strategy, the Semantic Keyphrase Prioritization (SKP) Algorithm, leverages prompts to assess the importance of each candidate keyphrase, ensuring that semantically crucial information is highlighted for downstream document understanding. The details of this process are provided in Appendix [Ref id=\"app:algorithm\"]. In this way, our method emphasizes keyphrases during chunk representation, making critical semantic information more salient. Consequently, our method bridges the gap between document segmentation and semantic coherence by ensuring that key content is preserved and highlighted within the entire document, despite input length constraints."
            },
            {
                "section": "§.§ Keyphrase Extraction",
                "type": "paragraph",
                "content": "We employ the Semantic Keyphrase Prioritization (SKP) algorithm to extract keyphrases that encapsulate the key semantic information of the entire document. The detailed are shown in Algorithm [Ref id=\"alg:algorithm\"]."
            },
            {
                "section": "§.§ Keyphrase Extraction",
                "type": "paragraph",
                "content": "While PromptRank uses prompts to rank keyphrases across the first segment of the document determined by its encoder model, our SKP applies this concept at the entire document level to ensure that each chunk can preserve the most informative content for the entire document. After obtaining the sorted phrases set $K_s$, we select top-n phrases as the keyphrases of the document, which can be regarded as ranked phrases according to their contextual significance within the entire document."
            },
            {
                "section": "§ LIMITATION",
                "type": "paragraph",
                "content": "There are several opportunities for future work, including extending the chunk representation to generative tasks such as long text generation, where chunk representation may extend the LLM's context range limitation and enhance generation quality. However, the performance of the keyphrase extraction method poses a potential risk, as its quality directly affects the overall effectiveness of the approach. We believe this work offers valuable insights into long text understanding and lays a foundation for advancements in related tasks."
            }
        ]
    },
    {
        "question": "What do the tables and figures report about ChuLo’s NER performance across document length ranges on CoNLL and GUM, how does it compare to Longformer, BigBird, and LLMs, and what explanations are provided for its advantages?",
        "intent": [
            "Comparative",
            "Evaluative",
            "Causal",
            "Verificative"
        ],
        "evidence": [
            {
                "section": "§.§ Token Classification",
                "type": "paragraph",
                "content": "To further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task—specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro."
            },
            {
                "section": "§.§ Token Classification",
                "type": "paragraph",
                "content": "As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification."
            },
            {
                "section": "§.§ Token Classification",
                "type": "paragraph",
                "content": "All baselines struggle with these longer inputs due to their limited capacity for handling extensive sequences. In contrast, our method’s ability to encode the entire document’s context through keyphrase-based chunk representations enables it to achieve higher accuracy in recognizing and classifying named entities. This is particularly evident in cases where long-distance dependencies and contextual nuances play a critical role in determining the correct labels."
            },
            {
                "section": "§.§ Token Classification",
                "type": "paragraph",
                "content": "Overall, the results indicate that our model's chunk representation not only enhances performance on document-level classification tasks but also proves highly effective for token-level tasks such as NER, "
            },
            {
                "section": "§.§ Token Classification in Longer Documents",
                "type": "paragraph",
                "content": "We further analyze the NER performance across different document length ranges. As presented in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"], we report the number of documents exceeding specific length thresholds and their corresponding performance metrics. On the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"] visualize the performance breakdown across varying length ranges. For the CoNLL, our model maintains high performance in all length intervals, while Longformer and BigBird exhibit comparable performance within the [1k-2k) range but degrade significantly for longer texts, even for documents that do not exceed their maximum input length. This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model’s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents."
            },
            {
                "section": "§.§ Token Classification in Longer Documents",
                "type": "paragraph",
                "content": "For the GUM, where document lengths are shorter (up to 1.3k tokens), our model consistently outperforms Longformer and BigBird in all intervals. The stable performance of all models on GUM aligns with the results on CoNLL, further confirming that our approach’s chunk representation is particularly effective when documents reach lengths that exceed the standard input capacities of the baselines."
            },
            {
                "section": "§.§ Token Classification in Longer Documents",
                "type": "paragraph",
                "content": "These results underscore the effectiveness of our chunk representation, which emphasizes keyphrase information, for coarse-grained document classification and fine-grained token-level classification tasks like NER. The ability to maintain performance across varying document lengths highlights the importance of incorporating global contextual information in NER tasks—a largely underexplored aspect. Additionally, off-the-shelf LLMs such as GPT-4o and Gemini 1.5 Pro show suboptimal performance on NER tasks without fine-tuning, and their performance deteriorates further as document length increases. This indicates that, despite their advancements, LLMs still require substantial optimization for effective long document understanding."
            },
            {
                "section": "§.§ Prompt Method",
                "type": "paragraph",
                "content": "Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"]. For instance, in Figure [Ref id=\"case9\"], both GPT4o and Gemini1.5pro only predicted a single correct label, “O”. Moreover, the models often fail to predict a sufficient number of token labels for longer inputs, or they repeatedly predict all “O” labels or redundant label sequences. These inconsistencies suggest that LLMs struggle to generate outputs matching the input length in token classification, highlighting substantial room for improvement in this area."
            }
        ]
    }
]