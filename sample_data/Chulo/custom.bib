% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{kong-etal-2023-promptrank,
    title = "{P}rompt{R}ank: Unsupervised Keyphrase Extraction Using Prompt",
    author = "Kong, Aobo  and
      Zhao, Shiwan  and
      Chen, Hao  and
      Li, Qicheng  and
      Qin, Yong  and
      Sun, Ruiqi  and
      Bai, Xiaoyan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.545",
    doi = "10.18653/v1/2023.acl-long.545",
    pages = "9788--9801",
    abstract = "The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to document embeddings. However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning. To this end, in this paper, we propose a simple yet effective unsupervised approach, PromptRank, based on the PLM with an encoder-decoder architecture. Specifically, PromptRank feeds the document into the encoder and calculates the probability of generating the candidate with a designed prompt by the decoder. We extensively evaluate the proposed PromptRank on six widely used benchmarks. PromptRank outperforms the SOTA approach MDERank, improving the F1 score relatively by 34.18{\%}, 24.87{\%}, and 17.57{\%} for 5, 10, and 15 returned results, respectively. This demonstrates the great potential of using prompt for unsupervised keyphrase extraction. We release our code at \url{https://github.com/HLT-NLP/PromptRank}.",
}
@inproceedings{hasan-ng-2014-automatic,
    title = "Automatic Keyphrase Extraction: A Survey of the State of the Art",
    author = "Hasan, Kazi Saidul  and
      Ng, Vincent",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1119",
    doi = "10.3115/v1/P14-1119",
    pages = "1262--1273",
}
@inproceedings{witten1999kea,
  title={KEA: Practical automatic keyphrase extraction},
  author={Witten, Ian H and Paynter, Gordon W and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G},
  booktitle={Proceedings of the fourth ACM conference on Digital libraries},
  pages={254--255},
  year={1999}
}
@article{el2009kp,
  title={KP-Miner: A keyphrase extraction system for English and Arabic documents},
  author={El-Beltagy, Samhaa R and Rafea, Ahmed},
  journal={Information systems},
  volume={34},
  number={1},
  pages={132--144},
  year={2009},
  publisher={Elsevier}
}
@inproceedings{liu2009clustering,
  title={Clustering to find exemplar terms for keyphrase extraction},
  author={Liu, Zhiyuan and Li, Peng and Zheng, Yabin and Sun, Maosong},
  booktitle={Proceedings of the 2009 conference on empirical methods in natural language processing},
  pages={257--266},
  year={2009}
}
@article{campos2020yake,
  title={YAKE! Keyword extraction from single documents using multiple local features},
  author={Campos, Ricardo and Mangaravite, V{\'\i}tor and Pasquali, Arian and Jorge, Al{\'\i}pio and Nunes, C{\'e}lia and Jatowt, Adam},
  journal={Information Sciences},
  volume={509},
  pages={257--289},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{won2019automatic,
  title={Automatic extraction of relevant keyphrases for the study of issue competition},
  author={Won, Miguel and Martins, Bruno and Raimundo, Filipa},
  booktitle={International Conference on Computational Linguistics and Intelligent Text Processing},
  pages={648--669},
  year={2019},
  organization={Springer}
}
@inproceedings{mihalcea2004textrank,
  title={Textrank: Bringing order into text},
  author={Mihalcea, Rada and Tarau, Paul},
  booktitle={Proceedings of the 2004 conference on empirical methods in natural language processing},
  pages={404--411},
  year={2004}
}
@article{brin1998anatomy,
  title={The anatomy of a large-scale hypertextual web search engine},
  author={Brin, Sergey and Page, Lawrence},
  journal={Computer networks and ISDN systems},
  volume={30},
  number={1-7},
  pages={107--117},
  year={1998},
  publisher={Elsevier}
}
@inproceedings{wan2008single,
  title={Single document keyphrase extraction using neighborhood knowledge.},
  author={Wan, Xiaojun and Xiao, Jianguo},
  year={2008}
}
@inproceedings{florescu2017positionrank,
  title={Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents},
  author={Florescu, Corina and Caragea, Cornelia},
  booktitle={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers)},
  pages={1105--1115},
  year={2017}
}

@inproceedings{bougouin2013topicrank,
  title={Topicrank: Graph-based topic ranking for keyphrase extraction},
  author={Bougouin, Adrien and Boudin, Florian and Daille, B{\'e}atrice},
  booktitle={International joint conference on natural language processing (IJCNLP)},
  pages={543--551},
  year={2013}
}
@article{yu2018wikirank,
  title={Wikirank: Improving keyphrase extraction based on background knowledge},
  author={Yu, Yang and Ng, Vincent},
  journal={arXiv preprint arXiv:1803.09000},
  year={2018}
}
@inproceedings{ferragina2010tagme,
  title={Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)},
  author={Ferragina, Paolo and Scaiella, Ugo},
  booktitle={Proceedings of the 19th ACM international conference on Information and knowledge management},
  pages={1625--1628},
  year={2010}
}
@inproceedings{bennani2018simple,
  title={Simple Unsupervised Keyphrase Extraction using Sentence Embeddings},
  author={Bennani-Smires, Kamil and Musat, Claudiu and Hossmann, Andreea and Baeriswyl, Michael and Jaggi, Martin},
  booktitle={Proceedings of the 22nd Conference on Computational Natural Language Learning},
  pages={221--229},
  year={2018}
}
@article{sun2020sifrank,
  title={SIFRank: a new baseline for unsupervised keyphrase extraction based on pre-trained language model},
  author={Sun, Yi and Qiu, Hangping and Zheng, Yu and Wang, Zhongwei and Zhang, Chaoran},
  journal={IEEE Access},
  volume={8},
  pages={10896--10906},
  year={2020},
  publisher={IEEE}
}
@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}
@article{ivgi-etal-2023-efficient,
    title = "Efficient Long-Text Understanding with Short-Text Models",
    author = "Ivgi, Maor  and
      Shaham, Uri  and
      Berant, Jonathan",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.17",
    doi = "10.1162/tacl_a_00547",
    pages = "284--299",
    abstract = "Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.",
}
@inproceedings{hassani2023neighborhood,
  title={Neighborhood attention transformer},
  author={Hassani, Ali and Walton, Steven and Li, Jiachen and Li, Shen and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6185--6194},
  year={2023}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}
@inproceedings{pappagari2019hierarchical,
  title={Hierarchical transformers for long document classification},
  author={Pappagari, Raghavendra and Zelasko, Piotr and Villalba, Jes{\'u}s and Carmiel, Yishay and Dehak, Najim},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={838--844},
  year={2019},
  organization={IEEE}
}
@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}
@inproceedings{gong2020recurrent,
  title={Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension},
  author={Gong, Hongyu and Shen, Yelong and Yu, Dian and Chen, Jianshu and Yu, Dong},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6751--6761},
  year={2020}
}
@inproceedings{zhao2021ror,
  title={RoR: Read-over-Read for Long Document Machine Reading Comprehension},
  author={Zhao, Jing and Bao, Junwei and Wang, Yifan and Zhou, Yongwei and Wu, Youzheng and He, Xiaodong and Zhou, Bowen},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={1862--1872},
  year={2021}
}
@inproceedings{zhang2022summn,
  title={SummN: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents},
  author={Zhang, Yusen and Ni, Ansong and Mao, Ziming and Wu, Chen Henry and Zhu, Chenguang and Deb, Budhaditya and Awadallah, Ahmed and Radev, Dragomir and Zhang, Rui},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1592--1604},
  year={2022}
}
@inproceedings{kiesel2019semeval,
  title={Semeval-2019 task 4: Hyperpartisan news detection},
  author={Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
  pages={829--839},
  year={2019}
}
@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}
@inproceedings{rashkin-etal-2017-truth,
    title = "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking",
    author = "Rashkin, Hannah  and
      Choi, Eunsol  and
      Jang, Jin Yea  and
      Volkova, Svitlana  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1317",
    doi = "10.18653/v1/D17-1317",
    pages = "2931--2937",
}
@inproceedings{greene2006practical,
  title={Practical solutions to the problem of diagonal dominance in kernel document clustering},
  author={Greene, Derek and Cunningham, P{\'a}draig},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={377--384},
  year={2006}
}
@inproceedings{pang2005seeing,
  title={Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales},
  author={Pang, Bo and Lee, Lillian},
  booktitle={Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05)},
  pages={115--124},
  year={2005}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}
@inproceedings{park2022efficient,
  title={Efficient Classification of Long Documents Using Transformers},
  author={Park, Hyunji and Vyas, Yogarshi and Shah, Kashif},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={702--709},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{yu2019review,
  title={A review of recurrent neural networks: LSTM cells and network architectures},
  author={Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  journal={Neural computation},
  volume={31},
  number={7},
  pages={1235--1270},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{li2021survey,
  title={A survey of convolutional neural networks: analysis, applications, and prospects},
  author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}
@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@article{choromanski2020masked,
  title={Masked language modeling for proteins via linearly scalable long-context transformers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Belanger, David and Colwell, Lucy and others},
  journal={arXiv preprint arXiv:2006.03555},
  year={2020}
}
@article{hutchins2022block,
  title={Block-recurrent transformers},
  author={Hutchins, DeLesley and Schlag, Imanol and Wu, Yuhuai and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33248--33261},
  year={2022}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{peng2021random,
  title={Random Feature Attention},
  author={Peng, H and Pappas, N and Yogatama, D and Schwartz, R and Smith, N and Kong, L},
  booktitle={International Conference on Learning Representations (ICLR 2021)},
  year={2021}
}
@inproceedings{chen2020compressed,
  title={Compressed Self-Attention for Deep Metric Learning with Low-Rank Approximation.},
  author={Chen, Ziye and Gong, Mingming and Ge, Lingjuan and Du, Bo}
}
@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating self-attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14138--14148},
  year={2021}
}
@inproceedings{liu2018generating,
  title={Generating Wikipedia by Summarizing Long Sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International conference on machine learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}
@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2441--2453},
  year={2021}
}
@inproceedings{peng2022abc,
  title={ABC: Attention with Bounded-memory Control},
  author={Peng, H and Kasai, J and Pappas, N and Yogatama, D and Wu, Z and Kong, L and Schwartz, R and Smith, N},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2022},
  organization={Association for Computational Linguistics.}
}
@inproceedings{guo2019star,
  title={Star-Transformer},
  author={Guo, Qipeng and Qiu, Xipeng and Liu, Pengfei and Shao, Yunfan and Xue, Xiangyang and Zhang, Zheng},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1315--1325},
  year={2019}
}
@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={Proceedings of the 37th International Conference on Machine Learning},
  pages={9438--9447},
  year={2020}
}
@inproceedings{kitaev2019reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{li2023towards,
  title={Towards Summarizing Multiple Documents with Hierarchical Relationships},
  author={Li, Miao and Hovy, Eduard and Lau, Jey Han},
  journal={arXiv preprint arXiv:2305.01498},
  year={2023}
}
@inproceedings{rashkin2017truth,
  title={Truth of varying shades: Analyzing language in fake news and political fact-checking},
  author={Rashkin, Hannah and Choi, Eunsol and Jang, Jin Yea and Volkova, Svitlana and Choi, Yejin},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={2931--2937},
  year={2017}
}
@article{bamman2013new,
  title={New alignment methods for discriminative book summarization},
  author={Bamman, David and Smith, Noah A},
  journal={arXiv preprint arXiv:1305.1319},
  year={2013}
}
@inproceedings{chalkidis2019large,
  title={Large-Scale Multi-Label Text Classification on EU Legislation},
  author={Chalkidis, Ilias and Fergadiotis, Emmanouil and Malakasiotis, Prodromos and Androutsopoulos, Ion},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6314--6322},
  year={2019}
}
@article{ding2020cogltx,
  title={Cogltx: Applying bert to long texts},
  author={Ding, Ming and Zhou, Chang and Yang, Hongxia and Tang, Jie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12792--12804},
  year={2020}
}
@inproceedings{li2023recurrent,
  title={Recurrent Attention Networks for Long-text Modeling},
  author={Li, Xianming and Li, Zongxi and Luo, Xiaotian and Xie, Haoran and Lee, Xing and Zhao, Yingbin and Wang, Fu Lee and Li, Qing},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={3006--3019},
  year={2023}
}
@inproceedings{li-etal-2023-recurrent,
    title = "Recurrent Attention Networks for Long-text Modeling",
    author = "Li, Xianming  and
      Li, Zongxi  and
      Luo, Xiaotian  and
      Xie, Haoran  and
      Lee, Xing  and
      Zhao, Yingbin  and
      Wang, Fu Lee  and
      Li, Qing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.188",
    doi = "10.18653/v1/2023.findings-acl.188",
    pages = "3006--3019",
}
@article{jaiswal2023breaking,
  title={Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT},
  author={Jaiswal, Aman and Milios, Evangelos},
  journal={arXiv preprint arXiv:2310.20558},
  year={2023}
}
@inproceedings{lin-zeldes-2021-wikigum,
    title = {{W}iki{GUM}: Exhaustive Entity Linking for Wikification in 12 Genres},
    author = {Jessica Lin and Amir Zeldes},
    booktitle = {Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 
                 3rd Designing Meaning Representations (DMR) Workshop (LAW-DMR 2021)},
    year = {2021},
    address = {Punta Cana, Dominican Republic},
    url = {https://aclanthology.org/2021.law-1.18},
    pages = {170--175},
}
@inproceedings{pradhan-etal-2013-towards,
    title = "Towards Robust Linguistic Analysis using {O}nto{N}otes",
    author = {Pradhan, Sameer  and
      Moschitti, Alessandro  and
      Xue, Nianwen  and
      Ng, Hwee Tou  and
      Bj{\"o}rkelund, Anders  and
      Uryupina, Olga  and
      Zhang, Yuchen  and
      Zhong, Zhi},
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-3516",
    pages = "143--152",
}
@article{mengliev2024developing,
  title={Developing named entity recognition algorithms for Uzbek: Dataset Insights and Implementation},
  author={Mengliev, Davlatyor and Barakhnin, Vladimir and Abdurakhmonova, Nilufar and Eshkulov, Mukhriddin},
  journal={Data in Brief},
  volume={54},
  pages={110413},
  year={2024},
  publisher={Elsevier}
}
@article{ccetindaug2023named,
  title={Named-entity recognition in Turkish legal texts},
  author={{\c{C}}etinda{\u{g}}, Can and Yaz{\i}c{\i}o{\u{g}}lu, Berkay and Ko{\c{c}}, Aykut},
  journal={Natural Language Engineering},
  volume={29},
  number={3},
  pages={615--642},
  year={2023},
  publisher={Cambridge University Press}
}
@article{dagdelen2024structured,
  title={Structured information extraction from scientific text with large language models},
  author={Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S and Ceder, Gerbrand and Persson, Kristin A and Jain, Anubhav},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={1418},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{yu2023grounded,
  title={Grounded multimodal named entity recognition on social media},
  author={Yu, Jianfei and Li, Ziyan and Wang, Jieming and Xia, Rui},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9141--9154},
  year={2023}
}
@article{wang2023gpt,
  title={Gpt-ner: Named entity recognition via large language models},
  author={Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2304.10428},
  year={2023}
}
@article{ehrmann2023named,
  title={Named entity recognition and classification in historical documents: A survey},
  author={Ehrmann, Maud and Hamdi, Ahmed and Pontes, Elvys Linhares and Romanello, Matteo and Doucet, Antoine},
  journal={ACM Computing Surveys},
  volume={56},
  number={2},
  pages={1--47},
  year={2023},
  publisher={ACM New York, NY}
}
@article{hu2024improving,
  title={Improving large language models for clinical named entity recognition via prompt engineering},
  author={Hu, Yan and Chen, Qingyu and Du, Jingcheng and Peng, Xueqing and Keloth, Vipina Kuttichi and Zuo, Xu and Zhou, Yujia and Li, Zehan and Jiang, Xiaoqian and Lu, Zhiyong and others},
  journal={Journal of the American Medical Informatics Association},
  pages={ocad259},
  year={2024},
  publisher={Oxford University Press}
}
@inproceedings{zhang2023reducing,
  title={Reducing the bias of visual objects in multimodal named entity recognition},
  author={Zhang, Xin and Yuan, Jingling and Li, Lin and Liu, Jianquan},
  booktitle={Proceedings of the Sixteenth ACM international conference on web search and data mining},
  pages={958--966},
  year={2023}
}
@article{park2023web,
  title={Web interface of NER and RE with BERT for biomedical text mining},
  author={Park, Yeon-Ji and Lee, Min-a and Yang, Geun-Je and Park, Soo Jun and Sohn, Chae-Bong},
  journal={Applied Sciences},
  volume={13},
  number={8},
  pages={5163},
  year={2023},
  publisher={MDPI}
}
@article{bhattacharya2023improving,
  title={Improving biomedical named entity recognition through transfer learning and asymmetric tri-training},
  author={Bhattacharya, Medha and Bhat, Swati and Tripathy, Sirshasree and Bansal, Anvita and Choudhary, Monika},
  journal={Procedia Computer Science},
  volume={218},
  pages={2723--2733},
  year={2023},
  publisher={Elsevier}
}
@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{li-etal-2025-midas,
    title = "{MIDAS}: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn {NLU}",
    author = "Li, Yan  and
      Kim, So-Eon  and
      Park, Seong-Bae  and
      Han, Caren",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.445/",
    pages = "7989--8012",
    ISBN = "979-8-89176-195-7",
}

@article{yang2025magic,
  title={MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering},
  author={Yang, Shuo and Luo, Siwen and Han, Soyeon Caren and Hovy, Eduard},
  journal={arXiv preprint arXiv:2503.18491},
  year={2025}
}

@article{Yang_Luo_Han_2025, title={Multimodal Commonsense Knowledge Distillation for Visual Question Answering (Student Abstract)}, volume={39}, url={https://ojs.aaai.org/index.php/AAAI/article/view/35320}, DOI={10.1609/aaai.v39i28.35320}, number={28}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Yang, Shuo and Luo, Siwen and Han, Soyeon Caren}, year={2025}, month={Apr.}, pages={29545-29547} }