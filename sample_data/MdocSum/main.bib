@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@misc{he2024pasa,
      title={PaSa: An LLM Agent for Comprehensive Academic Paper Search}, 
      author={Yichen He and Guanhua Huang and Peiyuan Feng and Yuan Lin and Yuchen Zhang and Hang Li and Weinan E},
      year={2025},
      eprint={2501.10120},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{chen2024internvl,
title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={24185--24198},
year={2024}
}

@misc{doubao,
  author       = {{Doubao Team}},
  title        = {Doubao-vision-pro-32k},
  howpublished = {\url{https://www.volcengine.com/product/doubao}},
  year         = {2025},
  note         = {Accessed: 2025-1-1},
}

@misc{Moonshot,
  author       = {{Moonshot AI}},
  title        = {Moonshot-v1-32k-vision-preview},
  howpublished = {\url{https://platform.moonshot.cn/}},
  year         = {2025},
  note         = {Accessed: 2025-1-1},
}

@misc{step,
  author       = {{StepFun}},
  title        = {Step-1o-vision-32k},
  howpublished = {\url{https://platform.stepfun.com/}},
  year         = {2025},
  note         = {Accessed: 2025-1-1},
}

@article{he2025pasa,
  title={PaSa: An LLM Agent for Comprehensive Academic Paper Search},
  author={He, Yichen and Huang, Guanhua and Feng, Peiyuan and Lin, Yuan and Zhang, Yuchen and Li, Hang and others},
  journal={arXiv preprint arXiv:2501.10120},
  year={2025}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{bai2025qwen2,
  title={Qwen2. 5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@inproceedings{TheC3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={},
  url={https://api.semanticscholar.org/CorpusID:268232499}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{openai_gpt4o,
  title = {GPT-4o Introduction},
  author = {{OpenAI}},
  year = {2024},
  url = {https://openai.com/index/hello-gpt-4o/},
  note = {Accessed: 2024-11-15}
}


@inproceedings{Slidevqa,
  title={Slidevqa: A dataset for document visual question answering on multiple images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13636--13645},
  year={2023}
}

@inproceedings{dude,
  title={Document understanding dataset and evaluation (dude)},
  author={Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19528--19540},
  year={2023}
}


@inproceedings{Infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}

@inproceedings{Docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}

@inproceedings{zhu2022towards,
  title={Towards complex document understanding by discrete reasoning},
  author={Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4857--4866},
  year={2022}
}

@article{Chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}



@inproceedings{kang2024multi,
  title={Multi-page document visual question answering using self-attention scoring mechanism},
  author={Kang, Lei and Tito, Rub{\`e}n and Valveny, Ernest and Karatzas, Dimosthenis},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={219--232},
  year={2024},
  organization={Springer}
}



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = ICCV,
pages = {234--778},
year = 2005
}

@article{zeroscrolls_s1,
  title={ZeroSCROLLS: A zero-shot benchmark for long text understanding},
  author={Shaham, Uri and Ivgi, Maor and Efrat, Avia and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2305.14196},
  year={2023}
}
@article{leval_s2,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Zhao, Xingjian and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2307.11088},
  year={2023}
}
@article{longbench_s3,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}
@article{infinite_s4,
  title={infty Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo Khai and Han, Xu and Thai, Zhen Leng and Wang, Shuo and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2402.13718},
  year={2024}
}
@article{bamboo_s5,
  title={Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models},
  author={Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2309.13345},
  year={2023}
}
@article{loogle_s6,
  title={Loogle: Can long-context language models understand long contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}
@article{searchqa_s7,
  title={Searchqa: A new q\&a dataset augmented with context from a search engine},
  author={Dunn, Matthew and Sagun, Levent and Higgins, Mike and Guney, V Ugur and Cirik, Volkan and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1704.05179},
  year={2017}
}
@inproceedings{question_s9,
  title={Question answering with long multiple-span answers},
  author={Zhu, Ming and Ahuja, Aman and Juan, Da-Cheng and Wei, Wei and Reddy, Chandan K},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3840--3849},
  year={2020}
}

@article{finqa_s10,
  title={Finqa: A dataset of numerical reasoning over financial data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others},
  journal={arXiv preprint arXiv:2109.00122},
  year={2021}
}
@article{docfinqa_s11,
  title={Docfinqa: A long-context financial reasoning dataset},
  author={Reddy, Varshini and Koncel-Kedziorski, Rik and Lai, Viet Dac and Krumdick, Michael and Lovering, Charles and Tanner, Chris},
  journal={arXiv preprint arXiv:2401.06915},
  year={2024}
}

@inproceedings{docvqa_s13,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}
@article{mlongdoc_s14,
  title={M-longdoc: A benchmark for multimodal super-long document understanding and a retrieval-aware tuning framework},
  author={Chia, Yew Ken and Cheng, Liying and Chan, Hou Pong and Liu, Chaoqun and Song, Maojia and Aljunied, Sharifah Mahani and Poria, Soujanya and Bing, Lidong},
  journal={arXiv preprint arXiv:2411.06176},
  year={2024}
}
@article{chartqa_s15,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}
@inproceedings{ocrvqa_s16,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={2019 international conference on document analysis and recognition (ICDAR)},
  pages={947--952},
  year={2019},
  organization={IEEE}
}
@inproceedings{infographicvqa_s17,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}
@article{docbench_s18,
  title={Docbench: A benchmark for evaluating llm-based document reading systems},
  author={Zou, Anni and Yu, Wenhao and Zhang, Hongming and Ma, Kaixin and Cai, Deng and Zhang, Zhuosheng and Zhao, Hai and Yu, Dong},
  journal={arXiv preprint arXiv:2407.10701},
  year={2024}
}

@article{webquest_s19,
  title={Webquest: A benchmark for multimodal qa on web page sequences},
  author={Wang, Maria and Sunkara, Srinivas and Baechler, Gilles and Lin, Jason and Zhu, Yun and Zubach, Fedir and Shu, Lei and Chen, Jindong},
  journal={arXiv preprint arXiv:2409.13711},
  year={2024}
}
@inproceedings{mmvqa_s20,
  title={MMVQA: A comprehensive dataset for investigating multipage multimodal information retrieval in pdf-based visual question answering},
  author={Ding, Yihao and Ren, Kaixuan and Huang, Jiabin and Luo, Siwen and Han, Soyeon Caren},
  booktitle={Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI},
  pages={3--9},
  year={2024}
}
@article{mvqa_s21,
  title={MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering},
  author={Ding, Yihao and Ren, Kaixuan and Huang, Jiabin and Luo, Siwen and Han, Soyeon Caren},
  journal={arXiv preprint arXiv:2404.12720},
  year={2024}
}
@article{needle_s22,
  title={Needle in a Haystack},
  author={Hannah-Moffat, Kelly},
  journal={Criminology \& Pub. Pol'y},
  volume={14},
  pages={113},
  year={2015},
  publisher={HeinOnline}
}
@article{counting_s23,
  title={Counting-stars: A simple, efficient, and reasonable strategy for evaluating long-context large language models},
  author={Song, Mingyang and Zheng, Mao and Luo, Xuan},
  journal={arXiv e-prints},
  pages={arXiv--2403},
  year={2024}
}
@article{ruler_s24,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}
@article{marathon_s25,
  title={Marathon: A race through the realm of long context with large language models},
  author={Zhang, Lei and Li, Yunshui and Liu, Ziqiang and Liu, Junhao and Chen, Longze and Luo, Run and Yang, Min and others},
  journal={arXiv preprint arXiv:2312.09542},
  year={2023}
}
@inproceedings{document_s26,
  title={Document collection visual question answering},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={778--792},
  year={2021},
  organization={Springer}
}
@article{document_s27,
  title={Document visual question answering challenge 2020},
  author={Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  journal={arXiv preprint arXiv:2008.08899},
  year={2020}
}
@inproceedings{towards_s28,
  title={Towards complex document understanding by discrete reasoning},
  author={Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4857--4866},
  year={2022}
}
@article{visualwebbench_s29,
  title={Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?},
  author={Liu, Junpeng and Song, Yifan and Lin, Bill Yuchen and Lam, Wai and Neubig, Graham and Li, Yuanzhi and Yue, Xiang},
  journal={arXiv preprint arXiv:2404.05955},
  year={2024}
}

@inproceedings{document_s31,
  title={Document understanding dataset and evaluation (dude)},
  author={Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19528--19540},
  year={2023}
}
@inproceedings{slidevqa_s32,
  title={Slidevqa: A dataset for document visual question answering on multiple images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13636--13645},
  year={2023}
}
@article{pdftriage_s33,
  title={Pdftriage: Question answering over long, structured documents},
  author={Saad-Falcon, Jon and Barrow, Joe and Siu, Alexa and Nenkova, Ani and Yoon, David Seunghyun and Rossi, Ryan A and Dernoncourt, Franck},
  journal={arXiv preprint arXiv:2309.08872},
  year={2023}
}

@article{financebench_s34,
  title={Financebench: A new benchmark for financial question answering},
  author={Islam, Pranab and Kannappan, Anand and Kiela, Douwe and Qian, Rebecca and Scherrer, Nino and Vidgen, Bertie},
  journal={arXiv preprint arXiv:2311.11944},
  year={2023}
}
@article{hierarchical_s35,
  title={Hierarchical multimodal transformers for multipage docvqa},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  journal={Pattern Recognition},
  volume={144},
  pages={109834},
  year={2023},
  publisher={Elsevier}
}
@article{mmlongbench_s36,
  title={Mmlongbench-doc: Benchmarking long-context document understanding with visualizations},
  author={Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={95963--96010},
  year={2025}
}


@article{xu2020layoutlmv2_ss2,
  title={Layoutlmv2: Multi-modal pre-training for visually-rich document understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
  journal={arXiv preprint arXiv:2012.14740},
  year={2020}
}
@inproceedings{huang2022layoutlmv3_ss3,
  title={Layoutlmv3: Pre-training for document ai with unified text and image masking},
  author={Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  booktitle={Proceedings of the 30th ACM international conference on multimedia},
  pages={4083--4091},
  year={2022}
}

@inproceedings{kim2022ocr_ss4,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle={European Conference on Computer Vision},
  pages={498--517},
  year={2022},
  organization={Springer}
}

@inproceedings{lee2023pix2struct_ss5,
  title={Pix2struct: Screenshot parsing as pretraining for visual language understanding},
  author={Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle={International Conference on Machine Learning},
  pages={18893--18912},
  year={2023},
  organization={PMLR}
}
@article{roqueevolution_ss6,
  title={The Evolution of Llama: From Llama 1 to Llama 3.1},
  author={Roque, Lu{\'\i}s}
}

@inproceedings{dong2024multi_ss8,
  title={Multi-page document VQA with recurrent memory transformer},
  author={Dong, Qi and Kang, Lei and Karatzas, Dimosthenis},
  booktitle={International Workshop on Document Analysis Systems},
  pages={57--70},
  year={2024},
  organization={Springer}
}
@article{jiang2024mantis_ss9,
  title={Mantis: Interleaved multi-image instruction tuning},
  author={Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.01483},
  year={2024}
}

@article{li2024llava_ss10,
  title={Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@article{jia2024leopard_ss11,
  title={Leopard: A vision language model for text-rich multi-image tasks},
  author={Jia, Mengzhao and Yu, Wenhao and Ma, Kaixin and Fang, Tianqing and Zhang, Zhihan and Ouyang, Siru and Zhang, Hongming and Jiang, Meng and Yu, Dong},
  journal={arXiv preprint arXiv:2410.01744},
  year={2024}
}
@article{hu2024mplug_ss12,
  title={mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding},
  author={Hu, Anwen and Xu, Haiyang and Zhang, Liang and Ye, Jiabo and Yan, Ming and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2409.03420},
  year={2024}
}
@article{yu2024visrag_ss13_ss14,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}
@inproceedings{blau2024gram_ss15,
  title={GRAM: Global reasoning for multi-page VQA},
  author={Blau, Tsachi and Fogel, Sharon and Ronen, Roi and Golts, Alona and Ganz, Roy and Ben Avraham, Elad and Aberdam, Aviad and Tsiper, Shahar and Litman, Ron},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15598--15607},
  year={2024}
}

@article{zhang2024read_ss17,
  title={Read and Think: An Efficient Step-wise Multimodal Language Model for Document Understanding and Reasoning},
  author={Zhang, Jinxu},
  journal={arXiv preprint arXiv:2403.00816},
  year={2024}
}
@inproceedings{borgeaud2022improving_ss18,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}
@article{shi2023replug_ss19,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}
@article{dao2023flashattention_ss20,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{liu2023ring_ss21,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@article{jaszczur2021sparse_ss22,
  title={Sparse is enough in scaling transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9895--9907},
  year={2021}
}

@article{chen2023longlora_ss23,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{ding2023longnet_ss24,
  title={Longnet: Scaling transformers to 1,000,000,000 tokens},
  author={Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}

@article{gu2021efficiently_ss25,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
@article{fu2022hungry_ss26,
  title={Hungry hungry hippos: Towards language modeling with state space models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2212.14052},
  year={2022}
}

@article{beck2025xlstm_ss27,
  title={xlstm: Extended long short-term memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={107547--107603},
  year={2025}
}


@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}