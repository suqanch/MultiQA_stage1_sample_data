\section{Training}
After establishing the InterSum-Bench, as shown in Figure~\ref{fig-main}(b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy. 

\input{tabs/0_main}

\subsection{Stage 1: Instruction-Tuning}
During this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.
To achieve this, we utilize the training set generated by the automated framework described in Section~\ref{sec:3.3}.
We train the model for one epoch on the training set using 64 H800 GPUs, configuring all components except the ViT to be trainable. 
For each image, we set the maximum pixels equal to the minimum pixels.
The globe batch size is set to 64, max length is set to 24k, and the learning rate is configured to 1e-5.

\subsection{Stage 2: Direct Preference Optimization}
In this stage, we introduce a novel method for constructing preference data. 
By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model. 

We utilize the model $\pi_\theta$ initialized from the first stage. 
Given an original prompt $x$, $\pi_\theta$ generates an interleaved summary $y$. 
We then modify the original prompt to obtain a degraded output $\widetilde{y}$ as a negative sample. 
The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices.
(2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection.
(3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.
These perturbations increase task complexity while reducing input information. 
The corrupted prompt $\widetilde{x}$ is entered into $\pi_\theta$ to generate a new interleaved summary, forming a preference data pair $\{x=x, y_w=y, y_l=\widetilde{y}\}$, where $y_w, y_l$ are the chosen and rejected sample, respectively.

To prevent the output $\widetilde{y}$ from not being strictly worse than $y$ after being given the prompt $\widetilde{x}$, an effective filtering mechanism is necessary. 
We use M-DocEval proposed in Section~\ref{sec:3.5}, employing Text Score (TS) and Image Score (IS) metrics to screen the preference data.
A preference data pair is considered standard if it satisfies the following formula:
\begin{equation}
    \Delta TS > 0~~\text{and}~~\Delta IS > 0~~and~~\Delta TS + \Delta IS > \delta,
\end{equation}
where the $\Delta TS$ and $\Delta IS$ represent the differences in TS and IS between $y$ and $\widetilde{y}$, and $\delta$ is an adjustable threshold that controls the margin.


We generate preference data using the same articles from the training set as in the first stage. 
The final policy model is optimized using the following loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{DPO}} = -&\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \\ \Big[ &\log \sigma \Big( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \Big) \Big],
\end{aligned}
\end{equation}
where $\pi_{\text{ref}}(y_w|x)$ denotes the model obtained during the stage 1.

We train M-DocSum-7B on 64 H800 GPUs, loading only one preference data pair per GPU per training step. 
Global batch size is set to 64, the learning rate is set to 1e-6, and $\beta$ is set to 0.2. 
All training processes utilize the DeepSpeed acceleration framework for efficiency.