\section{Conclusion}
This paper introduces the M-DocSum-Bench, a novel and challenging Multimodal Document Summarization Benchmark designed to evaluate the ability of LVLMs to understand interleaved image-text documents.
The benchmark requires generating reference-based, interleaved image-text summaries from 500 high-quality arXiv papers, providing a more comprehensive and intuitive assessment than existing benchmarks.
A fine-grained evaluation method M-DocEval is proposed to assess textual content, visual reference accuracy, and instruction following ability, leveraging a powerful LLM for reliable and consistent results. 
Experiments with leading LVLMs reveal a significant performance gap compared to humans, highlighting challenges in maintaining coherence and integrating information in long interleaved contexts.
Finally, a strong summarization baseline, M-DocSum-7B, is developed using a progressive two-stage training approach (instruction-tuning and DPO). This model achieves state-of-the-art performance among various models, demonstrating the potential of LVLMs for enhanced interleaved image-text understanding.

\section*{Acknowledgements}
We thank Yanan Wei, Yingmin Li, Aihu Zhang, Dingxin Wang for engineering support for this work.