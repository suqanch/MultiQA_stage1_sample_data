\section{Related Work}

\subsection{Benchmark for Document Understanding}
Document understanding evaluation has evolved through multiple stages, with studies varying in modality, spatial dimensions, and tasks. Early text-only benchmarks \cite{searchqa_s7, question_s9} like SearchQA, and QuALITY, test long-text processing (1,850-60k tokens) but lack multimodal understanding. ChartQA \cite{chartqa_s15} and others \cite{towards_s28, visualwebbench_s29} 
introduces multimodal inputs, but focuses on isolated components, limiting assessment of comprehensive reasoning.

Most benchmarks have been limited to single-page documents. DocVQA \cite{docvqa_s13}, VisualWebBench \cite{visualwebbench_s29}, OCR-VQA \cite{ocrvqa_s16}, and InfoVQA \cite{infographicvqa_s17} focus on local information extraction (VQA, text extraction). MP-DocVQA \cite{mmlongbench_s36} extends to multi-page, but still avoids cross-page dependencies. SlideVQA \cite{slidevqa_s32} has some cross-page questions (12.9\%) but low information density. DUDE \cite{document_s31} has few cross-page questions (2.1\%) and short contexts (5.3 pages). RULER \cite{ruler_s24} uses distracting information, and FinanceBench \cite{financebench_s34} has long documents and cross-page questions but is domain-specific and relied on costly human evaluation.

Long-context understanding research used synthetic tasks (Needle-in-a-Haystack \cite{needle_s22}, Counting stars \cite{counting_s23}) to test memory, but these lacked real-world complexity. ZeroSCROLLS \cite{zeroscrolls_s1}, LongBench \cite{longbench_s3}, and InfiniteBench \cite{infinite_s4} and others \cite{loogle_s6, marathon_s25} 
offered realistic NLP tasks, but were text-only.

Mmlongbench-doc \cite{mmlongbench_s36} achieved 47.5-page multimodal evaluation, and M-LongDoc \cite{mlongdoc_s14} extended to 210.8 pages with open-ended questions, but lacked refined metrics for locating and summarizing key information. Our proposed DocSum output format addresses this by reflecting the model's overall understanding of interleaved image-text information and comprehensively covers all critical information.

\subsection{Method for Document Understanding}
Document understanding methods have evolved alongside multimodal representation learning and context expansion. Early approaches used cascaded OCR and layout analysis \cite{xu2020layoutlmv2_ss2, huang2022layoutlmv3_ss3}, but this was computationally redundant and not easily optimized end-to-end. OCR-free models \cite{kim2022ocr_ss4,lee2023pix2struct_ss5} addressed this by directly extracting image features and combining them with text embeddings, reducing error accumulation, but still struggled with cross-page visual cues.

To handle long documents, hierarchical encoding \cite{hierarchical_s35,dong2024multi_ss8} was introduced, decomposing documents into page units for separate encoding before LLM processing. However, this could compromise global coherence. Retrieval-augmented generation (RAG) \cite{yu2024visrag_ss13_ss14,blau2024gram_ss15,mmvqa_s20,shi2023replug_ss19} enhanced parsing with external knowledge, and RETRO \cite{borgeaud2022improving_ss18} used chunked cross-attention for long context integration. Recent end-to-end methods \cite{hu2024mplug_ss12} improved long-range dependency modeling with dynamic visual token trimming and large-scale instruction tuning.

In model architecture, advances in long context LVLMs and LLMs have boosted document understanding. Optimizations like sparse attention \cite{liu2023ring_ss21,jaszczur2021sparse_ss22} and state space models \cite{jaszczur2021sparse_ss22,chen2023longlora_ss23,ding2023longnet_ss24} reduced memory overhead, enabling processing of longer contexts. Innovations such as hierarchical representation \cite{gu2021efficiently_ss25}, recursive memory \cite{fu2022hungry_ss26}, and scalable position encoding \cite{beck2025xlstm_ss27} improved capture of spatial-semantic relationships. However, these advancements, while effective for text, face challenges in multi-page multimodal documents, including low cross-modal alignment efficiency and inadequate long-distance visual dependency modeling. We explore LVLM performance boundaries in document summarization using automatically generated preference data, instruction tuning, and DPO.
