{CVPR}2025{2025}{Docopilot}-750K{Doc-750K}{UTF8}{gbsn}{Docopilot: Improving Multimodal Models for Document-Level Understanding}
    {{0.91}{Yuchen Duan$^{1,2*}$, Zhe Chen$^{3, 1*}$, Yusong Hu$^{4,1*}$, Weiyun Wang$^{*5,1}$, Shenglong Ye$^1$, Botian Shi$^1$,}

{0.91}{ Lewei Lu$^7$, Qibin Hou$^4$, Tong Lu$^{3,1}$, Hongsheng Li$^{2,1}$, Jifeng Dai$^{6,1}$, Wenhai Wang$^{2,1}$}
{0.91}{ $^1$Shanghai AI Laboratory, $^2$The Chinese University of Hong Kong, $^3$Nanjing University,}
{0.91}{ $^4$Nankai University, $^5$Fudan University, $^6$Tsinghua University, $^7$SenseTime Research}
}
    August 25, 2025
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================

{* Equal contribution; 
  Corresponding author: wangwenhai@pjlab.org.cn}
Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. 
In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents.
This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents.
Building on the dataset, we develop a native multimodal model—Docopilot, which can accurately handle document-level dependencies without relying on RAG.
Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. 
Data, code, and models are released at <https://github.com/OpenGVLab/Docopilot>.



§ INTRODUCTION

In recent years, multimodal large language models (MLLMs) <cit.> have rapidly developed, achieving remarkable performance in various visual understanding tasks <cit.>, particularly image-level tasks, such as image captioning <cit.>, optical character recognition (OCR) <cit.>, and visual question answering (VQA) <cit.>.
Despite these advances, current MLLMs still face significant challenges in document-level understanding <cit.>, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.


{
    < g r a p h i c s >
}{Accuracy v.s inference latency on MM-NIAH. 
    The proposed Docopilot-8B shows a notable improvement over baseline models <cit.>, achieving a +19.9% accuracy gain compared to InternVL2-8B and surpassing InternVL2-26B with less than 31% of the inference latency. Additionally, Docopilot-2B uses fewer parameters (less than 10%) while exhibiting comparable performance to the 10$×$ larger InternVL2-26B. These results suggest that our Docopilot strikes a reasonable balance between latency, model size, and performance.}


Current research on long-content understanding primarily focuses on text-only models <cit.>, targeting specific retrieval tasks such as “Needle in a Haystack” (NIAH) <cit.>.
However, existing open-source MLLMs <cit.> are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.
Retrieval-augmented generation (RAG) methods <cit.> attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.
(1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; 
(2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;
(3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.



To address these problems, two primary challenges need to be considered.
(1) High-Quality Multimodal Document Dataset. While extensive datasets <cit.> exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.
(2) Native Document-Level MLLMs. Although RAG-based methods <cit.> provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.

In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts <cit.>, this dataset has the following features:
(1) Large Scale. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.
(2) High Quality. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6% of the dataset. 
(3) Multimodal. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.


Building upon this dataset, we developed a native baseline model for document-level multimodal understanding–Docopilot.
Unlike existing approaches <cit.> that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention <cit.>, and Liger Kernel <cit.>.
Leveraging the proxy tasks carefully designed within the dataset, Docopilot can directly handle long-distance dependencies and cross-page information integration without external retrieval support.
As shown in Figure <ref>, this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.

The main contributions are summarized as follows:

(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6% real QA pairs directly extracted from documents.

(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. 
This approach greatly improves its ability to integrate and comprehend information across multi-page documents.

(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. 
As shown in Figure <ref>, Docopilot-8B achieves a score of 61.8 on MM-NIAH <cit.>, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31% of the latency.
We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.




§ RELATED WORK


 §.§ Multimodal Large Language Models


Multimodal large language models (MLLMs) have demonstrated impressive capabilities in processing image and text information, opening up new directions for applications such as visual question answering and image captioning.
Early models <cit.> trained with contrastive learning methods excelled in recognizing and understanding open-world semantics within an image-text matching framework. However, their limited generative abilities restricted their applicability.
To leverage the powerful generation abilities of large language models (LLMs), subsequent works <cit.> introduced a connector to align the embedding spaces of vision encoders and LLMs, allowing encoded image embeddings to serve as soft prompts for LLMs. 
Another series of works <cit.> extended LLMs by integrating additional visual experts, reducing reliance on standalone vision encoders.
More recently, models capable of both understanding and generating images have also made notable progress <cit.>, leveraging the insight that image generation can enhance image understanding.
Despite these advancements, current MLLMs still face challenges with long-context multimodal inputs. For instance, InternVL 2.0 <cit.> performs optimally within a token range of up to 8192, constraining its effectiveness in document-level applications.


{
    < g r a p h i c s >
}{Multimodal document dataset generation pipeline. 
    This pipeline involves three main stages: 
    (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats. 
    (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format. 
    (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o.
    }


 §.§ Document Understanding Models


Extracting key information from documents is crucial for industries and academic research.
OCR-model-driven methods <cit.> represent one of the primary technical approaches. These methods extract text, layout, and bounding box information from external systems and integrate it with another model. However, they are prone to error propagation and high processing times due to their reliance on multiple components.
Benefitting from the rapid advancements in LLMs, OCR-free methods have also achieved great progress. Donut <cit.> is the first end-to-end training framework based on a Transformer without requiring OCR engines or APIs. 
Subsequent works <cit.> propose diverse modifications in model architectures and training algorithms.
However, these models are designed for specific tasks and lack general abilities.




 §.§ Long-Context Large Language Models


With advancements in engineering, architecture, and algorithms, long-context large language models have made substantial progress. Techniques such as Flash Attention <cit.> and Ring Attention <cit.> have notably reduced GPU memory usage for training on extended contexts. Additionally, various sparse attention mechanisms—including Shifted Sparse Attention <cit.>, Dilated Attention <cit.>, and Attention Sinks <cit.>—have enabled efficient scaling to handle larger contexts. 
New positional embedding methods, like ALiBi <cit.>, xPOS <cit.>, and RoPE <cit.>, further enhance the models’ generalization capabilities in length extrapolation.
However, these advancements remain largely confined to natural language processing, and methods to extend the context size of MLLMs are still under-explored.
Another research approach aims to reduce context size by leveraging retrieval augmented generation (RAG) <cit.>, where only the most relevant passages are retrieved and fed into the generation model. However, this retrieval-based approach can disrupt the coherence of the semantic chain, particularly in complex reasoning tasks, due to fragmented information flow.
In this work, we integrate the above engineering techniques into MLLMs and demonstrate that a model fine-tuned on a high-quality, long-context training corpus is a strong baseline, achieving superior performance compared to its RAG counterpart.


{
    < g r a p h i c s >
}{Visualization of an example from {Doc-750K}. The left side presents the interleaved text-image format data obtained through Document Content Extraction, while the right side showcases the annotations generated via Question-Answer Pairs Construction.}


§ MULTIMODAL DOCUMENT DATASET GENERATION


In this section, we begin by introducing the details of the data engine.
Following this, we provide a comprehensive overview of the dataset—Doc-750K.



 §.§ Data Engine

The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.
Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: 



{{0.95}}
 Here, the , , and  are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.

Document Content Extraction.
In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.
In this work, we process each document into two formats as follows:

(1) Interleaved Text-Image Format. Using the document content extractor MinerU <cit.>, we segment the document content into interleaved text and image annotations, for example, 
“”
This format captures the document’s textual content, making it easier to construct question-answer pairs. 


(2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern “”. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.

After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.


Question-Answer Pairs Construction.
In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:

(1) For documents with reliable QA annotations, like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.

{
    < g r a p h i c s >
}{Data distribution of our dataset.
        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.
        Left: Data distribution of {Doc-750K}.
        Right: Data distribution of our complete SFT training dataset. 
        Note that the number reported in the figure represents the number of samples. “MT” is short for multi-turn.
    }


(2) For documents with a clear textual structure, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table <ref>. 

(3) For other documents, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o <cit.>. 


Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8% of the data is LLM-generated, reinforcing the overall reliability and quality of the dataset.


Tasks                1c{Questions}

Abstract Writing         
Read the full text of the paper and provide a concise summary in the form of an abstract.     


Paper Titling           Based on the provided abstract or introduction of the research paper, please generate a concise and informative title          

Caption Writing         Give the relative texts of the images or tables, please write a caption for each image or table based on the relative texts provided.   

Experiment Writing      Please write the "Experiments" section based on the incomplete research paper provided.       

Translation             Please read the full text of the following research paper and translate the Experiments section into Chinese.         
{Questions format for different tasks.
For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.
}


 §.§ Multimodal Document Dataset
Data Source. 
The composition and distribution of our training data are detailed in Figure <ref>. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6% of the total data. The multimodal data, carefully selected to augment our model's learning dimensions, makes up about 88.8% of our dataset. This strategic distribution is designed to optimize the training process and improve the model's ability to generalize across different types of data inputs.

Dataset Statistics.
In our {Doc-750K} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4%. The remaining data, sourced from Sci-Hub, is processed using our designed tasks. The overall distribution and number of tasks are shown in Figure <ref>(a).
Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table <ref>. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table <ref>.



 §.§ Data Recipe for Supervised Fine-Tuning


Although Doc-750K effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.
As shown in Figure <ref>(b), these datasets are organized into 4 categories as follows:

 (1) For multi-page document QA, Doc-750K serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix <cit.>, MP-DocVQA <cit.>, DUDE <cit.>, and Taesiri-ArxivQA <cit.>
offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences. 

 (2) For multi-image general QA, MMDU-45K <cit.> offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.

(3) For single-page document QA, We introduce DocVQA <cit.>, DocReason <cit.>, InfoVQA <cit.>, and ChartQA <cit.> to further enhance the diversity of the SFT dataset.
These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.

(4) For pure-text QA, we add datasets including LongAlpaca <cit.>, LongAlpaca-16K-Length <cit.>, LongQLoRA <cit.>, LongCite <cit.>, LongAlign <cit.>, and LongReward <cit.> to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.


This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.



§ ENHANCED BASELINE FOR DOCUMENT-LEVEL MULTIMODAL UNDERSTANDING


 §.§ Model Architecture

Our model architecture leverages the widely-adopted ViT-MLP-LLM structure <cit.>, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework.





 §.§ Optimizing Training Efficiency


The training efficiency of MLLMs is hindered by two key challenges: (1) Inconsistent Sample Lengths. Samples with different context lengths will result in excessive padding and lower training throughput; and (2) Limited GPU Memory. 
As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:

(1) Multimodal Data Packing. To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. 
Specifically, thresholds $T_{ img}$ and $T_{ tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \(s\) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (, $T_{ img}$, $T_{ tok}$), the combined sample is pushed back into the priority queue. If \(s\) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \(M\), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. 


(2) Ring Attention. We implement the Ring Attention mechanism  <cit.> to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.

(3) Liger Kernel. To further improve memory and computational efficiency, we integrate the Liger Kernel <cit.>, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.




§ EXPERIMENTS



 §.§ Experimental Setup

Training Details.
Our model is available in two sizes: Docopilot-2B and Docopilot-8B, both of which are based on the InternVL2 <cit.> and fine-tuned for one epoch using the data recipe that includes Doc-750K. 
The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.
To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy <cit.> to enhance OCR for document understanding. 
The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.

Baselines.
We compare our Docopilot with a series of open-source document-level MLLMs <cit.> that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro <cit.>, GPT-4o <cit.>. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG <cit.>, InternVL + RAG <cit.>, and M3DocRAG <cit.>. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs <cit.>.





 §.§ Multi-Page VQA
Benchmarks. For the multi-page VQA task, we evaluate our model on three benchmarks:
(1) MP-DocVQA<cit.>, 
which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.
(2) MMLongbench-Doc<cit.>, a benchmark for evaluating the performance of MLLMs on multi-modal documents.
(3) DocGenome<cit.>, a large-scale benchmark for the evaluation of scientific document comprehension.


Results.
As illustrated in Table <ref>, our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.
Notably, our Docopilot-8B surpasses Gemini-1.5-Pro <cit.> on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. 
In comparison to RAG-based methods <cit.>, our model demonstrates advantages in multi-page scenarios.
For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6% over InternVL2-8B.



 §.§ Interleaved Long-Context QA
Benchmarks.
For the interleaved long-context QA task, we evaluate our models on MM-NIAH <cit.>, a benchmark designed for long multimodal document comprehension.

Results.
The right side of Table <ref> presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into "Short," "Medium," and "Long" based on the context window of InternVL2 (8K) and Docopilot (32K). Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, Docopilot-2B outperforms InternVL2-2B by 110%, and Docopilot-8B surpasses InternVL2-8B by 70%. Furthermore, our model performs comparably to the state-of-the-art multimodal long-context model Gemini-1.5-Pro in contexts longer than 8K, establishing a new state-of-the-art performance among open-source long-context MLLMs.



 §.§ Single-Page VQA
Benchmarks.
For single-page VQA tasks, we evaluate our model on three benchmarks:
(1) DocVQA <cit.>, a benchmark for the evaluation of extracting key information from an image of the given document.
(2) ChartQA <cit.>, a benchmark for evaluating the reasoning abilities for chart images.
(3) InfoVQA <cit.>, a benchmark for infographic image comprehension.

Results.
As shown in Table <ref>, our model achieves 
comparable performance to baseline models. 
Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that Doc-750K effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents.




 §.§ Ablation Study
Effect of Doc-750K.
We conducted ablation studies on MMLongBench-Doc <cit.> to analyze the impact of our Doc-750K. We divided Doc-750K into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table <ref>. 
We observed that with the inclusion of different parts of Doc-750K, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.


Latency Analysis.
To compare the latency in inference between RAG methods and our Docopilot, we conducted a latency analysis on MMLongBench-Doc <cit.>, as reported in Table <ref>. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130% slower than InternVL2-2B, and VisRAG is 28% slower than MiniCPM-V2.6. Our Docopilot does not require additional processes and therefore has the same inference time as baseline models, making it more suitable for analyzing long documents.





§ CONCLUSIONS



This work introduced a diverse document-level question-answering dataset that covers complex structures and cross-page dependencies, providing a robust foundation for training and evaluating document understanding models. We also proposed a retrieval-free long-document understanding model that effectively integrates multi-page information, reducing reliance on external retrieval systems. Experimental results show that our model achieves state-of-the-art performance across several document-level QA benchmarks, underscoring its strength in multi-page integration and complex reasoning. Future work will focus on improving computational efficiency, extending the model to larger multimodal tasks, and adapting it to broader applications for enhanced practicality and generalization.



§ ACKNOWLEDGMENTS

This project was supported by the National Key R&D Program of China (No. 2022ZD0161300, 2022ZD0160101), the National Natural Science Foundation of China (No. 62376134, 62372223). Zhe Chen is supported by the Youth PhD Student Research Project under the National Natural Science Foundation (No. 623B2050).

{{ieeenat_fullname}}

Datasets             #QA      Image Types                              Tasks 

MP-DocVQA <cit.>    46K       PDF Documents 
    VQA 

DUDE <cit.>    41K       PDF Documents 
   [tl]{VQA}

MP-DocStruct1M <cit.>    1M        PDF Documents 
   [tl]{Text Parsing, Text Lookup}

MP-DocReason51K <cit.>    51K      [tl]{PDF Documents, 
 Infographics, Webpages, 
 Charts, Natural images}    VQA 

DocGenome <cit.>    N/A      [tl]{PDF Documents}   [tl]{Layout Detection, Document Transformation}

Doc-750K (ours)      758K     [tl]{PDF documents, 
 Charts, Tables}   [tl]{VQA, Abstract Writing, Paper Titling, 
 Caption Writing, Experiment Writing, 
 Translation, Review, Reply }
{Comparison with other document-level datasets. }

Layout        Benchmark           Description                                 Metric                                                                                      
3*{Single-Page}    DocVQA <cit.>    VQA on documents.                           ANLS                                          
    ChartVQA <cit.>    VQA on charts.                              Relaxed EM                                    
    InfoVQA <cit.>    VQA on infographics.                        ANLS                                          
5*{Multi-Page}    MP-DocVQA <cit.>    VQA on multi-page documents.                ANLS                                          
    MMLongBench-Doc <cit.>    VQA on super-long PDF documents.            Accuracy, F1 Score                            
   3*[3ex]{DocGenome <cit.>}   3*[3ex]{VQA on multi-page scientific documents.}    Classification Acc, 
          Title ED, Abstract ED, 
          Single-Page Acc,   Multi-Page Acc 

Interleaved                      MM-NIAH <cit.>    VQA on natural texts or images.             Accuracy   
{Evaluation benchmarks and metrics for document understanding.
This table summarizes key benchmarks used in document understanding tasks across three layout types: single-page, multi-page, and interleaved. 
}

Dataset            #Images     #QA Pairs     #Tokens         

Docmatix <cit.>    2,444,750     9,500,000        390,000,000      


DocVQA <cit.>    10,189        39,463           337,829          


TextCaps <cit.>    21,953        21,953           389,658          


TextVQA <cit.>    21,953        34,602           181,918          


ST-VQA <cit.>    17,247        23,121           127,846          


OCR-VQA <cit.>    165,746       801,579          6,073,824        


VisualMRC <cit.>    3,027         11,988           168,828          


DUDE <cit.>    147,597       23,716           11,341,228       
{gray!15}
Doc-750K (ours)                         3,103,494     758,000          5,200,000,000    
{Comparison with popular VQA datasets.}


    Models                                                    Acc        F1    

    Baseline (InternVL2-2B <cit.>)                  10.5       10.8  
– Variant1: SFT using data recipe w/o Doc-750K          18.4       9.4   
– Variant2: Variant1 + our Sci-Hub data                  18.5       15.2  
– Variant3: Variant2 + our Arxiv data                    20.5       15.5  
{gray!15}
    Docopilot-2B: Variant3 + our OpenReview data             21.8       16.0  
{Ablation study on the data recipe.
    We evaluate the effectiveness of the training data from different sources on MMLongBench-Doc. Our Doc-750K can consistently enhance the ability of the model to understand multi-page documents.}

Statistics    1c{Number}

Total Questions           758K   

Total Images              3.1M   

Total Conversations       251K   

Multi-Turn Questions      87K    

Single-Turn Questions     164K   

Average Text Tokens       11245  

Average Image Tokens      6178   
{Key statistics of the {Doc-750K} datasets.
It comprises 758K questions, 3.1M images, and 251K conversations, including 87K multi-turn and 164K single-turn questions. With an average of 11,245 text tokens and 6,178 image tokens, it highlights the dataset’s richness and diversity for multimodal research.
}

2{c|}{Settings}    Docopilot-2B         Docopilot-8B             
2*{{90}{Model}}    ViT                   InternViT-300M          InternViT-300M 
    LLM        InternLM2-1.8B     InternLM2.5-7B 
{11}*{{90}{Training Hyperparameters}}    Tile Resolution      2c{448}
    Batch Size           2c{128}
    Optimizer            2c{AdamW}
    Learning Rate        2c{1.00E-05}
    Warmup Ratio         2c{0.03}
    LR Scheduler         2c{Cosine}
    Weight Decay          0.01                    0.05           
    ViT Drop Path        2c{0.1}
    Max Tile Number      2c{24}
    Image Threshold      2c{48}
    Token Threshold       2c{32K}
    Epochs      2c1
{Training settings and hyperparameters for Docopilot models. Key configurations for Docopilot-2B and Docopilot-8B, including model architectures and training parameters.}

    Models                                        Latency                   Acc       F1   

    MiniCPM-V2.6 <cit.>     225.4ms                  16.9      15.4 

    VisRAG-12B <cit.>     288.3ms                  18.8      18.3 

    InternVL2-2B <cit.>      35.9ms                  10.5       10.8 

    InternVL2-2B + RAG <cit.>      82.9ms                  17.2       16.7 
{gray!15}
    Docopilot-2B (ours)                            35.9ms                  21.8       16.0 

    InternVL2-8B <cit.>      81.0ms                  17.4       16.5 

    InternVL2-8B + RAG <cit.>     113.4ms                  24.2       24.5 
{gray!15}
    Docopilot-8B (ours)                            81.0ms                  28.8       23.0 
{Latency analysis. We evaluate the average token output latency of model outputs on MMLongBench-Doc <cit.>. 
    RAG-based methods <cit.> exhibit slower processing speeds due to their two-stage inference process, making them less efficient than document MLLMs for handling multimodal long documents.
    }
=3.3pt
2*{Models}    MP-Doc            2c{MMLong-Doc}   5c{DocGenome}   4c{MM-NIAH}
(l){2-2}(l){3-4}(l){5-9}(l){10-13}    ANSL$↑$    Acc$↑$    F1$↑$    Class Acc$↑$    Title ED$↓$    Abstract ED$↓$    SP Acc$↑$    MP Acc$↑$    Short        Medium        Long        Overall 
Proprietary Models

    Gemini-1.5-Pro <cit.>   –    28.2     20.6                               –   –   –   –   –    73.8     65.2     60.8     67.1 

    GPT-4o <cit.>   –    42.8     44.9                                97.6     9.5     6.5     71.8     67.6 
       –   –   –   –
Open-Source Models
 
    MiniMonkey-2B <cit.>    70.3               10.3      8.6                                57.4     16.5     55.0     40.3     28.9                         
        40.9     26.9     23.5     31.0 


    InternVL2-2B <cit.>    71.8               10.5     10.8                                60.8     18.4     54.3     39.4     28.9                       
        36.6     21.2     19.4     26.4 

    InternVL2-2B + RAG <cit.>    72.6               17.2     16.7                                60.8     18.4     54.3     39.4     28.4 
        36.8     30.2     34.8     33.8 

    Llama3.2-3B-Instruct$^†$ <cit.>   –    23.7     21.2                                85.3     194.7     51.0     40.2     34.9 
        15.5      2.2      0.5      6.6 
{gray!15}
    Docopilot-2B (ours)            
        76.2               21.8     16.0                                56.2     4.5      43.6     45.1     37.4  
        58.0     46.7     40.9     49.2 

    MiniCPM-V2.6-8B <cit.>   –    16.9     15.4                                92.8     10.2     32.6     60.0     54.2     
        49.0     15.3     0.0     23.4 


    LLaVA-OneVision-8B <cit.>   –    10.8      9.6                                85.6     49.9     77.5      9.8      7.1 
        65.7     38.0     0.0     37.7 

    mPLUG-DocOwl2-8B <cit.>    69.4               13.4      8.9                               –   –   –   –   –    17.9      0.1     0.0      6.6 

    M3DocRAG <cit.>    84.4               21.0     22.6                               –   –   –   –   –   –   –   –   –


    VisRAG-8B <cit.>   –    18.8     18.3                                92.8     10.2     32.6     60.0     50.7 
        47.1     29.2     29.5     35.8 

    
    InternLM2.5-7B-1M$^†$ <cit.>   –    28.7     25.6                                92.7     77.6     59.3     42.7     42.5 
        40.5     37.2     35.1     37.8 


    InternVL2-8B <cit.>    79.3               17.4     16.5                                90.6      8.2     39.6     56.0     46.1 
        56.4     37.3     32.4     42.9 

    InternVL2-8B + RAG <cit.>    78.7               24.2     24.5                                90.6      8.2     39.6     56.0     46.0  
        55.7     43.4     45.2     48.4 


    InternVL2-26B <cit.>   –    15.5     15.4                                87.5     16.9     23.3     49.7     42.7 
        65.0     48.7     41.9     52.8 
{gray!15}
    Docopilot-8B (ours)            
        81.3               28.8     23.0                                93.8     2.0      19.7     53.9     51.9 
        71.2     57.4     55.3     61.8 
{Evaluation on multi-page and interleaved VQA benchmarks. 
    We report the metrics on MP-DocVQA <cit.> (MP-Doc), MMLongbench-Doc <cit.> (MMLong-Doc), DocGenome <cit.>, and MM-NIAH <cit.>.
    Our model outperforms document-level MLLMs and multimodal RAG methods on multi-page, medium, and long-context QA. 
    The “Short”, “Medium”, and “Long” in MM-NIAH refer to input length in ${[0,8k]}$, ${(8k, 32k]}$, ${( 32k, 64k]}$, respectively. “$†$” denotes input documents are parsed by OCR models.}


    Models     DocVQA     ChartQA     InfoVQA 

    Gemini-1.5-Pro <cit.>    93.1        87.2        81.0    

    GPT-4o <cit.>    92.8        85.7       –

    MiniMonkey-2B <cit.>    87.4        76.5        60.1    

    InternVL2-2B <cit.>    86.9        76.2        58.9    
{gray!15}
    Docopilot-2B (ours)                         87.3        76.4        58.5    

    Monkey-8B <cit.>    66.5        65.1        36.1    

    TextMonkey-9B <cit.>    73.0        66.9        28.6    

    mPLUG-DocOwl2-8B <cit.>    80.7       70.0        46.4    

    IXC2.5-7B <cit.>    90.9        82.2        69.9    

    InternVL2-8B <cit.>    91.6        83.3        74.8    
{gray!15}
    Docopilot-8B (ours)                         92.0        83.3        73.3    

{Results on single-page VQA benchmarks.
    Our Docopilot models perform comparably to baselines <cit.>, demonstrating enhanced long-context modeling without loss on shorter tasks.
    }
