@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@misc{laurencon2024building,
      title={Building and better understanding vision-language models: insights and future directions.}, 
      author={Hugo Laurençon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
      year={2024},
      eprint={2408.12637},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{tanaka2021visualmrc,
  title={Visualmrc: Machine reading comprehension on document images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13878--13888},
  year={2021}
}

@article{ma2024unifying,
  title={Unifying multimodal retrieval via document screenshot embedding},
  author={Ma, Xueguang and Lin, Sheng-Chieh and Li, Minghan and Chen, Wenhu and Lin, Jimmy},
  journal={arXiv preprint arXiv:2406.11251},
  year={2024}
}

@misc{arxivqa,
  author = {Mohammad Reza Taesiri},
  title = {ArxivQA},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/taesiri/ArXivQA}},
}

@article{shi2024eagle,
  title={Eagle: Exploring the design space for multimodal llms with mixture of encoders},
  author={Shi, Min and Liu, Fuxiao and Wang, Shihao and Liao, Shijia and Radhakrishnan, Subhashree and Huang, De-An and Yin, Hongxu and Sapra, Karan and Yacoob, Yaser and Shi, Humphrey and others},
  journal={arXiv preprint arXiv:2408.15998},
  year={2024}
}

@article{wang2024mineru,
  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},
  journal={arXiv preprint arXiv:2409.18839},
  year={2024}
}

@inproceedings{luo2024layoutllm,
  title={LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding},
  author={Luo, Chuwei and Shen, Yufan and Zhu, Zhaoqing and Zheng, Qi and Yu, Zhi and Yao, Cong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15630--15640},
  year={2024}
}

@article{wang2023docllm,
  title={DocLLM: A layout-aware generative language model for multimodal document understanding},
  author={Wang, Dongsheng and Raman, Natraj and Sibue, Mathieu and Ma, Zhiqiang and Babkin, Petr and Kaur, Simerjot and Pei, Yulong and Nourbakhsh, Armineh and Liu, Xiaomo},
  journal={arXiv preprint arXiv:2401.00908},
  year={2023}
}

@inproceedings{appalaraju2024docformerv2,
  title={Docformerv2: Local features for document understanding},
  author={Appalaraju, Srikar and Tang, Peng and Dong, Qi and Sankaran, Nishant and Zhou, Yichu and Manmatha, R},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={2},
  pages={709--718},
  year={2024}
}

@article{bai2022wukong,
  title={Wukong-reader: Multi-modal pre-training for fine-grained visual document understanding},
  author={Bai, Haoli and Liu, Zhiguang and Meng, Xiaojun and Li, Wentao and Liu, Shuang and Xie, Nian and Zheng, Rongfu and Wang, Liangwei and Hou, Lu and Wei, Jiansheng and others},
  journal={arXiv preprint arXiv:2212.09621},
  year={2022}
}

@inproceedings{tang2023unifying,
  title={Unifying vision, text, and layout for universal document processing},
  author={Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={19254--19264},
  year={2023}
}

@inproceedings{kim2022ocr,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle={European Conference on Computer Vision},
  pages={498--517},
  year={2022},
  organization={Springer}
}

@article{wang2023towards,
  title={Towards improving document understanding: An exploration on text-grounding via mllms},
  author={Wang, Yonghui and Zhou, Wengang and Feng, Hao and Zhou, Keyi and Li, Houqiang},
  journal={arXiv preprint arXiv:2311.13194},
  year={2023}
}

@article{sharifymoghaddam2024unirag,
  title={UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models},
  author={Sharifymoghaddam, Sahel and Upadhyay, Shivani and Chen, Wenhu and Lin, Jimmy},
  journal={arXiv preprint arXiv:2405.10311},
  year={2024}
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{feng2023docpedia,
  title={Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding},
  author={Feng, Hao and Liu, Qi and Liu, Hao and Zhou, Wengang and Li, Houqiang and Huang, Can},
  journal={arXiv preprint arXiv:2311.11810},
  year={2023}
}

@article{yu2024visrag,
  title={VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{faysse2024colpali,
  title={Colpali: Efficient document retrieval with vision language models},
  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\'e}line and Colombo, Pierre},
  journal={arXiv preprint arXiv:2407.01449},
  year={2024}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@article{chen2023longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{ding2023longnet,
  title={Longnet: Scaling transformers to 1,000,000,000 tokens},
  author={Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}

@article{han2023lm,
  title={Lm-infinite: Simple on-the-fly length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2023}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{sun2022length,
  title={A length-extrapolatable transformer},
  author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10554},
  year={2022}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  pages={10012--10022},
  year={2021}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}


@article{abdin2024phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}


@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}



@article{zhang2019rmsnorm,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}


@article{dong2024ixc2,
  title={InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Wei, Xilin and Zhang, Songyang and Duan, Haodong and Cao, Maosong and others},
  journal={arXiv preprint arXiv:2401.16420},
  year={2024}
}


@inproceedings{guo2019eaten,
  title={Eaten: Entity-aware attention for single shot visual text extraction},
  author={Guo, He and Qin, Xiameng and Liu, Jiaming and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  booktitle={ICDAR},
  pages={254--259},
  year={2019}
}


@inproceedings{singh2021textocr,
  title={Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text},
  author={Singh, Amanpreet and Pang, Guan and Toh, Mandy and Huang, Jing and Galuba, Wojciech and Hassner, Tal},
  booktitle={CVPR},
  pages={8802--8812},
  year={2021}
}

@article{masry2023unichart,
  title={Unichart: A universal vision-language pretrained model for chart comprehension and reasoning},
  author={Masry, Ahmed and Kavehzadeh, Parsa and Do, Xuan Long and Hoque, Enamul and Joty, Shafiq},
  journal={arXiv preprint arXiv:2305.14761},
  year={2023}
}

@inproceedings{methani2020plotqa,
  title={Plotqa: Reasoning over scientific plots},
  author={Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M and Kumar, Pratyush},
  booktitle={WACV},
  pages={1527--1536},
  year={2020}
}

@article{liu2020casia,
  title={Offline handwritten Chinese text recognition with convolutional neural networks},
  author={Liu, Brian and Xu, Xianchao and Zhang, Yu},
  journal={arXiv preprint arXiv:2006.15619},
  year={2020}
}

@article{veit2016cocotext,
  title={Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge},
  journal={arXiv preprint arXiv:1601.07140},
  year={2016}
}


@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{reid2024gemini1_5,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{tong2024mmvp,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2401.06209},
  year={2024}
}

@inproceedings{kim2022synthdog,
  title     = {OCR-Free Document Understanding Transformer},
  author    = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle = {ECCV},
  year      = {2022}
}

@article{yuan2019ctw,
  title={A large chinese text dataset in the wild},
  author={Yuan, Tai-Ling and Zhu, Zhe and Xu, Kun and Li, Cheng-Jun and Mu, Tai-Jiang and Hu, Shi-Min},
  journal={Journal of Computer Science and Technology},
  volume={34},
  pages={509--521},
  year={2019},
  publisher={Springer}
}

@inproceedings{gupta2016synthtext,
  title={Synthetic data for text localisation in natural images},
  author={Gupta, Ankush and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={CVPR},
  pages={2315--2324},
  year={2016}
}

@article{chen2023internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Muyan, Zhong and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={ICCV},
  pages={568--578},
  year={2021}
}

@inproceedings{li2023unmasked,
  title={Unmasked teacher: Towards training-efficient video foundation models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  booktitle={ICCV},
  pages={19948--19960},
  year={2023}
}

@misc{mmseg2020,
    title={{MMSegmentation}: OpenMMLab Semantic Segmentation Toolbox and Benchmark},
    author={MMSegmentation Contributors},
    howpublished = {\url{https://github.com/open-mmlab/mmsegmentation}},
    year={2020}
}

@inproceedings{he2022mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  pages={16000--16009},
  year={2022}
}

@inproceedings{kirillov2019panoptic,
  title={Panoptic feature pyramid networks},
  author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={CVPR},
  pages={6399--6408},
  year={2019}
}

@inproceedings{xiao2018upernet,
  title={Unified perceptual parsing for scene understanding},
  author={Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  booktitle={ECCV},
  pages={418--434},
  year={2018}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={ICML},
  pages={10347--10357},
  year={2021}
}


@inproceedings{wang2023internimage,
  title={Internimage: Exploring large-scale vision foundation models with deformable convolutions},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  booktitle={CVPR},
  pages={14408--14419},
  year={2023}
}

@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy V and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and HAZIZA, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={TMLR},
  year={2023}
}


@article{chen2015cococaption,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@article{chen2023palix,
  title={PaLI-X: On Scaling up a Multilingual Vision and Language Model},
  author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal={arXiv preprint arXiv:2305.18565},
  year={2023}
}

@article{wei2022fdswin,
  title={Contrastive learning rivals masked image modeling in fine-tuning via feature distillation},
  author={Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2205.14141},
  year={2022}
}

@inproceedings{cheng2022mask2former,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  booktitle={CVPR},
  pages={1290--1299},
  year={2022}
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Dollar, Piotr and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Girshick, Ross},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@article{wang2021pvtv2,
  title={Pvt v2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={CVMJ},
  volume={8},
  number={3},
  pages={415--424},
  year={2022},
  publisher={Springer}
}


@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={ICCV},
  pages={22--31},
  year={2021}
}

@article{gu2021hrvit,
  title={Hrvit: Multi-scale high-resolution vision transformer},
  author={Gu, Jiaqi and Kwon, Hyoukjun and Wang, Dilin and Ye, Wei and Li, Meng and Chen, Yu-Hsin and Lai, Liangzhen and Chandra, Vikas and Pan, David Z},
  journal={arXiv preprint arXiv:2111.01236},
  year={2021}
}

@article{lu2024deepseekvl,
  title={Deepseek-vl: Towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{kuznetsova2020openimage,
  title={The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal={IJCV},
  volume={128},
  number={7},
  pages={1956--1981},
  year={2020}
}

@article{liu2023mmcinst,
  title={Mmc: Advancing multimodal chart understanding with large-scale instruction tuning},
  author={Liu, Fuxiao and Wang, Xiaoyang and Yao, Wenlin and Chen, Jianshu and Song, Kaiqiang and Cho, Sangwoo and Yacoob, Yaser and Yu, Dong},
  journal={arXiv preprint arXiv:2311.10774},
  year={2023}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{yuan2021hrformer,
  title={HRFormer: High-Resolution Vision Transformer for Dense Prediction},
  author={Yuan, Yuhui and Fu, Rao and Huang, Lang and Lin, Weihong and Zhang, Chao and Chen, Xilin and Wang, Jingdong},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@article{dong2021cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2107.00652},
  year={2021}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  pages={11976--11986},
  year={2022}
}

@inproceedings{zhang2020bridging,
  title={Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection},
  author={Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z},
  booktitle={CVPR},
  pages={9759--9768},
  year={2020}
}

@article{cai2019cascade,
  title={Cascade R-CNN: high quality object detection and instance segmentation},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  journal={TPAMI},
  volume={43},
  number={5},
  pages={1483--1498},
  year={2019}
}

@article{li2020generalized,
  title={Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection},
  author={Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
  journal={NeurIPS},
  volume={33},
  pages={21002--21012},
  year={2020}
}

@inproceedings{sun2021sparse,
  title={Sparse r-cnn: End-to-end object detection with learnable proposals},
  author={Sun, Peize and Zhang, Rufeng and Jiang, Yi and Kong, Tao and Xu, Chenfeng and Zhan, Wei and Tomizuka, Masayoshi and Li, Lei and Yuan, Zehuan and Wang, Changhu and others},
  booktitle={CVPR},
  pages={14454--14463},
  year={2021}
}


@article{li2021benchmarking,
  title={Benchmarking detection transfer learning with vision transformers},
  author={Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
  journal={arXiv preprint arXiv:2111.11429},
  year={2021}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle={ICCV},
  pages={32--42},
  year={2021}
}

@article{chu2021conditional,
  title={Conditional positional encodings for vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={arXiv preprint arXiv:2102.10882},
  year={2021}
}

@inproceedings{huang2016droppath,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={ECCV},
  pages={646--661},
  year={2016}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SIGKDD},
  pages={3505--3506},
  year={2020}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={CVPR},
  pages={1251--1258},
  year={2017}
}

@article{zhu2023ghost,
  title={Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory},
  author={Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:2305.17144},
  year={2023}
}

@inproceedings{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle={ICLR},
  year={2020}
}

@article{thomee2016yfcc100m,
  title={YFCC100M: The new data in multimedia research},
  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={64--73},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{mehta2020delight,
  title={DeLighT: Deep and Light-weight Transformer},
  author={Mehta, Sachin and Ghazvininejad, Marjan and Iyer, Srinivasan and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={ICLR},
  year={2020}
}

@misc{laion_ai_2023_clip,
  author = {LAION-AI},
  title = {CLIP Benchmark: CLIP-like model evaluation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/LAION-AI/CLIP_benchmark}},
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  pages={740--755},
  year={2014}
}

@inproceedings{yao2021filip,
  title={FILIP: Fine-grained Interactive Language-Image Pre-Training},
  author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{cai2022reversible,
  title={Reversible Column Networks},
  author={Cai, Yuxuan and Zhou, Yizhuang and Han, Qi and Sun, Jianjian and Kong, Xiangwen and Li, Jun and Zhang, Xiangyu},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={CVPR},
  pages={12104--12113},
  year={2022}
}



@article{wang2023onepeace,
  title={ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities},
  author={Wang, Peng and Wang, Shijie and Lin, Junyang and Bai, Shuai and Zhou, Xiaohuan and Zhou, Jingren and Wang, Xinggang and Zhou, Chang},
  journal={arXiv preprint arXiv:2305.11172},
  year={2023}
}

@inproceedings{singh2023maws,
  title={The effectiveness of MAE pre-pretraining for billion-scale pretraining},
  author={Singh, Mannat and Duval, Quentin and Alwala, Kalyan Vasudev and Fan, Haoqi and Aggarwal, Vaibhav and Adcock, Aaron and Joulin, Armand and Doll{\'a}r, Piotr and Feichtenhofer, Christoph and Girshick, Ross and others},
  booktitle={ICCV},
  pages={5484--5494},
  year={2023}
}

@inproceedings{agrawal2019nocaps,
  title={Nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={ICCV},
  pages={8948--8957},
  year={2019}
}

@article{bai2023touchstone,
  title={Touchstone: Evaluating vision-language models by language models},
  author={Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.16890},
  year={2023}
}

@article{liu2023mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@misc{textocr_gpt4v_dataset,
  title = {TextOCR GPT-4V Dataset},
  author = {Jimmycarter},
  howpublished = {\url{https://huggingface.co/datasets/jimmycarter/textocr-gpt4v}},
  organization = {Hugging Face},
  year={2023}
}

@misc{OpenHermes2_5,
  title = {OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants},
  author = {Teknium},
  year = {2023},
  organization = {HuggingFace},
  howpublished = {\url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}}
}

@article{bai2024coig,
  title={COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning},
  author={Bai, Yuelin and Du, Xinrun and Liang, Yiming and Jin, Yonggang and Liu, Ziqiang and Zhou, Junting and Zheng, Tianyu and Zhang, Xincheng and Ma, Nuo and Wang, Zekun and others},
  journal={arXiv preprint arXiv:2403.18058},
  year={2024}
}



@article{wang2023lvisinstruct4v,
  title={To see is to believe: Prompting gpt-4v for better visual instruction tuning},
  author={Wang, Junke and Meng, Lingchen and Weng, Zejia and He, Bo and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2311.07574},
  year={2023}
}

@article{he2023wanjuan,
  title={Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models},
  author={He, Conghui and Jin, Zhenjiang and Xu, Chao and Qiu, Jiantao and Wang, Bin and Li, Wei and Yan, Hang and Wang, Jiaqi and Lin, Dahua},
  journal={arXiv preprint arXiv:2308.10755},
  year={2023}
}

@misc{laion_gpt4v_dataset,
  title = {GPT-4V Dataset},
  author={LAION},
  howpublished = {\url{https://huggingface.co/datasets/laion/gpt4v-dataset}},
  organization = {LAION},
  year={2023}
}

@article{chen2024allava,
  title={ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model},
  author={Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2402.11684},
  year={2024}
}


@article{krishna2017vg,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={IJCV},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}


@inproceedings{lerner2022viquae,
  title={ViQuAE, a dataset for knowledge-based visual question answering about named entities},
  author={Lerner, Paul and Ferret, Olivier and Guinaudeau, Camille and Le Borgne, Herv{\'e} and Besan{\c{c}}on, Romaric and Moreno, Jos{\'e} G and Lov{\'o}n Melgarejo, Jes{\'u}s},
  booktitle={SIGIR},
  pages={3108--3120},
  year={2022}
}


@inproceedings{li2023superclevr,
  title={Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning},
  author={Li, Zhuowan and Wang, Xingrui and Stengel-Eskin, Elias and Kortylewski, Adam and Ma, Wufei and Van Durme, Benjamin and Yuille, Alan L},
  booktitle={CVPR},
  pages={14963--14973},
  year={2023}
}

@inproceedings{shah2019kvqa,
  title={Kvqa: Knowledge-aware visual question answering},
  author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim},
  booktitle={AAAI},
  volume={33},
  number={01},
  pages={8876--8884},
  year={2019}
}


@article{yu2023mathqa,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}


@article{lu2021geometry3k,
  title={Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning},
  author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2105.04165},
  year={2021}
}


@inproceedings{kafle2018dvqa,
  title={Dvqa: Understanding data visualizations via question answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle={CVPR},
  pages={5648--5656},
  year={2018}
}

@article{lindstrom2022clevrmath,
  title={Clevr-math: A dataset for compositional language, visual and mathematical reasoning},
  author={Lindstr{\"o}m, Adam Dahlgren and Abraham, Savitha Sam},
  journal={arXiv preprint arXiv:2208.05358},
  year={2022}
}

@article{lu2022tablemwp,
  title={Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
  author={Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2209.14610},
  year={2022}
}

@inproceedings{cao2022geoqa_plus,
  title={An augmented benchmark dataset for geometric question answering through dual parallel text encoding},
  author={Cao, Jie and Xiao, Jing},
  booktitle={COLING},
  pages={1511--1520},
  year={2022}
}


@inproceedings{kembhavi2017tqa,
  title={Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
  author={Kembhavi, Aniruddha and Seo, Minjoon and Schwenk, Dustin and Choi, Jonghyun and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle={CVPR},
  pages={4999--5007},
  year={2017}
}

@article{liu2023vsr,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={TACL},
  volume={11},
  pages={635--651},
  year={2023}
}

@inproceedings{zhu2023languagebind,
  title={LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment},
  author={Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and HongFa, WANG and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and others},
  booktitle={ICLR},
  year={2023}
}

@article{wang2024charxiv,
  title={Charxiv: Charting gaps in realistic chart understanding in multimodal llms},
  author={Wang, Zirui and Xia, Mengzhou and He, Luxi and Chen, Howard and Liu, Yitao and Zhu, Richard and Liang, Kaiqu and Wu, Xindi and Liu, Haotian and Malladi, Sadhika and others},
  journal={arXiv preprint arXiv:2406.18521},
  year={2024}
}

@article{zhang2024vcr,
  title={VCR: Visual Caption Restoration},
  author={Zhang, Tianyu and Wang, Suyuchen and Li, Lu and Zhang, Ge and Taslakian, Perouz and Rajeswar, Sai and Fu, Jie and Liu, Bang and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2406.06462},
  year={2024}
}

@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{li2024seedbench2plus,
  title={Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}


@inproceedings{chen2022altclip,
  title={AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities},
  author={Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Yang, Qinghong and Wu, Ledell},
  booktitle={ACL},
  pages={8666--8682},
  year={2023}
}

@article{xie2022zero,
  title={Zero and R2D2: A large-scale Chinese cross-modal benchmark and A vision-language framework},
  author={Xie, Chunyu and Li, Jincheng and Cai, Heng and Kong, Fanjing and Wu, Xiaoyu and Song, Jianfei and Morimitsu, Henrique and Yao, Lin and Wang, Dexin and Leng, Dawei and others},
  journal={arXiv preprint arXiv:2205.03860},
  year={2022}
}

@article{fengshenbang,
  title={Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence},
  author={Zhang, Jiaxing and Gan, Ruyi and Wang, Junjie and Zhang, Yuxiang and Zhang, Lin and Yang, Ping and Gao, Xinyu and Wu, Ziwei and Dong, Xiaoqun and He, Junqing and others},
  journal={arXiv preprint arXiv:2209.02970},
  year={2022}
}


@article{gu2022wukong,
  title={Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark},
  author={Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Minzhe, Niu and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and others},
  journal={NeurIPS},
  volume={35},
  pages={26418--26431},
  year={2022}
}

@article{sun2023evaclip,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}

@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal={NeurIPS},
  volume={35},
  pages={5696--5710},
  year={2022}
}

@misc{openclip,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  year         = 2021,
  howpublished = {Zenodo. Version 0.1. \url{https://doi.org/10.5281/zenodo.5143773}},
  note         = {DOI: 10.5281/zenodo.5143773}
}

@inproceedings{li2023pope,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={EMNLP},
  pages={292--305},
  year={2023}
}


@misc{idefics2023,
  title = {Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model},
  author = {{IDEFICS}},
  year = {2023},
  howpublished = {\url{https://huggingface.co/blog/idefics}}
}


@inproceedings{li2023flip,
  title={Scaling language-image pre-training via masking},
  author={Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer, Christoph and He, Kaiming},
  booktitle={CVPR},
  pages={23390--23400},
  year={2023}
}

@article{li2023monkey,
  title={Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}

@inproceedings{singh2019textvqa,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={CVPR},
  pages={8317--8326},
  year={2019}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  pages={3608--3617},
  year={2018}
}

@article{carreira2018k600,
  title={A short note about kinetics-600},
  author={Carreira, Joao and Noland, Eric and Banki-Horvath, Andras and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1808.01340},
  year={2018}
}

@article{carreira2019k700,
  title={A short note on the kinetics-700 human action dataset},
  author={Carreira, Joao and Noland, Eric and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1907.06987},
  year={2019}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={ICML},
  pages={4904--4916},
  year={2021}
}

@misc{contributors2020mmsegmentation,
  title={MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark},
  author={Contributors, MMSegmentation},
  year={2020}
}

@inproceedings{yang2020muse,
  title={Multilingual Universal Sentence Encoder for Semantic Retrieval},
  author={Yang, Yinfei and Cer, Daniel and Ahmad, Amin and Guo, Mandy and Law, Jax and Constant, Noah and Abrego, Gustavo Hernandez and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and others},
  booktitle={ACL},
  pages={87--94},
  year={2020}
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={CVPR},
  pages={18123--18133},
  year={2022}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@inproceedings{zhou2017ade20k,
  title={Scene parsing through ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle={CVPR},
  pages={633--641},
  year={2017}
}

@inproceedings{touvron2022deit3,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  booktitle={ECCV},
  pages={516--533},
  year={2022}
}

@article{laurenccon2024docmatix,
  title={Building and better understanding vision-language models: insights and future directions},
  author={Lauren{\c{c}}on, Hugo and Marafioti, Andr{\'e}s and Sanh, Victor and Tronchon, L{\'e}o},
  journal={arXiv preprint arXiv:2408.12637},
  year={2024}
}

@article{yang2023longqlora,
  title={Longqlora: Efficient and effective method to extend context length of large language models},
  author={Yang, Jianxin},
  journal={arXiv preprint arXiv:2311.04879},
  year={2023}
}

@article{zhang2024longcite,
  title={Longcite: Enabling llms to generate fine-grained citations in long-context qa},
  author={Zhang, Jiajie and Bai, Yushi and Lv, Xin and Gu, Wanjun and Liu, Danqing and Zou, Minhao and Cao, Shulin and Hou, Lei and Dong, Yuxiao and Feng, Ling and others},
  journal={arXiv e-prints},
  pages={arXiv--2409},
  year={2024}
}

@article{zhang2024longreward,
  title={LongReward: Improving Long-context Large Language Models with AI Feedback},
  author={Zhang, Jiajie and Hou, Zhongni and Lv, Xin and Cao, Shulin and Hou, Zhenyu and Niu, Yilin and Hou, Lei and Dong, Yuxiao and Feng, Ling and Li, Juanzi},
  journal={arXiv preprint arXiv:2410.21252},
  year={2024}
}


@article{bai2024longalign,
  title={Longalign: A recipe for long context alignment of large language models},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
  journal={arXiv preprint arXiv:2401.18058},
  year={2024}
}

@article{dai2024ligerkernel,
  title={Liger Kernel: Efficient Triton Kernels for LLM Training},
  author={Dai, Yun and Kothapalli, Vignesh and Song, Qingquan and Tang, Shao and Zhu, Siyu and Shimizu, Steven and Sahni, Shivam and Ning, Haowen and Chen, Yanning and others},
  journal={arXiv preprint arXiv:2410.10989},
  year={2024}
}

@misc{InternVL2,
  author = {OpenGVLab Team},
  title = {InternVL2: Better than the Best—Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy},
  year = {2024},
  url = {https://internvl.github.io/blog/2024-07-02-InternVL-2.0/}
}

@article{hu2024mplugdocowl2,
  title={mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding},
  author={Hu, Anwen and Xu, Haiyang and Zhang, Liang and Ye, Jiabo and Yan, Ming and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2409.03420},
  year={2024}
}

@article{tito2023mpdocvqa,
  title={Hierarchical multimodal transformers for multipage docvqa},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  journal={Pattern Recognition},
  volume={144},
  pages={109834},
  year={2023},
  publisher={Elsevier}
}

@article{huang2024minimonkey,
  title={Mini-monkey: Alleviate the sawtooth effect by multi-scale adaptive cropping},
  author={Huang, Mingxin and Liu, Yuliang and Liang, Dingkang and Jin, Lianwen and Bai, Xiang},
  journal={arXiv preprint arXiv:2408.02034},
  year={2024}
}

@misc{LLMTest_NeedleInAHaystack,
  author = {Greg Kamradt},
  title = {LLMTest\_NeedleInAHaystack},
  year = {2024},
  howpublished = {\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}},
  note = {Accessed: 2024-11-11}
}


@article{an2023leval,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Zhao, Xingjian and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2307.11088},
  year={2023}
}

@inproceedings{dehghani2023vit22b,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={ICML},
  pages={7480--7512},
  year={2023}
}

@inproceedings{carreira2017k400,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={CVPR},
  pages={6299--6308},
  year={2017}
}

@inproceedings{wang2023internvid,
  title={Internvid: A large-scale video-text dataset for multimodal understanding and generation},
  author={Wang, Yi and He, Yinan and Li, Yizhuo and Li, Kunchang and Yu, Jiashuo and Ma, Xin and Chen, Xinyuan and Wang, Yaohui and Luo, Ping and Liu, Ziwei and others},
  booktitle={ICLR},
  year={2024}
}

@article{yuan2021florence,
  title={Florence: A new foundation model for computer vision},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021}
}

@inproceedings{fei2004learning,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={CVPRW},
  pages={178--178},
  year={2004}
}

@inproceedings{xiao2010sun,
  title={Sun database: Large-scale scene recognition from abbey to zoo},
  author={Xiao, Jianxiong and Hays, James and Ehinger, Krista A and Oliva, Aude and Torralba, Antonio},
  booktitle={CVPRW},
  pages={3485--3492},
  year={2010}
}

@inproceedings{berg2014birdsnap,
  title={Birdsnap: Large-scale fine-grained visual categorization of birds},
  author={Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L and Jacobs, David W and Belhumeur, Peter N},
  booktitle={CVPR},
  pages={2011--2018},
  year={2014}
}

@inproceedings{goodfellow2013fer2013,
  title={Challenges in representation learning: A report on three machine learning contests},
  author={Goodfellow, Ian J and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and others},
  booktitle={ICONIP},
  pages={117--124},
  year={2013}
}

@inproceedings{nilsback2008flowers,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={ICVGIP},
  pages={722--729},
  year={2008}
}

@inproceedings{bossard2014food101,
  title={Food-101--mining discriminative components with random forests},
  author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle={ECCV},
  pages={446--461},
  year={2014}
}

@article{stallkamp2012gtsrb,
  title={Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition},
  author={Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},
  journal={Neural networks},
  volume={32},
  pages={323--332},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{parkhi2012cats,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle={CVPR},
  pages={3498--3505},
  year={2012},
}


@inproceedings{coates2011stl10,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle={AISTAT},
  pages={215--223},
  year={2011}
}

@article{everingham2015pascal,
  title={The pascal visual object classes challenge: A retrospective},
  author={Everingham, Mark and Eslami, SM Ali and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={IJCV},
  volume={111},
  pages={98--136},
  year={2015},
  publisher={Springer}
}

@article{cheng2017resisc45,
  title={Remote sensing image scene classification: Benchmark and state of the art},
  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
  journal={Proceedings of the IEEE},
  volume={105},
  number={10},
  pages={1865--1883},
  year={2017},
  publisher={IEEE}
}

@article{helber2019eurosat,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={12},
  number={7},
  pages={2217--2226},
  year={2019},
  publisher={IEEE}
}

@inproceedings{krause2013cars,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={ICCVW},
  pages={554--561},
  year={2013}
}

@inproceedings{cimpoi2014d2d,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={CVPR},
  pages={3606--3613},
  year={2014}
}


@article{maji2013fgvc,
  title={Fine-grained visual classification of aircraft},
  author={Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:1306.5151},
  year={2013}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{li2019cococn,
  title={COCO-CN for cross-lingual image tagging, captioning, and retrieval},
  author={Li, Xirong and Xu, Chaoxi and Wang, Xiaoxu and Lan, Weiyu and Jia, Zhengxiong and Yang, Gang and Xu, Jieping},
  journal={TMM},
  volume={21},
  number={9},
  pages={2347--2360},
  year={2019},
  publisher={IEEE}
}

@article{krizhevsky2009cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009}
}

@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  pages={448--456},
  year={2015}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={ICCV},
  pages={2641--2649},
  year={2015}
}

@inproceedings{lan2017flickrcn,
  title={Fluency-guided cross-lingual image captioning},
  author={Lan, Weiyu and Li, Xirong and Dong, Jianfeng},
  booktitle={ACM MM},
  pages={1549--1557},
  year={2017}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={ICCV},
  pages={2961--2969},
  year={2017}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={ICCV},
  pages={2980--2988},
  year={2017}
}

@article{li2022uniformer,
  title={UniFormer: Unifying Convolution and Self-attention for Visual Recognition},
  author={Li, Kunchang and Wang, Yali and Zhang, Junhao and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2201.09450},
  year={2022}
}

@article{yang2021focal,
  title={Focal self-attention for local-global interactions in vision transformers},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2107.00641},
  year={2021}
}

@article{huang2021shuffle,
  title={Shuffle transformer: Rethinking spatial shuffle for vision transformer},
  author={Huang, Zilong and Ben, Youcheng and Luo, Guozhong and Cheng, Pei and Yu, Gang and Fu, Bin},
  journal={arXiv preprint arXiv:2106.03650},
  year={2021}
}

@article{steiner2021train,
  title={How to train your vit? data, augmentation, and regularization in vision transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}

@inproceedings{he2019rethinking,
  title={Rethinking imagenet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle={ICCV},
  pages={4918--4927},
  year={2019}
}

@inproceedings{hu2018senet,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={CVPR},
  pages={7132--7141},
  year={2018}
}

@inproceedings{dai2017deformable,
  title={Deformable convolutional networks},
  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  booktitle={ICCV},
  pages={764--773},
  year={2017}
}

@inproceedings{ding2021repvgg,
  title={Repvgg: Making vgg-style convnets great again},
  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle={CVPR},
  pages={13733--13742},
  year={2021}
}

@inproceedings{ghiasi2021simple,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle={CVPR},
  pages={2918--2928},
  year={2021}
}

@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}

@article{chu2021twins,
  title={Twins: Revisiting the design of spatial attention in vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@article{zhu2021uni,
  title={Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks},
  author={Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Wang, Xiaogang and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
  journal={arXiv preprint arXiv:2112.01522},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  pages={248--255},
  year={2009}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={NAACL},
  pages={4171--4186},
  year={2019}
}


@article{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  journal={arXiv preprint arXiv:2202.03555},
  year={2022}
}

@article{cui2023chinesellama,
  title={Efficient and effective text encoding for chinese llama and alpaca},
  author={Cui, Yiming and Yang, Ziqing and Yao, Xin},
  journal={arXiv preprint arXiv:2304.08177},
  year={2023}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={CVPR},
  pages={1492--1500},
  year={2017}
}

@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={ICML},
  pages={4651--4664},
  year={2021}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={JMLR},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}


@article{girdhar2022omnivore,
  title={Omnivore: A Single Model for Many Visual Modalities},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and van der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  journal={arXiv preprint arXiv:2201.08377},
  year={2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{smith2022using,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@inproceedings{stickland2019bert,
  title={Bert and pals: Projected attention layers for efficient adaptation in multi-task learning},
  author={Stickland, Asa Cooper and Murray, Iain},
  booktitle={ICML},
  pages={5986--5995},
  year={2019}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={ICML},
  pages={2790--2799},
  year={2019}
}

@article{sung2021vl,
  title={VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  journal={arXiv preprint arXiv:2112.06825},
  year={2021}
}

@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle={ICCV},
  pages={7262--7272},
  year={2021}
}

@inproceedings{ranftl2021vision,
  title={Vision transformers for dense prediction},
  author={Ranftl, Ren{\'e} and Bochkovskiy, Alexey and Koltun, Vladlen},
  booktitle={ICCV},
  pages={12179--12188},
  year={2021}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={CVPR},
  pages={6881--6890},
  year={2021}
}

@inproceedings{xu2021co,
  title={Co-scale conv-attentional image transformers},
  author={Xu, Weijian and Xu, Yifan and Chang, Tyler and Tu, Zhuowen},
  booktitle={ICCV},
  pages={9981--9990},
  year={2021}
}

@article{han2021transformer,
  title={Transformer in transformer},
  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@article{li2021localvit,
  title={Localvit: Bringing locality to vision transformers},
  author={Li, Yawei and Zhang, Kai and Cao, Jiezhang and Timofte, Radu and Van Gool, Luc},
  journal={arXiv preprint arXiv:2104.05707},
  year={2021}
}

@article{liu2021video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}

@article{liu2021swinv2,
  title={Swin Transformer V2: Scaling Up Capacity and Resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  journal={arXiv preprint arXiv:2111.09883},
  year={2021}
}


@article{zhang2021tip,
  title={Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling},
  author={Zhang, Renrui and Fang, Rongyao and Gao, Peng and Zhang, Wei and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2111.03930},
  year={2021}
}

@article{gao2021clip,
  title={Clip-adapter: Better vision-language models with feature adapters},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2110.04544},
  year={2021}
}


@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={CVPR},
  pages={2117--2125},
  year={2017}
}


@inproceedings{li2021panoptic,
  title={Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers},
  author={Li, Zhiqi and Wang, Wenhai and Xie, Enze and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Lu, Tong and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@article{ali2021xcit,
  title={Xcit: Cross-covariance image transformers},
  author={Ali, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and others},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@article{rosenfeld2018incremental,
  title={Incremental learning through deep adaptation},
  author={Rosenfeld, Amir and Tsotsos, John K},
  journal={TPAMI},
  volume={42},
  number={3},
  pages={651--663},
  year={2018}
}

@inproceedings{rebuffi2018efficient,
  title={Efficient parametrization of multi-domain deep neural networks},
  author={Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  booktitle={CVPR},
  pages={8119--8127},
  year={2018}
}

@article{rebuffi2017learning,
  title={Learning multiple visual domains with residual adapters},
  author={Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@article{chen2021simple,
  title={A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation},
  author={Chen, Wuyang and Du, Xianzhi and Yang, Fan and Beyer, Lucas and Zhai, Xiaohua and Lin, Tsung-Yi and Chen, Huizhong and Li, Jing and Song, Xiaodan and Wang, Zhangyang and others},
  journal={arXiv preprint arXiv:2112.09747},
  year={2021}
}

@inproceedings{bolya2020tide,
  title={Tide: A general toolbox for identifying object detection errors},
  author={Bolya, Daniel and Foley, Sean and Hays, James and Hoffman, Judy},
  booktitle={ECCV},
  pages={558--573},
  year={2020}
}

@inproceedings{kamann2020benchmarking,
  title={Benchmarking the robustness of semantic segmentation models},
  author={Kamann, Christoph and Rother, Carsten},
  booktitle={CVPR},
  pages={8828--8838},
  year={2020}
}

@article{michaelis2019benchmarking,
  title={Benchmarking robustness in object detection: Autonomous driving when winter is coming},
  author={Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S and Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:1907.07484},
  year={2019}
}

@inproceedings{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  booktitle={ICLR},
  year={2022}
}

@article{peng2022beitv2,
  title={BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers},
  author={Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  journal={arXiv preprint arXiv:2208.06366},
  year={2022}
}

@inproceedings{peng2021conformer,
  title={Conformer: Local features coupling global representations for visual recognition},
  author={Peng, Zhiliang and Huang, Wei and Gu, Shanzhi and Xie, Lingxi and Wang, Yaowei and Jiao, Jianbin and Ye, Qixiang},
  booktitle={ICCV},
  pages={367--376},
  year={2021}
}

@article{cheng2021masked,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  journal={arXiv preprint arXiv:2112.01527},
  year={2021}
}

@inproceedings{huang2021fapn,
  title={FaPN: Feature-aligned pyramid network for dense image prediction},
  author={Huang, Shihua and Lu, Zhichao and Cheng, Ran and He, Cheng},
  booktitle={ICCV},
  pages={864--873},
  year={2021}
}

@article{jain2021semask,
  title={SeMask: Semantically Masked Transformers for Semantic Segmentation},
  author={Jain, Jitesh and Singh, Anukriti and Orlov, Nikita and Huang, Zilong and Li, Jiachen and Walton, Steven and Shi, Humphrey},
  journal={arXiv preprint arXiv:2112.12782},
  year={2021}
}

@article{cui2022region,
  title={Region Rebalance for Long-Tailed Semantic Segmentation},
  author={Cui, Jiequan and Yuan, Yuhui and Zhong, Zhisheng and Tian, Zhuotao and Hu, Han and Lin, Stephen and Jia, Jiaya},
  journal={arXiv preprint arXiv:2204.01969},
  year={2022}
}


@inproceedings{fang2019instaboost,
  title={Instaboost: Boosting instance segmentation via probability map guided copy-pasting},
  author={Fang, Hao-Shu and Sun, Jianhua and Wang, Runzhong and Gou, Minghao and Li, Yong-Lu and Lu, Cewu},
  booktitle={ICCV},
  pages={682--691},
  year={2019}
}

@article{liang2021cbnetv2,
  title={Cbnetv2: A composite backbone network architecture for object detection},
  author={Liang, Tingting and Chu, Xiaojie and Liu, Yudong and Wang, Yongtao and Tang, Zhi and Chu, Wei and Chen, Jingdong and Ling, Haibin},
  journal={arXiv preprint arXiv:2107.00420},
  year={2021}
}

@inproceedings{caesar2018coco,
  title={Coco-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle={CVPR},
  pages={1209--1218},
  year={2018}
}

@inproceedings{Cordts_2016_CVPR,
    author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
    title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
    booktitle = {CVPR},
    year = {2016}
}

@inproceedings{neuhold2017mapillary,
  title={The mapillary vistas dataset for semantic understanding of street scenes},
  author={Neuhold, Gerhard and Ollmann, Tobias and Rota Bulo, Samuel and Kontschieder, Peter},
  booktitle={ICCV},
  pages={4990--4999},
  year={2017}
}

@article{tao2020hierarchical,
  title={Hierarchical multi-scale attention for semantic segmentation},
  author={Tao, Andrew and Sapra, Karan and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2005.10821},
  year={2020}
}

@inproceedings{yuan2019segmentation,
  title={Segmentation transformer: Object-contextual representations for semantic segmentation},
  author={Yuan, Yuhui and Chen, Xiaokang and Chen, Xilin and Wang, Jingdong},
  booktitle={ECCV},
  year={2020}
}

@article{bousselham2021efficient,
  title={Efficient Self-Ensemble Framework for Semantic Segmentation},
  author={Bousselham, Walid and Thibault, Guillaume and Pagano, Lucas and Machireddy, Archana and Gray, Joe and Chang, Young Hwan and Song, Xubo},
  journal={arXiv preprint arXiv:2111.13280},
  year={2021}
}

@article{huang2021channelized,
  title={Channelized Axial Attention for Semantic Segmentation--Considering Channel Relation within Spatial Attention for Semantic Segmentation},
  author={Huang, Ye and Kang, Di and Jia, Wenjing and He, Xiangjian and Liu, Liu},
  journal={arXiv preprint arXiv:2101.07434},
  year={2021}
}

@article{lin2022structtoken,
  title={StructToken: Rethinking Semantic Segmentation with Structural Prior},
  author={Lin, Fangjian and Liang, Zhanhao and He, Junjun and Zheng, Miao and Tian, Shengwei and Chen, Kai},
  journal={arXiv preprint arXiv:2203.12612},
  year={2022}
}

@article{huang2022car,
  title={CAR: Class-aware Regularizations for Semantic Segmentation},
  author={Huang, Ye and Kang, Di and Chen, Liang and Zhe, Xuefei and Jia, Wenjing and He, Xiangjian and Bao, Linchao},
  journal={arXiv preprint arXiv:2203.07160},
  year={2022}
}

@inproceedings{mottaghi2014role,
  title={The role of context for object detection and semantic segmentation in the wild},
  author={Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
  booktitle={CVPR},
  pages={891--898},
  year={2014}
}

@inproceedings{chen2019hybrid,
  title={Hybrid task cascade for instance segmentation},
  author={Chen, Kai and Pang, Jiangmiao and Wang, Jiaqi and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Shi, Jianping and Ouyang, Wanli and others},
  booktitle={CVPR},
  pages={4974--4983},
  year={2019}
}

@inproceedings{dai2021dynamic,
  title={Dynamic head: Unifying object detection heads with attentions},
  author={Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
  booktitle={CVPR},
  pages={7373--7382},
  year={2021}
}

@inproceedings{li2021improved,
  title={Mvitv2: Improved multiscale vision transformers for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  pages={4804--4814},
  year={2022}
}

@article{zhu2022uni,
  title={Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs},
  author={Zhu, Jinguo and Zhu, Xizhou and Wang, Wenhai and Wang, Xiaohua and Li, Hongsheng and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2206.04674},
  year={2022}
}

@article{wu2022p2t,
  title={P2T: Pyramid pooling transformer for scene understanding},
  author={Wu, Yu-Huan and Liu, Yun and Zhan, Xin and Cheng, Ming-Ming},
  journal={TPAMI},
  year={2022}
}


@article{fang2022unleashing,
  title={Unleashing vanilla vision transformer with masked image modeling for object detection},
  author={Fang, Yuxin and Yang, Shusheng and Wang, Shijie and Ge, Yixiao and Shan, Ying and Wang, Xinggang},
  journal={arXiv preprint arXiv:2204.02964},
  year={2022}
}

@article{li2022exploring,
  title={Exploring plain vision transformer backbones for object detection},
  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2203.16527},
  year={2022}
}

@article{park2022vision,
  title={How Do Vision Transformers Work?},
  author={Park, Namuk and Kim, Songkuk},
  journal={arXiv preprint arXiv:2202.06709},
  year={2022}
}

@article{si2022inception,
  title={Inception Transformer},
  author={Si, Chenyang and Yu, Weihao and Zhou, Pan and Zhou, Yichen and Wang, Xinchao and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2205.12956},
  year={2022}
}

@inproceedings{wu2022pale,
  title={Pale transformer: A general vision transformer backbone with pale-shaped attention},
  author={Wu, Sitong and Wu, Tianyi and Tan, Haoru and Guo, Guodong},
  booktitle={AAAI},
  volume={36},
  number={3},
  pages={2731--2739},
  year={2022}
}

@inproceedings{shao2019objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle={ICCV},
  pages={8430--8439},
  year={2019}
}

@inproceedings{bodla2017soft,
  title={Soft-NMS--improving object detection with one line of code},
  author={Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S},
  booktitle={ICCV},
  pages={5561--5569},
  year={2017}
}

@article{rao2022hornet,
  title={HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions},
  author={Rao, Yongming and Zhao, Wenliang and Tang, Yansong and Zhou, Jie and Lim, Ser-Nam and Lu, Jiwen},
  journal={arXiv preprint arXiv:2207.14284},
  year={2022}
}

@article{li2022maskdino,
  title={Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung and others},
  journal={arXiv preprint arXiv:2206.02777},
  year={2022}
}

@article{wei2022kdswin,
  title={Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation},
  author={Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2205.14141},
  year={2022}
}



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{jia2022visual,
  title={Visual prompt tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2203.12119},
  year={2022}
}

@article{bahng2022exploring,
  title={Exploring visual prompts for adapting large-scale models},
  author={Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  journal={arXiv preprint arXiv:2203.17274},
  volume={1},
  number={3},
  pages={4},
  year={2022}
}

@article{chen2022adaptformer,
  title={AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
  author={Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal={arXiv preprint arXiv:2205.13535},
  year={2022}
}

@inproceedings{japanese-clip,
  author={Makoto Shiin, Tianyu Zhao, Kei Sawada},
  title={Construction and Public Release of Language Image Pretraining Models in Japanese},
  booktitle={MIRU},
  year={2022}
}

@inproceedings{carlsson2022mclip,
  title={Cross-lingual and multilingual clip},
  author={Carlsson, Fredrik and Eisen, Philipp and Rekathati, Faton and Sahlgren, Magnus},
  booktitle={LREC},
  pages={6848--6854},
  year={2022}
}

@inproceedings{hendrycks2021imagenet_a,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={CVPR},
  pages={15262--15271},
  year={2021}
}

@article{aggarwal2020xtd,
  title={Towards zero-shot Cross-lingual Image retrieval},
  author={Aggarwal, Pranav and Kale, Ajinkya},
  journal={arXiv preprint arXiv:2012.05107},
  year={2020}
}

@article{jain2021mural,
  title={Mural: multimodal, multitask retrieval across languages},
  author={Jain, Aashi and Guo, Mandy and Srinivasan, Krishna and Chen, Ting and Kudugunta, Sneha and Jia, Chao and Yang, Yinfei and Baldridge, Jason},
  journal={arXiv preprint arXiv:2109.05125},
  year={2021}
}

@article{wang2019imagenet_sketch,
  title={Learning robust global representations by penalizing local predictive power},
  author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@inproceedings{recht2019imagenetv2,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={ICML},
  pages={5389--5400},
  year={2019}
}

@article{barbu2019objectnet,
  title={Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author={Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{huang2023kosmos1,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}

@article{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}

@inproceedings{sun2023emu,
  title={Generative pretraining in multimodality},
  author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{dong2023dreamllm,
  title={DreamLLM: Synergistic Multimodal Comprehension and Creation},
  author={Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others},
  booktitle={ICLR},
  year={2024}
}


@article{instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={NeurIPS},
  volume={36},
  year={2024}
}


@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@inproceedings{wang2023allseeing,
  title={The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World},
  author={Wang, Weiyun and Shi, Min and Li, Qingyun and Wang, Wenhai and Huang, Zhenhang and Xing, Linjie and Chen, Zhe and Li, Hao and Zhu, Xizhou and Cao, Zhiguo and others},
  booktitle={ICLR},
  year={2024}
}


@article{peng2023kosmos2,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2306.14824},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{hao2022metavlm,
  title={Language models are general-purpose interfaces},
  author={Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2206.06336},
  year={2022}
}

@inproceedings{hendrycks2021imagenet_r,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={ICCV},
  pages={8340--8349},
  year={2021}
}

@article{bianchi2021clip_italian,
  title={Contrastive Language-Image Pre-training for the Italian Language},
  author={Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri},
  journal={arXiv preprint arXiv:2108.08688},
  year={2021}
}


@article{yang2022cnclip,
  title={Chinese clip: Contrastive vision-language pretraining in chinese},
  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},
  journal={arXiv preprint arXiv:2211.01335},
  year={2022}
}

@article{zhang2022neural,
  title={Neural Prompt Search},
  author={Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2206.04673},
  year={2022}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={ICML},
  pages={12888--12900},
  year={2022}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}


@inproceedings{wang2023beit3,
  title={Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle={CVPR},
  pages={19175--19186},
  year={2023}
}

@article{li2021albef,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={NeurIPS},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@article{zeng2023lynx,
  title={What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?},
  author={Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
  journal={arXiv preprint arXiv:2307.02469},
  year={2023}
}

@article{shao2023tiny,
  title={Tiny lvlm-ehub: Early multimodal experiments with bard},
  author={Shao, Wenqi and Hu, Yutao and Gao, Peng and Lei, Meng and Zhang, Kaipeng and Meng, Fanqing and Xu, Peng and Huang, Siyuan and Li, Hongsheng and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2308.03729},
  year={2023}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}


@article{jie2022convolutional,
  title={Convolutional bypasses are better vision transformer adapters},
  author={Jie, Shibo and Deng, Zhi-Hong},
  journal={arXiv preprint arXiv:2207.07039},
  year={2022}
}

@inproceedings{fang2022eva,
  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={CVPR},
  pages={19358--19369},
  year={2023}
}

@article{liu2023llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  volume={36},
  year={2023}
}

@article{liu2023ocrbench,
  title={On the hidden mystery of ocr in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Li, Hongliang and Yu, Wenwen and Huang, Mingxin and Peng, Dezhi and Liu, Mingyu and Chen, Mingrui and Li, Chunyuan and Jin, Lianwen and others},
  journal={arXiv preprint arXiv:2305.07895},
  year={2023}
}


@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={WACV},
  pages={2200--2209},
  year={2021}
}

@article{wang2023visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={NeurIPS},
  volume={36},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2022rest,
  title={Rest v2: simpler, faster and stronger},
  author={Zhang, Qinglong and Yang, Yu-Bin},
  journal={NeurIPS},
  volume={35},
  pages={36440--36452},
  year={2022}
}

@article{zhang2021rest,
  title={Rest: An efficient transformer for visual recognition},
  author={Zhang, Qinglong and Yang, Yu-Bin},
  journal={NeurIPS},
  volume={34},
  pages={15475--15485},
  year={2021}
}

@article{fang2023eva02,
  title={Eva-02: A visual representation for neon genesis},
  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.11331},
  year={2023}
}

@Misc{xFormers2022,
  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},
  title =        {xFormers: A modular and hackable Transformer modelling library},
  howpublished = {\url{https://github.com/facebookresearch/xformers}},
  year =         {2022}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={NeurIPS},
  volume={35},
  pages={16344--16359},
  year={2022}
}


@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NeurIPS},
  volume={25},
  year={2012}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@article{openai2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}



@article{schuhmann2022laion5b,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={NeurIPS},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@misc{byeon2022coyo,
  title={Coyo-700m: Image-text pair dataset},
  author={Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year={2022}
}

@inproceedings{changpinyo2021cc12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={CVPR},
  pages={3558--3568},
  year={2021}
}


@inproceedings{chen2022vitadapter,
  title={Vision Transformer Adapter for Dense Predictions},
  author={Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  booktitle={ICLR},
  year={2022}
}

@article{wei2023skywork,
  title={Skywork: A More Open Bilingual Foundation Model},
  author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and others},
  journal={arXiv preprint arXiv:2310.19341},
  year={2023}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  pages={6700--6709},
  year={2019}
}

@article{ustalov2023toloka,
  title={Toloka Visual Question Answering Benchmark},
  author={Ustalov, Dmitry and Pavlichenko, Nikita and Koshelev, Sergey and Likhobaba, Daniil and Smirnova, Alisa},
  journal={arXiv preprint arXiv:2309.16511},
  year={2023}
}

@article{driess2023palme,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{liu2023controlllm,
  title={ControlLLM: Augment Language Models with Tools by Searching on Graphs},
  author={Liu, Zhaoyang and Lai, Zeqiang and Gao, Zhangwei and Cui, Erfei and Zhu, Xizhou and Lu, Lewei and Chen, Qifeng and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  journal={arXiv preprint arXiv:2310.17796},
  year={2023}
}

@article{yang2023gpt4tools,
  title={Gpt4tools: Teaching large language model to use tools via self-instruction},
  author={Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{ye2023mplug2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{liu2024convbench,
  title={ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models},
  author={Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and others},
  journal={arXiv preprint arXiv:2403.20194},
  year={2024}
}

@article{chen2023videollm,
  title={Videollm: Modeling video sequence with large language models},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Jiahao and Xu, Jilan and Huang, Yifei and Pan, Junting and Wang, Yi and Wang, Yali and Qiao, Yu and Lu, Tong and others},
  journal={arXiv preprint arXiv:2305.13292},
  year={2023}
}

@article{wang2024internvideo2,
  title={InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arXiv preprint arXiv:2403.15377},
  year={2024}
}

@misc{opencompass2023,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@article{ormazabal2024reka,
  title={Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models},
  author={Ormazabal, Aitor and Zheng, Che and d'Autume, Cyprien de Masson and Yogatama, Dani and Fu, Deyu and Ong, Donovan and Chen, Eric and Lamprecht, Eugenie and Pham, Hai and Ong, Isaac and others},
  journal={arXiv preprint arXiv:2404.12387},
  year={2024}
}

@article{lu2022scienceqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={NeurIPS},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@article{ma2024groma,
  title={Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models},
  author={Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2404.13013},
  year={2024}
}


@article{mmtbench,
  title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and Lei, Jiayi and Lu, Quanfeng and Chen, Runjian and Xu, Peng and Zhang, Renrui and Zhang, Haozhe and Gao, Peng and Wang, Yali and Qiao, Yu and Luo, Ping and Zhang, Kaipeng and Shao, Wenqi},
  journal={arXiv preprint arXiv:2404.16006},
  year={2024}
}

@inproceedings{das2017visualdialog,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  pages={326--335},
  year={2017}
}

@article{li2022paddleocr,
  title={PP-OCRv3: More attempts for the improvement of ultra lightweight OCR system},
  author={Li, Chenxia and Liu, Weiwei and Guo, Ruoyu and Yin, Xiaoting and Jiang, Kaitao and Du, Yongkun and Du, Yuning and Zhu, Lingfeng and Lai, Baohua and Hu, Xiaoguang and others},
  journal={arXiv preprint arXiv:2206.03001},
  year={2022}
}

@article{chen2023sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@inproceedings{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle={ICCV},
  pages={11888--11898},
  year={2023}
}

@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{guan2023hallusionbench,
  title={Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  journal={arXiv preprint arXiv:2310.14566},
  year={2023}
}

@article{lu2023mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@article{li2024llavaonevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}


@article{wang2024qwen2vl,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}


@article{teknium2024hermes3,
  title={Hermes 3 Technical Report},
  author={Teknium, Ryan and Quesnelle, Jeffrey and Guang, Chen},
  journal={arXiv preprint arXiv:2408.11857},
  year={2024}
}


@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}

@article{yang2023mmreact,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}

@article{yue2023mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv preprint arXiv:2311.16502},
  year={2023}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021}
}

@inproceedings{zhu2023minigpt4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={ICLR},
  year={2024}
}


% internlm
@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

% moss
@article{sun2023moss,
  title={Moss: Training conversational language models from synthetic data},
  author={Sun, Tianxiang and Zhang, Xiaotian and He, Zhengfu and Li, Peng and Cheng, Qinyuan and Yan, Hang and Liu, Xiangyang and Shao, Yunfan and Tang, Qiong and Zhao, Xingjian and others},
  journal={arXiv preprint arXiv:2307.15020},
  volume={7},
  year={2023}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICLR},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}


% chatglm
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={ACL},
  pages={320--335},
  year={2022}
}

@inproceedings{caesar2018cocostuff,
  title={Coco-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1209--1218},
  year={2018}
}


@inproceedings{chen2022pali,
  title={PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  booktitle={ICLR},
  year={2022}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}


@misc{google_bard,
  title        = {Google Bard},
  author       = {Google},
  year         = {2023},
  howpublished = {\url{https://bard.google.com/}},
}


@article{zheng2023vicuna,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={NeurIPS},
  volume={36},
  year={2024}
}


@article{baichuan2023baichuan2,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv preprint arXiv:2309.10305},
  url={https://arxiv.org/abs/2309.10305},
  year={2023}
}

@article{refinedweb,
  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal={arXiv preprint arXiv:2306.01116},
  eprint={2306.01116},
  eprinttype = {arXiv},
  url={https://arxiv.org/abs/2306.01116},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@misc{claude3series2024,
  author = {{Anthropic}},
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com}},
  url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}
}


@misc{step1v2023,
  author = {{StepFun Research Team}},
  title = {Step-1V: A Hundred Billion Parameter Multimodal Large Model},
  howpublished = {\url{https://platform.stepfun.com}},
  year = {2024}
}

@article{bi2024deepseekllm,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{dong2024xc24khd,
  title={InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={arXiv preprint arXiv:2404.06512},
  year={2024}
}

@misc{hptpro2024,
  author = {{HyperGAI Research Team}},
  title = {Introducing HPT: A Family of Leading Multimodal LLMs},
  year = {2024},
  howpublished = {\url{https://www.hypergai.com/blog/introducing-hpt-a-family-of-leading-multimodal-llms}}
}


@article{ye2023mplugdocowl,
  title={mplug-docowl: Modularized multimodal large language model for document understanding},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Dan, Yuhao and Zhao, Chenlin and Xu, Guohai and Li, Chenliang and Tian, Junfeng and others},
  journal={arXiv preprint arXiv:2307.02499},
  year={2023}
}

@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

@article{lin2024moellava,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@article{beyer2020imagenetreal,
  title={Are we done with imagenet?},
  author={Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  journal={arXiv preprint arXiv:2006.07159},
  year={2020}
}

@article{2023interngpt,
  title={InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language},
  author={Liu, Zhaoyang and He, Yinan and Wang, Wenhai and Wang, Weiyun and Wang, Yi and Chen, Shoufa and Zhang, Qinglong and Lai, Zeqiang and Yang, Yang and Li, Qingyun and Yu, Jiashuo and others},
  journal={arXiv preprint arXiv:2305.05662},
  year={2023}
}

@article{wu2023visual,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{mu2023embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{ordonez2011sbu,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
  journal={NeurIPS},
  volume={24},
  year={2011}
}


@inproceedings{sharma2018cc3m,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}

@article{schuhmann2022laioncoco,
  title={Laion coco: 600m synthetic captions from laion2b-en.},
  author={Schuhmann, Christoph and Köpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
  journal = {https://laion.ai/blog/laion-coco/},
  year={2022}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{lin2023sphinx,
  title={Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models},
  author={Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
  journal={arXiv preprint arXiv:2311.07575},
  year={2023}
}

@article{lv2023kosmos2_5,
  title={Kosmos-2.5: A multimodal literate model},
  author={Lv, Tengchao and Huang, Yupan and Chen, Jingye and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and Dong, Li and Luo, Weiyao and others},
  journal={arXiv preprint arXiv:2309.11419},
  year={2023}
}

@article{li2024miniGemini,
  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv preprint arXiv:2403.18814},
  year={2024}
}

@article{liu2024textmonkey,
  title={TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
  author={Liu, Yuliang and Yang, Biao and Liu, Qiang and Li, Zhang and Ma, Zhiyin and Zhang, Shuo and Bai, Xiang},
  journal={arXiv preprint arXiv:2403.04473},
  year={2024}
}

@inproceedings{zhai2023siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  pages={11975--11986},
  year={2023}
}

@inproceedings{shi2017rctw17,
  title={Icdar2017 competition on reading chinese text in the wild (rctw-17)},
  author={Shi, Baoguang and Yao, Cong and Liao, Minghui and Yang, Mingkun and Xu, Pei and Cui, Linyan and Belongie, Serge and Lu, Shijian and Bai, Xiang},
  booktitle={ICDAR},
  volume={1},
  pages={1429--1434},
  year={2017}
}

@inproceedings{chng2019art,
  title={Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art},
  author={Chng, Chee Kheng and Liu, Yuliang and Sun, Yipeng and Ng, Chun Chet and Luo, Canjie and Ni, Zihan and Fang, ChuanMing and Zhang, Shuaitao and Han, Junyu and Ding, Errui and others},
  booktitle={ICDAR},
  pages={1571--1576},
  year={2019}
}

@inproceedings{zhang2019rects,
  title={Icdar 2019 robust reading challenge on reading chinese text on signboard},
  author={Zhang, Rui and Zhou, Yongsheng and Jiang, Qianyi and Song, Qi and Li, Nan and Zhou, Kai and Wang, Lei and Wang, Dong and Liao, Minghui and Yang, Mingkun and others},
  booktitle={ICDAR},
  pages={1577--1581},
  year={2019}
}

@misc{xai2024grokv,
  author = {X.ai},
  title = {Grok-1.5 Vision Preview},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@inproceedings{sun2019lsvt,
  title={ICDAR 2019 competition on large-scale street view text with partial labeling-RRC-LSVT},
  author={Sun, Yipeng and Ni, Zihan and Chng, Chee-Kheng and Liu, Yuliang and Luo, Canjie and Ng, Chun Chet and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and others},
  booktitle={ICDAR},
  pages={1557--1562},
  year={2019}
}

@inproceedings{qi2022dureadervis,
  title={DuReadervis: A Chinese dataset for open-domain document visual question answering},
  author={Qi, Le and Lv, Shangwen and Li, Hongyu and Liu, Jing and Zhang, Yu and She, Qiaoqiao and Wu, Hua and Wang, Haifeng and Liu, Ting},
  booktitle={ACL},
  pages={1338--1351},
  year={2022}
}

@article{ye2023ureader,
  title={Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Xu, Guohai and Li, Chenliang and Tian, Junfeng and Qian, Qi and Zhang, Ji and others},
  journal={arXiv preprint arXiv:2310.05126},
  year={2023}
}

@article{xu2024llava_uhd,
  title={LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}

@article{li2023otterhd,
  title={Otterhd: A high-resolution multi-modality model},
  author={Li, Bo and Zhang, Peiyuan and Yang, Jingkang and Zhang, Yuanhan and Pu, Fanyi and Liu, Ziwei},
  journal={arXiv preprint arXiv:2311.04219},
  year={2023}
}

@article{hu2024mplug_docowl_1_5,
  title={mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding},
  author={Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
  journal={arXiv preprint arXiv:2403.12895},
  year={2024}
}

@article{luo2024llava_hr,
  title={Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models},
  author={Luo, Gen and Zhou, Yiyi and Zhang, Yuxin and Zheng, Xiawu and Sun, Xiaoshuai and Ji, Rongrong},
  journal={arXiv preprint arXiv:2403.03003},
  year={2024}
}

@article{wei2023vary,
  title={Vary: Scaling up the vision vocabulary for large vision-language models},
  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2312.06109},
  year={2023}
}


@article{hong2023cogagent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv preprint arXiv:2312.08914},
  year={2023}
}

@article{wang2024allseeingv2,
  title={The All-Seeing Project V2: Towards General Relation Comprehension of the Open World},
  author={Wang, Weiyun and Ren, Yiming and Luo, Haowen and Li, Tiantong and Yan, Chenxiang and Chen, Zhe and Wang, Wenhai and Li, Qingyun and Lu, Lewei and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2402.19474},
  year={2024}
}

@article{tian2024mminterleaved,
  title={Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer},
  author={Tian, Changyao and Zhu, Xizhou and Xiong, Yuwen and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Chen, Yuntao and Lu, Lewei and Lu, Tong and Zhou, Jie and others},
  journal={arXiv preprint arXiv:2401.10208},
  year={2024}
}

@inproceedings{zhang2023llama-adapter,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  booktitle={ICLR},
  year={2024}
}

@article{gao2023llama-adapterv2,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{zhang2023gpt4roi,
  title={Gpt4roi: Instruction tuning large language model on region-of-interest},
  author={Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
  journal={arXiv preprint arXiv:2307.03601},
  year={2023}
}

@article{wu2023nextgpt,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2309.05519},
  year={2023}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{lai2023lisa,
  title={Lisa: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2308.00692},
  year={2023}
}

@article{yang2023gpt-4v,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  volume={9},
  year={2023}
}


@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{zhang2023video-llama,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle={ECCV},
  pages={742--758},
  year={2020}
}

@inproceedings{goyal2017vqav2,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  pages={6904--6913},
  year={2017}
}

@inproceedings{marino2019okvqa,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={CVPR},
  pages={3195--3204},
  year={2019}
}

@inproceedings{schwenk2022aokvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={ECCV},
  pages={146--162},
  year={2022}
}

@article{lu2021iconqa,
  title={Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning},
  author={Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2110.13214},
  year={2021}
}

@inproceedings{kembhavi2016ai2d,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={ECCV},
  pages={235--251},
  year={2016}
}

@inproceedings{das2017visdial,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  pages={326--335},
  year={2017}
}


@inproceedings{masry2022chartqa,
  title={ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author={Masry, Ahmed and Do, Xuan Long and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  booktitle={ACL},
  pages={2263--2279},
  year={2022}
}


@inproceedings{biten2019stvqa,
  title={Scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  booktitle={ICCV},
  pages={4291--4301},
  year={2019}
}

@inproceedings{clark2017docqa,
  title={Simple and Effective Multi-Paragraph Reading Comprehension},
  author={Clark, Christopher and Gardner, Matt},
  booktitle={ACL},
  pages={845--855},
  year={2018}
}


@inproceedings{mishra2019ocrvqa,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR},
  pages={947--952},
  year={2019}
}

@inproceedings{wang2020estvqa,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle={CVPR},
  pages={10126--10135},
  year={2020}
}

@article{zhang2023llavar,
  title={Llavar: Enhanced visual instruction tuning for text-rich image understanding},
  author={Zhang, Yanzhe and Zhang, Ruiyi and Gu, Jiuxiang and Zhou, Yufan and Lipka, Nedim and Yang, Diyi and Sun, Tong},
  journal={arXiv preprint arXiv:2306.17107},
  year={2023}
}

@inproceedings{yu2016refcoco,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  pages={69--85},
  year={2016}
}

@inproceedings{mao2016refcocog,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  pages={11--20},
  year={2016}
}

@article{zhao2023svit,
  title={Svit: Scaling up visual instruction tuning},
  author={Zhao, Bo and Wu, Boya and Huang, Tiejun},
  journal={arXiv preprint arXiv:2307.04087},
  year={2023}
}

@article{liu2023lrv-instruction,
  title={Aligning Large Multi-Modal Model with Robust Instruction Tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}

@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={WACV},
  pages={1697--1706},
  year={2022}
}

@inproceedings{xu2016msrvtt,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={CVPR},
  pages={5288--5296},
  year={2016}
}

@article{xue2023alleviating,
  title={Alleviating data insufficiency for Chinese sign language recognition},
  author={Xue, Wanli and Liu, Jingze and Yan, Siyi and Zhou, Yuxi and Yuan, Tiantian and Guo, Qing},
  journal={Visual Intelligence},
  volume={1},
  number={1},
  pages={26},
  year={2023},
  publisher={Springer}
}

@misc{gpt4v,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  howpublished={\url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}},
  year={2023}
}

@article{hendrycks2020measuring,
    title={Measuring massive multitask language understanding},
    author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
    journal={arXiv preprint arXiv:2009.03300},
    year={2020}
}

@misc{li2023cmmlu,
      title={CMMLU: Measuring massive multitask language understanding in Chinese}, 
      author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},
      year={2023},
      eprint={2306.09212},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{huang2023ceval,
    title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
    author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and  Fu, Yao and Sun, Maosong and He, Junxian},
    journal={arXiv preprint arXiv:2305.08322},
    year={2023}
}

@misc{zhong2023agieval,
    title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
    author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
    year={2023},
    eprint={2304.06364},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Zhang2023EvaluatingTP,
    title={Evaluating the Performance of Large Language Models on GAOKAO Benchmark},
    author={Xiaotian Zhang and Chunyang Li and Yi Zong and Zhengyu Ying and Liang He and Xipeng Qiu},
    year={2023}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}


@article{naturalquestion,
  title={Natural Questions: a Benchmark for Question Answering Research},
  author={Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
  year={2019},
  journal={Transactions of the Association of Computational Linguistics}
}

@article{sun2019investigating,
  title={Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension},
  author={Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  url={https://arxiv.org/abs/1904.09679v3}
}

@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794",
}


@article{nllb2022,
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author    = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year      = {2022}
}

@inproceedings{DBLP:conf/aaai/SakaguchiBBC20,
  author       = {Keisuke Sakaguchi and
                  Ronan Le Bras and
                  Chandra Bhagavatula and
                  Yejin Choi},
  title        = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {8732--8740},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6399},
  doi          = {10.1609/AAAI.V34I05.6399},
  timestamp    = {Mon, 04 Sep 2023 16:50:27 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ZellersHBFC19,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1472},
  doi          = {10.18653/V1/P19-1472},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/SuzgunSSGTCCLCZ23,
  author       = {Mirac Suzgun and
                  Nathan Scales and
                  Nathanael Sch{\"{a}}rli and
                  Sebastian Gehrmann and
                  Yi Tay and
                  Hyung Won Chung and
                  Aakanksha Chowdhery and
                  Quoc V. Le and
                  Ed H. Chi and
                  Denny Zhou and
                  Jason Wei},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve
                  Them},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2023,
                  Toronto, Canada, July 9-14, 2023},
  pages        = {13003--13051},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.824},
  doi          = {10.18653/V1/2023.FINDINGS-ACL.824},
  timestamp    = {Mon, 29 Jan 2024 20:33:18 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/SuzgunSSGTCCLCZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2110-14168,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.14168},
  eprinttype    = {arXiv},
  eprint       = {2110.14168},
  timestamp    = {Mon, 12 Jun 2023 08:23:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-14168.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/HendrycksBKABTS21,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HendrycksBKABTS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/ChenYKLWMXWX23,
  author       = {Wenhu Chen and
                  Ming Yin and
                  Max Ku and
                  Pan Lu and
                  Yixin Wan and
                  Xueguang Ma and
                  Jianyu Xu and
                  Xinyi Wang and
                  Tony Xia},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {TheoremQA: {A} Theorem-driven Question Answering Dataset},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {7889--7901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.489},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/ChenYKLWMXWX23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Fei2024QueryOC,
  title={Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora},
  author={Zhaoye Fei and Yunfan Shao and Linyang Li and Zhiyuan Zeng and Hang Yan and Xipeng Qiu and Dahua Lin},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.14624},
  url={https://api.semanticscholar.org/CorpusID:267301281}
}


@inproceedings{DBLP:conf/iclr/LoshchilovH19,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
  timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}

@article{chen2024agent,
  title={Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models},
  author={Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
  journal={work in progress},
  year={2024}
}

@inproceedings{ds-1000,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={ICLR},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}


@article{DBLP:journals/corr/abs-2306-01116,
  author       = {Guilherme Penedo and
                  Quentin Malartic and
                  Daniel Hesslow and
                  Ruxandra Cojocaru and
                  Alessandro Cappelli and
                  Hamza Alobeidli and
                  Baptiste Pannier and
                  Ebtesam Almazrouei and
                  Julien Launay},
  title        = {The RefinedWeb Dataset for Falcon {LLM:} Outperforming Curated Corpora
                  with Web Data, and Web Data Only},
  journal      = {CoRR},
  volume       = {abs/2306.01116},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.01116},
  doi          = {10.48550/ARXIV.2306.01116},
  eprinttype    = {arXiv},
  eprint       = {2306.01116},
  timestamp    = {Mon, 12 Jun 2023 16:25:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-01116.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2311-16867,
  author       = {Ebtesam Almazrouei and
                  Hamza Alobeidli and
                  Abdulaziz Alshamsi and
                  Alessandro Cappelli and
                  Ruxandra Cojocaru and
                  M{\'{e}}rouane Debbah and
                  {\'{E}}tienne Goffinet and
                  Daniel Hesslow and
                  Julien Launay and
                  Quentin Malartic and
                  Daniele Mazzotta and
                  Badreddine Noune and
                  Baptiste Pannier and
                  Guilherme Penedo},
  title        = {The Falcon Series of Open Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.16867},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.16867},
  doi          = {10.48550/ARXIV.2311.16867},
  eprinttype    = {arXiv},
  eprint       = {2311.16867},
  timestamp    = {Mon, 04 Dec 2023 14:05:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-16867.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2309-10305,
  author       = {Aiyuan Yang and
                  Bin Xiao and
                  Bingning Wang and
                  Borong Zhang and
                  Ce Bian and
                  Chao Yin and
                  Chenxu Lv and
                  Da Pan and
                  Dian Wang and
                  Dong Yan and
                  Fan Yang and
                  Fei Deng and
                  Feng Wang and
                  Feng Liu and
                  Guangwei Ai and
                  Guosheng Dong and
                  Haizhou Zhao and
                  Hang Xu and
                  Haoze Sun and
                  Hongda Zhang and
                  Hui Liu and
                  Jiaming Ji and
                  Jian Xie and
                  Juntao Dai and
                  Kun Fang and
                  Lei Su and
                  Liang Song and
                  Lifeng Liu and
                  Liyun Ru and
                  Luyao Ma and
                  Mang Wang and
                  Mickel Liu and
                  MingAn Lin and
                  Nuolan Nie and
                  Peidong Guo and
                  Ruiyang Sun and
                  Tao Zhang and
                  Tianpeng Li and
                  Tianyu Li and
                  Wei Cheng and
                  Weipeng Chen and
                  Xiangrong Zeng and
                  Xiaochuan Wang and
                  Xiaoxi Chen and
                  Xin Men and
                  Xin Yu and
                  Xuehai Pan and
                  Yanjun Shen and
                  Yiding Wang and
                  Yiyu Li and
                  Youxin Jiang and
                  Yuchen Gao and
                  Yupeng Zhang and
                  Zenan Zhou and
                  Zhiying Wu},
  title        = {Baichuan 2: Open Large-scale Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.10305},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.10305},
  doi          = {10.48550/ARXIV.2309.10305},
  eprinttype    = {arXiv},
  eprint       = {2309.10305},
  timestamp    = {Thu, 28 Sep 2023 16:22:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-10305.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/AinslieLJZLS23,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{GQA:} Training Generalized Multi-Query Transformer Models from Multi-Head
                  Checkpoints},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.298},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/AinslieLJZLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23,
  author       = {Aakanksha Chowdhery and
                  Sharan Narang and
                  Jacob Devlin and
                  Maarten Bosma and
                  Gaurav Mishra and
                  Adam Roberts and
                  Paul Barham and
                  Hyung Won Chung and
                  Charles Sutton and
                  Sebastian Gehrmann and
                  Parker Schuh and
                  Kensen Shi and
                  Sasha Tsvyashchenko and
                  Joshua Maynez and
                  Abhishek Rao and
                  Parker Barnes and
                  Yi Tay and
                  Noam Shazeer and
                  Vinodkumar Prabhakaran and
                  Emily Reif and
                  Nan Du and
                  Ben Hutchinson and
                  Reiner Pope and
                  James Bradbury and
                  Jacob Austin and
                  Michael Isard and
                  Guy Gur{-}Ari and
                  Pengcheng Yin and
                  Toju Duke and
                  Anselm Levskaya and
                  Sanjay Ghemawat and
                  Sunipa Dev and
                  Henryk Michalewski and
                  Xavier Garcia and
                  Vedant Misra and
                  Kevin Robinson and
                  Liam Fedus and
                  Denny Zhou and
                  Daphne Ippolito and
                  David Luan and
                  Hyeontaek Lim and
                  Barret Zoph and
                  Alexander Spiridonov and
                  Ryan Sepassi and
                  David Dohan and
                  Shivani Agrawal and
                  Mark Omernick and
                  Andrew M. Dai and
                  Thanumalayan Sankaranarayana Pillai and
                  Marie Pellat and
                  Aitor Lewkowycz and
                  Erica Moreira and
                  Rewon Child and
                  Oleksandr Polozov and
                  Katherine Lee and
                  Zongwei Zhou and
                  Xuezhi Wang and
                  Brennan Saeta and
                  Mark Diaz and
                  Orhan Firat and
                  Michele Catasta and
                  Jason Wei and
                  Kathy Meier{-}Hellstern and
                  Douglas Eck and
                  Jeff Dean and
                  Slav Petrov and
                  Noah Fiedel},
  title        = {PaLM: Scaling Language Modeling with Pathways},
  journal      = {J. Mach. Learn. Res.},
  volume       = {24},
  pages        = {240:1--240:113},
  year         = {2023},
  url          = {http://jmlr.org/papers/v24/22-1144.html},
  timestamp    = {Thu, 19 Oct 2023 09:44:46 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/ZhangS19a,
  author       = {Biao Zhang and
                  Rico Sennrich},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Root Mean Square Layer Normalization},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {12360--12371},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
  timestamp    = {Fri, 21 Oct 2022 14:36:34 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ZhangS19a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-05202,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BaKH16,
  author       = {Lei Jimmy Ba and
                  Jamie Ryan Kiros and
                  Geoffrey E. Hinton},
  title        = {Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype    = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/LabanKAFXJW23,
  author       = {Philippe Laban and
                  Wojciech Kryscinski and
                  Divyansh Agarwal and
                  Alexander R. Fabbri and
                  Caiming Xiong and
                  Shafiq Joty and
                  Chien{-}Sheng Wu},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {SummEdits: Measuring {LLM} Ability at Factual Reasoning Through The
                  Lens of Summarization},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {9662--9676},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.600},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LabanKAFXJW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/aaai/BiskZLGC20,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/AAAI.V34I05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/MihaylovCKS18,
  author       = {Todor Mihaylov and
                  Peter Clark and
                  Tushar Khot and
                  Ashish Sabharwal},
  editor       = {Ellen Riloff and
                  David Chiang and
                  Julia Hockenmaier and
                  Jun'ichi Tsujii},
  title        = {Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open
                  Book Question Answering},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural
                  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages        = {2381--2391},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/d18-1260},
  doi          = {10.18653/V1/D18-1260},
  timestamp    = {Fri, 06 Aug 2021 00:40:21 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/MihaylovCKS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Anonymouscibench,
  title={CIBench: Evaluating Your LLMs with a Code Interpreter Plugin},
  author={Anonymous},
  booktitle={Openreview},
  year={2024},
  url={https://openreview.net/forum?id=O8jmCw5puG}
}

@inproceedings{Anonymousmathbench,
  title={MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark},
  author={Anonymous},
  booktitle={Openreview},
  year={2024},
  url={https://openreview.net/forum?id=4vRO48RwVG}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{DBLP:journals/corr/abs-2307-11088,
  author       = {Chenxin An and
                  Shansan Gong and
                  Ming Zhong and
                  Mukai Li and
                  Jun Zhang and
                  Lingpeng Kong and
                  Xipeng Qiu},
  title        = {L-Eval: Instituting Standardized Evaluation for Long Context Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2307.11088},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.11088},
  doi          = {10.48550/ARXIV.2307.11088},
  eprinttype    = {arXiv},
  eprint       = {2307.11088},
  timestamp    = {Wed, 30 Aug 2023 16:03:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-11088.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2308-14508,
  author       = {Yushi Bai and
                  Xin Lv and
                  Jiajie Zhang and
                  Hongchang Lyu and
                  Jiankai Tang and
                  Zhidian Huang and
                  Zhengxiao Du and
                  Xiao Liu and
                  Aohan Zeng and
                  Lei Hou and
                  Yuxiao Dong and
                  Jie Tang and
                  Juanzi Li},
  title        = {LongBench: {A} Bilingual, Multitask Benchmark for Long Context Understanding},
  journal      = {CoRR},
  volume       = {abs/2308.14508},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.14508},
  doi          = {10.48550/ARXIV.2308.14508},
  eprinttype    = {arXiv},
  eprint       = {2308.14508},
  timestamp    = {Wed, 15 Nov 2023 15:09:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-14508.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}
@article{parmar2024nemotron,
  title={Nemotron-4 15B Technical Report},
  author={Parmar, Jupinder and Prabhumoye, Shrimai and Jennings, Joseph and Patwary, Mostofa and Subramanian, Sandeep and Su, Dan and Zhu, Chen and Narayanan, Deepak and Jhunjhunwala, Aastha and Dattagupta, Ayush and others},
  journal={arXiv preprint arXiv:2402.16819},
  year={2024}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}
@inproceedings{romero2022accelerating,
  title={Accelerating collective communication in data parallel training across deep learning frameworks},
  author={Romero, Joshua and Yin, Junqi and Laanait, Nouamane and Xie, Bing and Young, M Todd and Treichler, Sean and Starchenko, Vitalii and Borisevich, Albina and Sergeev, Alex and Matheson, Michael},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages={1027--1040},
  year={2022}
}

@article{DBLP:journals/corr/abs-2304-08354,
  author       = {Yujia Qin and
                  Shengding Hu and
                  Yankai Lin and
                  Weize Chen and
                  Ning Ding and
                  Ganqu Cui and
                  Zheni Zeng and
                  Yufei Huang and
                  Chaojun Xiao and
                  Chi Han and
                  Yi Ren Fung and
                  Yusheng Su and
                  Huadong Wang and
                  Cheng Qian and
                  Runchu Tian and
                  Kunlun Zhu and
                  Shihao Liang and
                  Xingyu Shen and
                  Bokai Xu and
                  Zhen Zhang and
                  Yining Ye and
                  Bowen Li and
                  Ziwei Tang and
                  Jing Yi and
                  Yuzhang Zhu and
                  Zhenning Dai and
                  Lan Yan and
                  Xin Cong and
                  Yaxi Lu and
                  Weilin Zhao and
                  Yuxiang Huang and
                  Junxi Yan and
                  Xu Han and
                  Xian Sun and
                  Dahai Li and
                  Jason Phang and
                  Cheng Yang and
                  Tongshuang Wu and
                  Heng Ji and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {Tool Learning with Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.08354},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.08354},
  doi          = {10.48550/ARXIV.2304.08354},
  eprinttype    = {arXiv},
  eprint       = {2304.08354},
  timestamp    = {Fri, 01 Sep 2023 13:50:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-08354.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2307-04964,
  author       = {Rui Zheng and
                  Shihan Dou and
                  Songyang Gao and
                  Yuan Hua and
                  Wei Shen and
                  Binghai Wang and
                  Yan Liu and
                  Senjie Jin and
                  Qin Liu and
                  Yuhao Zhou and
                  Limao Xiong and
                  Lu Chen and
                  Zhiheng Xi and
                  Nuo Xu and
                  Wenbin Lai and
                  Minghao Zhu and
                  Cheng Chang and
                  Zhangyue Yin and
                  Rongxiang Weng and
                  Wensen Cheng and
                  Haoran Huang and
                  Tianxiang Sun and
                  Hang Yan and
                  Tao Gui and
                  Qi Zhang and
                  Xipeng Qiu and
                  Xuanjing Huang},
  title        = {Secrets of {RLHF} in Large Language Models Part {I:} {PPO}},
  journal      = {CoRR},
  volume       = {abs/2307.04964},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.04964},
  doi          = {10.48550/ARXIV.2307.04964},
  eprinttype    = {arXiv},
  eprint       = {2307.04964},
  timestamp    = {Fri, 27 Oct 2023 10:07:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-04964.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2305-18290,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Stefano Ermon and
                  Christopher D. Manning and
                  Chelsea Finn},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a
                  Reward Model},
  journal      = {CoRR},
  volume       = {abs/2305.18290},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.18290},
  doi          = {10.48550/ARXIV.2305.18290},
  eprinttype    = {arXiv},
  eprint       = {2305.18290},
  timestamp    = {Wed, 07 Jun 2023 14:31:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-18290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/Ouyang0JAWMZASR22,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{DBLP:journals/corr/abs-2307-09288,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2307-16789,
  author       = {Yujia Qin and
                  Shihao Liang and
                  Yining Ye and
                  Kunlun Zhu and
                  Lan Yan and
                  Yaxi Lu and
                  Yankai Lin and
                  Xin Cong and
                  Xiangru Tang and
                  Bill Qian and
                  Sihan Zhao and
                  Runchu Tian and
                  Ruobing Xie and
                  Jie Zhou and
                  Mark Gerstein and
                  Dahai Li and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world
                  APIs},
  journal      = {CoRR},
  volume       = {abs/2307.16789},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.16789},
  doi          = {10.48550/ARXIV.2307.16789},
  eprinttype    = {arXiv},
  eprint       = {2307.16789},
  timestamp    = {Tue, 07 Nov 2023 17:01:53 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-16789.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2302-04761,
  author       = {Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Roberto Dess{\`{\i}} and
                  Roberta Raileanu and
                  Maria Lomeli and
                  Luke Zettlemoyer and
                  Nicola Cancedda and
                  Thomas Scialom},
  title        = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  journal      = {CoRR},
  volume       = {abs/2302.04761},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.04761},
  doi          = {10.48550/ARXIV.2302.04761},
  eprinttype    = {arXiv},
  eprint       = {2302.04761},
  timestamp    = {Mon, 13 Feb 2023 14:23:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-04761.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2023t,
  title={T-eval: Evaluating the tool utilization capability step by step},
  author={Chen, Zehui and Du, Weihua and Zhang, Wenwei and Liu, Kuikun and Liu, Jiangning and Zheng, Miao and Zhuo, Jingming and Zhang, Songyang and Lin, Dahua and Chen, Kai and others},
  journal={arXiv preprint arXiv:2312.14033},
  year={2023}
}

@inproceedings{DBLP:conf/iclr/YaoZYDSN023,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=WE\_vluYUL-X},
  timestamp    = {Wed, 16 Aug 2023 16:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}


@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}


@article{ke2023critiquellm,
  title={Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation},
  author={Ke, Pei and Wen, Bosi and Feng, Zhuoer and Liu, Xiao and Lei, Xuanyu and Cheng, Jiale and Wang, Shengyuan and Zeng, Aohan and Dong, Yuxiao and Wang, Hongning and others},
  journal={arXiv preprint arXiv:2311.18702},
  year={2023}
}


@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}

@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
      journal={arXiv preprint arXiv:2305.20050},
      year={2023}
}

@InProceedings{pmlr-v162-ethayarajh22a,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th ICLR},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher = {PMLR},
}


@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{DBLP:journals/corr/abs-2309-16609,
  author       = {Jinze Bai and
                  Shuai Bai and
                  Yunfei Chu and
                  Zeyu Cui and
                  Kai Dang and
                  Xiaodong Deng and
                  Yang Fan and
                  Wenbin Ge and
                  Yu Han and
                  Fei Huang and
                  Binyuan Hui and
                  Luo Ji and
                  Mei Li and
                  Junyang Lin and
                  Runji Lin and
                  Dayiheng Liu and
                  Gao Liu and
                  Chengqiang Lu and
                  Keming Lu and
                  Jianxin Ma and
                  Rui Men and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Chuanqi Tan and
                  Sinan Tan and
                  Jianhong Tu and
                  Peng Wang and
                  Shijie Wang and
                  Wei Wang and
                  Shengguang Wu and
                  Benfeng Xu and
                  Jin Xu and
                  An Yang and
                  Hao Yang and
                  Jian Yang and
                  Shusheng Yang and
                  Yang Yao and
                  Bowen Yu and
                  Hongyi Yuan and
                  Zheng Yuan and
                  Jianwei Zhang and
                  Xingxuan Zhang and
                  Yichang Zhang and
                  Zhenru Zhang and
                  Chang Zhou and
                  Jingren Zhou and
                  Xiaohuan Zhou and
                  Tianhang Zhu},
  title        = {Qwen Technical Report},
  journal      = {CoRR},
  volume       = {abs/2309.16609},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.16609},
  doi          = {10.48550/ARXIV.2309.16609},
  eprinttype    = {arXiv},
  eprint       = {2309.16609},
  timestamp    = {Wed, 18 Oct 2023 08:13:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-16609.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished={\url{https://github.com/open-compass/opencompass}},
    year={2023}
}



@article{yue2024mmmu,
  title={Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yin, Ming and Yu, Botao and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}

@article{meng2024mmiu,
  title={MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models},
  author={Meng, Fanqing and Wang, Jin and Li, Chuanhao and Lu, Quanfeng and Tian, Hao and Liao, Jiaqi and Zhu, Xizhou and Dai, Jifeng and Qiao, Yu and Luo, Ping and others},
  journal={arXiv preprint arXiv:2408.02718},
  year={2024}
}

@inproceedings{zhang2025mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},
  booktitle={European Conference on Computer Vision},
  pages={169--186},
  year={2025},
  organization={Springer}
}

@article{liu2024mmdu,
  title={MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs},
  author={Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2406.11833},
  year={2024}
}

@article{wang2024muirbench,
  title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
  author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2406.09411},
  year={2024}
}

@article{fu2024blink,
  title={BLINK: Multimodal Large Language Models Can See but Not Perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2404.12390},
  year={2024}
}

@article{li2024r,
  title={R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions?},
  author={Li, Chunyi and Zhang, Jianbo and Zhang, Zicheng and Wu, Haoning and Tian, Yuan and Sun, Wei and Lu, Guo and Liu, Xiaohong and Min, Xiongkuo and Lin, Weisi and others},
  journal={arXiv preprint arXiv:2410.05474},
  year={2024}
}

@misc{ma2024mmlong,
      title={MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations}, 
      author={Yubo Ma and Yuhang Zang and Liangyu Chen and Meiqi Chen and Yizhu Jiao and Xinze Li and Xinyuan Lu and Ziyu Liu and Yan Ma and Xiaoyi Dong and Pan Zhang and Liangming Pan and Yu-Gang Jiang and Jiaqi Wang and Yixin Cao and Aixin Sun},
      year={2024},
      eprint={2407.01523},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.01523}, 
}

@inproceedings{van2023document,
  title={Document Understanding Dataset and Evaluation (DUDE)},
  author={Van Landeghem, Jordy and Tito, Ruben and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawe{\l} and Powalski, Rafa{\l} and Jurkiewicz, Dawid and Coustaty, Mickael and Ackaert, Bertrand and Valveny, Ernest and others},
  booktitle={Proceedings IEEE/CVF international conference on computer vision-ICCV 2023},
  pages={19528--19540},
  year={2023},
  organization={IEEE/CVF}
}

@article{wang2023amber,
  title={Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
  author={Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
  journal={arXiv preprint arXiv:2311.07397},
  year={2023}
}

@article{zhang2024mme,
  title={MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
  author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
  journal={arXiv preprint arXiv:2408.13257},
  year={2024}
}

@article{zhang2024task,
  title={Task Me Anything},
  author={Zhang, Jieyu and Huang, Weikai and Ma, Zixian and Michel, Oscar and He, Dong and Gupta, Tanmay and Ma, Wei-Chiu and Farhadi, Ali and Kembhavi, Aniruddha and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2406.11775},
  year={2024}
}

@misc{wang2024measuring,
      title={Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset}, 
      author={Ke Wang and Junting Pan and Weikang Shi and Zimu Lu and Mingjie Zhan and Hongsheng Li},
      year={2024},
      eprint={2402.14804},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{sun2024mm,
  title={MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification},
  author={Sun, Kai and Bai, Yushi and Qi, Ji and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2404.05091},
  year={2024}
}

@article{sun2024parrot,
  title={Parrot: Multilingual Visual Instruction Tuning},
  author={Sun, Hai-Long and Zhou, Da-Wei and Li, Yang and Lu, Shiyin and Yi, Chao and Chen, Qing-Guo and Xu, Zhao and Luo, Weihua and Zhang, Kaifu and Zhan, De-Chuan and others},
  journal={arXiv preprint arXiv:2406.02539},
  year={2024}
}

@misc{tang2024mtvqa,
      title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering}, 
      author={Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and Yanjie Wang and Yuliang Liu and Hao Liu and Xiang Bai and Can Huang},
      year={2024},
      eprint={2405.11985},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{matos2024worldmedqa,
  title={WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation},
  author={Matos, Jo{\~a}o and Chen, Shan and Placino, Siena and Li, Yingya and Pardo, Juan Carlos Climent and Idan, Daphna and Tohyama, Takeshi and Restrepo, David and Nakayama, Luis F and Pascual-Leone, Jose MM and others},
  journal={arXiv preprint arXiv:2410.12722},
  year={2024}
}

@article{chen2024gmai,
  title={Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai},
  author={Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and others},
  journal={arXiv preprint arXiv:2408.03361},
  year={2024}
}

@article{wang2024needle,
  title={Needle In A Multimodal Haystack}, 
  author={Wang, Weiyun and Zhang, Shuibo and Ren, Yiming and Duan, Yuchen and Li, Tiantong and Liu, Shuo and Hu, Mengkang and Chen, Zhe and Zhang, Kaipeng and Lu, Lewei and Zhu, Xizhou and Luo, Ping and Qiao, Yu and Dai, Jifeng and Shao, Wenqi and Wang, Wenhai},
  journal={arXiv preprint arXiv:2406.07230},
  year={2024}
}

@inproceedings{tanaka2023slidevqa,
  title={Slidevqa: A dataset for document visual question answering on multiple images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13636--13645},
  year={2023}
}

@article{rohrbach2018object,
  title={Object hallucination in image captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1809.02156},
  year={2018}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@article{xia2024docgenome,
  title={DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models},
  author={Xia, Renqiu and Mao, Song and Yan, Xiangchao and Zhou, Hongbin and Zhang, Bo and Peng, Haoyang and Pi, Jiahao and Fu, Daocheng and Wu, Wenjie and Ye, Hancheng and others},
  journal={arXiv preprint arXiv:2406.11633},
  year={2024}
}

@misc{realworldqa,
  title = {Grok-1.5 Vision Preview: Connecting the digital and physical worlds with our first multimodal model.},
  author = {X.AI Corp.},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={11--20},
  year={2016}
}

@article{yu2024mmvetv2,
  title={Mm-vet v2: A challenging benchmark to evaluate large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Ren, Linfeng and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2408.00765},
  year={2024}
}

@article{huang2024aesbench,
  title={Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception},
  author={Huang, Yipo and Yuan, Quan and Sheng, Xiangfei and Yang, Zhichao and Wu, Haoning and Chen, Pengfei and Yang, Yuzhe and Li, Leida and Lin, Weisi},
  journal={arXiv preprint arXiv:2401.08276},
  year={2024}
}

@article{ying2024mmt,
  title={Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and others},
  journal={arXiv preprint arXiv:2404.16006},
  year={2024}
}

@inproceedings{wu2024qbench,
    author = {Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and Lin, Weisi},
    title = {Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision},
    booktitle = {ICLR},
    year = {2024}
}

@article{zhang2024bench,
  title={A-Bench: Are LMMs Masters at Evaluating AI-generated Images?},
  author={Zhang, Zicheng and Wu, Haoning and Li, Chunyi and Zhou, Yingjie and Sun, Wei and Min, Xiongkuo and Chen, Zijian and Liu, Xiaohong and Lin, Weisi and Zhai, Guangtao},
  journal={arXiv preprint arXiv:2406.03070},
  year={2024}
}

@article{chen2024mmstar,
  title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2403.20330},
  year={2024}
}


@article{li2023seed2,
  title={SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2311.17092},
  year={2023}
}

@article{li2024omnicorpus,
  title={OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text},
  author={Li, Qingyun and Chen, Zhe and Wang, Weiyun and Wang, Wenhai and Ye, Shenglong and Jin, Zhenjiang and Chen, Guanzhou and He, Yinan and Gao, Zhangwei and Cui, Erfei and others},
  journal={arXiv preprint arXiv:2406.08418},
  year={2024}
}

@article{wang2024mmniah,
  title={Needle In A Multimodal Haystack},
  author={Wang, Weiyun and Zhang, Shuibo and Ren, Yiming and Duan, Yuchen and Li, Tiantong and Liu, Shuo and Hu, Mengkang and Chen, Zhe and Zhang, Kaipeng and Lu, Lewei and others},
  journal={arXiv preprint arXiv:2406.07230},
  year={2024}
}

@article{yao2024minicpm_v,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{laurenccon2024obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={NIPS},
  volume={36},
  year={2024}
}

@article{zhu2024mmc4,
  title={Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author={Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal={NIPS},
  volume={36},
  year={2024}
}

@inproceedings{xu2020layoutlm,
  title={Layoutlm: Pre-training of text and layout for document image understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1192--1200},
  year={2020}
}

@article{zhang2024internlm,
  title={Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output},
  author={Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Cao, Yuhang and Qian, Rui and Chen, Lin and Guo, Qipeng and Duan, Haodong and Wang, Bin and Ouyang, Linke and others},
  journal={arXiv preprint arXiv:2407.03320},
  year={2024}
}

@article{cho2024m3docrag,
  title={M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding},
  author={Cho, Jaemin and Mahata, Debanjan and Irsoy, Ozan and He, Yujie and Bansal, Mohit},
  journal={arXiv preprint arXiv:2411.04952},
  year={2024}
}

@article{gao2024mini_internvl,
  title={Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5\% Parameters and 90\% Performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2410.16261},
  year={2024}
}

@misc{gpt4o,
  title={GPT-4o System Card},
  author={OpenAI},
  howpublished={\url{https://openai.com/index/gpt-4o-system-card/}},
  year={2024}
}

@article{tu2024overview,
  title={An overview of large AI models and their applications},
  author={Tu, Xiaoguang and He, Zhi and Huang, Yi and Zhang, Zhi-Hao and Yang, Ming and Zhao, Jian},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={1--22},
  year={2024},
  publisher={Springer}
}

@article{xie2024fusionmamba,
  title={Fusionmamba: Dynamic feature enhancement for multimodal image fusion with mamba},
  author={Xie, Xinyu and Cui, Yawen and Tan, Tao and Zheng, Xubin and Yu, Zitong},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={37},
  year={2024},
  publisher={Springer}
}

@article{jiang2024effectiveness,
  title={Effectiveness assessment of recent large vision-language models},
  author={Jiang, Yao and Yan, Xinyu and Ji, Ge-Peng and Fu, Keren and Sun, Meijun and Xiong, Huan and Fan, Deng-Ping and Khan, Fahad Shahbaz},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={17},
  year={2024},
  publisher={Springer}
}