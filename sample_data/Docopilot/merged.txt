% ===== main.tex =====

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}

\input{preamble}
\usepackage{CJKutf8}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{marvosym}
\usepackage[accsupp]{axessibility}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\def\confName{CVPR}
\def\confYear{2025}


\newcommand{\promptbox}[1]{
\begin{center}
\setlength{\fboxsep}{6pt} 
\fbox{
    \parbox{0.95\linewidth}{
        \renewcommand{\baselinestretch}{0.8}\selectfont 
        \vspace{3pt} 
        \texttt{\scriptsize #1} 
        \vspace{3pt}
    }
}
\end{center}}

\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

\def\modelname{Docopilot\xspace}
\def\dataname{Doc-750K\xspace}


\title{Docopilot: Improving Multimodal Models for Document-Level Understanding}

\author{\scalebox{0.91}{Yuchen Duan$^{1,2*}$, Zhe Chen$^{3, 1*}$, Yusong Hu$^{4,1*}$, Weiyun Wang$^{*5,1}$, Shenglong Ye$^1$, Botian Shi$^1$,} \\
\vspace{4px}
\scalebox{0.91}{~Lewei Lu$^7$, Qibin Hou$^4$, Tong Lu$^{3,1}$, Hongsheng Li$^{2,1}$, Jifeng Dai$^{6,1}$, Wenhai Wang$^{2,1}$\textsuperscript{\Letter}} \\
\scalebox{0.91}{~$^1$Shanghai AI Laboratory,~$^2$The Chinese University of Hong Kong, $^3$Nanjing University,} \\
\scalebox{0.91}{~$^4$Nankai University, $^5$Fudan University, $^6$Tsinghua University, $^7$SenseTime Research}
\vspace{-2ex}
}

\begin{document}
\begin{CJK}{UTF8}{gbsn} 
\maketitle

\blfootnote{*\ Equal contribution; \\ \indent ~\Letter\ Corresponding author:~wangwenhai@pjlab.org.cn}

\begin{abstract}
Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. 
In this work, we present a high-quality document-level dataset, \dataname, designed to support in-depth understanding of multimodal documents.
This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents.
Building on the dataset, we develop a native multimodal modelâ€”\modelname, which can accurately handle document-level dependencies without relying on RAG.
Experiments demonstrate that \modelname achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. 
Data, code, and models are released at \url{https://github.com/OpenGVLab/Docopilot}.
\end{abstract}


\vspace{-2ex}
\section{Introduction}
\label{sec:intro}
In recent years, multimodal large language models (MLLMs)~\cite{liu2023improved, chen2023internvl, bai2023qwenvl, wang2024qwen2vl, wang2023allseeing, wang2023cogvlm, gpt4v, team2023gemini, reid2024gemini1_5, yao2024minicpm_v} have rapidly developed, achieving remarkable performance in various visual understanding tasks~\cite{tu2024overview, jiang2024effectiveness}, particularly image-level tasks, such as image captioning~\cite{chen2015cococaption, liu2023llava}, optical character recognition (OCR)~\cite{liu2023ocrbench, singh2019textvqa}, and visual question answering (VQA)~\cite{liu2023mmbench, fu2023mme}.
Despite these advances, current MLLMs still face significant challenges in document-level understanding~\cite{ma2024mmlong, xia2024docgenome, tito2023mpdocvqa}, where models are required to identify and integrate key information across multi-page documents, setting high expectations for their long-context processing capabilities of MLLMs.


\begin{figure}[t!]
    \centering
    {\includegraphics[width=\linewidth]{figure/teaser.pdf}}
    \caption{\textbf{Accuracy \emph{v.s} inference latency on MM-NIAH.} 
    The proposed \modelname-8B shows a notable improvement over baseline models~\cite{InternVL2}, achieving a +19.9\% accuracy gain compared to InternVL2-8B and surpassing InternVL2-26B with less than 31\% of the inference latency. Additionally, \modelname-2B uses fewer parameters (less than 10\%) while exhibiting comparable performance to the 10$\times$ larger InternVL2-26B. These results suggest that our \modelname strikes a reasonable balance between latency, model size, and performance.}
    \label{fig:teaser}
    \vspace{-3mm}
\end{figure}

Current research on long-content understanding primarily focuses on text-only models~\cite{qwen, qwen2.5, cai2024internlm2}, targeting specific retrieval tasks such as ``Needle in a Haystack'' (NIAH)~\cite{an2023leval, LLMTest_NeedleInAHaystack}.
However, existing open-source MLLMs \cite{chen2023internvl, wang2024qwen2vl, li2024llavaonevision, chen2024far, liu2023improved,shi2024eagle,wang2024allseeingv2} are primarily trained on image-level data, lacking the long-context understanding capacity required for document-level understanding.
Retrieval-augmented generation (RAG) methods~\cite{yu2024visrag, cho2024m3docrag,faysse2024colpali,ma2024unifying,sharifymoghaddam2024unirag,shi2023replug} attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks.
(1) \emph{Fragmented Retrieval Contexts}. Retrieved information is fragmented, lacking the overall structure of the document; 
(2) \emph{Multi-Stage Error Accumulation}. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks;
(3) \emph{Extra Time Costs}. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.



To address these problems, two primary challenges need to be considered.
(1) \emph{High-Quality Multimodal Document Dataset}. While extensive datasets \cite{chen2023longlora, yang2023longqlora, zhang2024longreward, zhang2024longcite} exist for long-context, text-only tasks, high-quality document-level question-answering datasets remain scarce. This shortage is largely attributed to the high costs associated with annotation and the lack of streamlined construction pipelines.
(2) \emph{Native Document-Level MLLMs}. Although RAG-based methods \cite{yu2024visrag, cho2024m3docrag, wang2024mmniah} provide some relief, native multimodal models with long-context processing abilities are crucial. However, training native MLLMs specifically for document-level understanding is constrained by current hardware limitations.

In this work, we introduce a new multimodal document dataset that supports document-level understanding tasks. Compared to counterparts \cite{van2023document, laurenccon2024docmatix, tito2023mpdocvqa}, this dataset has the following features:
(1) \emph{Large Scale}. It includes a total of 758K question-answer samples, containing 5.2B text tokens and 3.1M images. It encompasses content from various sources, such as Sci-Hub, Arxiv, and OpenReview, covering a wide range of topics and document layouts.
(2) \emph{High Quality}. Unlike existing datasets that insert irrelevant questions into documents, we collect real, in-depth question-answer pairs and construct single-page and cross-page questions based on document structure. Such high-quality question-answer data accounts for 31.6\% of the dataset. 
(3) \emph{Multimodal}. For the document content, we provide not only the conventional interleaved text-image context but also purely rendered image inputs, catering to the needs of different models.


Building upon this dataset, we developed a native baseline model for document-level multimodal understanding--\modelname.
Unlike existing approaches~\cite{yu2024visrag, cho2024m3docrag, wang2024mmniah} that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention \cite{liu2023ring}, and Liger Kernel \cite{dai2024ligerkernel}.
Leveraging the proxy tasks carefully designed within the dataset, \modelname can directly handle long-distance dependencies and cross-page information integration without external retrieval support.
As shown in Figure \ref{fig:teaser}, this approach not only enhances coherence and accuracy compared to RAG methods but also significantly reduces the response time of the entire question-answering system, delivering superior real-time performance in multi-turn interactions.

The main contributions are summarized as follows:

(1) We develop the first large-scale, high-quality dataset for document-level multimodal understanding, consisting of 758K QA pairs from 3 sources, supporting 9 types of proxy tasks. This dataset includes 31.6\% real QA pairs directly extracted from documents.

(2) Based on the dataset, we implement Docopilot, a native MLLM designed for document-level understanding without relying on retrieval mechanisms. 
This approach greatly improves its ability to integrate and comprehend information across multi-page documents.

(3) Through extensive experiments on multiple document-level benchmarks, our method demonstrates performance significantly superior to existing approaches, proving its effectiveness and generality. 
As shown in Figure \ref{fig:teaser}, \modelname-8B achieves a score of 61.8 on MM-NIAH~\cite{wang2024mmniah}, outperforming InternVL2-8B by 19.9 points and surpassing InternVL2-26B with less than 31\% of the latency.
We hope this work could provide a baseline for future advancements in MLLMs for document-level tasks.


\section{Related Work}

\subsection{Multimodal Large Language Models}

Multimodal large language models (MLLMs) have demonstrated impressive capabilities in processing image and text information, opening up new directions for applications such as visual question answering and image captioning.
Early models~\cite{openclip, rebuffi2017learning, li2022blip, chen2023internvl} trained with contrastive learning methods excelled in recognizing and understanding open-world semantics within an image-text matching framework. However, their limited generative abilities restricted their applicability.
To leverage the powerful generation abilities of large language models (LLMs), subsequent works~\cite{li2023blip2, liu2023llava, chen2024far, wang2023allseeing, wang2024allseeingv2,2023interngpt} introduced a connector to align the embedding spaces of vision encoders and LLMs, allowing encoded image embeddings to serve as soft prompts for LLMs. 
Another series of works~\cite{alayrac2022flamingo,li2024omnicorpus,laurenccon2024obelics,zhu2024mmc4} extended LLMs by integrating additional visual experts, reducing reliance on standalone vision encoders.
More recently, models capable of both understanding and generating images have also made notable progress~\cite{tian2024mminterleaved, dong2023dreamllm, sun2023emu, li2023seed}, leveraging the insight that image generation can enhance image understanding.
Despite these advancements, current MLLMs still face challenges with long-context multimodal inputs. For instance, InternVL 2.0~\cite{chen2024far,gao2024mini_internvl} performs optimally within a token range of up to 8192, constraining its effectiveness in document-level applications.


\begin{figure*}[t!]
    \centering
    {\includegraphics[width=\linewidth]{figure/data_engine.pdf}}
    \caption{
    \textbf{Multimodal document dataset generation pipeline.} 
    This pipeline involves three main stages: 
    (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats. 
    (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format. 
    (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o.
    }
    \label{fig:data_pipeline}
    \vspace{-2mm}
\end{figure*}


\subsection{Document Understanding Models}

Extracting key information from documents is crucial for industries and academic research.
OCR-model-driven methods~\cite{wang2023docllm,appalaraju2024docformerv2,bai2022wukong,tang2023unifying} represent one of the primary technical approaches. These methods extract text, layout, and bounding box information from external systems and integrate it with another model. However, they are prone to error propagation and high processing times due to their reliance on multiple components.
Benefitting from the rapid advancements in LLMs, OCR-free methods have also achieved great progress. Donut~\cite{kim2022ocr} is the first end-to-end training framework based on a Transformer without requiring OCR engines or APIs. 
Subsequent works \cite{wang2023towards,feng2023docpedia,ye2023ureader,wei2023vary,luo2024layoutllm} propose diverse modifications in model architectures and training algorithms.
However, these models are designed for specific tasks and lack general abilities.


\subsection{Long-Context Large Language Models}

With advancements in engineering, architecture, and algorithms, long-context large language models have made substantial progress. Techniques such as Flash Attention \cite{dao2022flashattention,dao2023flashattention} and Ring Attention \cite{liu2023ring} have notably reduced GPU memory usage for training on extended contexts. Additionally, various sparse attention mechanismsâ€”including Shifted Sparse Attention~\cite{chen2023longlora}, Dilated Attention~\cite{ding2023longnet}, and Attention Sinks~\cite{han2023lm,xiao2023efficient}â€”have enabled efficient scaling to handle larger contexts. 
New positional embedding methods, like ALiBi~\cite{press2021train}, xPOS~\cite{sun2022length}, and RoPE~\cite{su2024roformer}, further enhance the modelsâ€™ generalization capabilities in length extrapolation.
However, these advancements remain largely confined to natural language processing, and methods to extend the context size of MLLMs are still under-explored.
Another research approach aims to reduce context size by leveraging retrieval augmented generation (RAG)~\cite{yu2024visrag,faysse2024colpali, cho2024m3docrag}, where only the most relevant passages are retrieved and fed into the generation model. However, this retrieval-based approach can disrupt the coherence of the semantic chain, particularly in complex reasoning tasks, due to fragmented information flow.
In this work, we integrate the above engineering techniques into MLLMs and demonstrate that a model fine-tuned on a high-quality, long-context training corpus is a strong baseline, achieving superior performance compared to its RAG counterpart.


\begin{figure*}[t!]
    \centering
    {\includegraphics[width=\linewidth]{figure/example.pdf}}
    \caption{\textbf{Visualization of an example from {\dataname}.} The left side presents the interleaved text-image format data obtained through Document Content Extraction, while the right side showcases the annotations generated via Question-Answer Pairs Construction.}
    \label{fig:data_example}
    \vspace{-3mm}
\end{figure*}


\section{Multimodal Document Dataset Generation}

In this section, we begin by introducing the details of the data engine.
Following this, we provide a comprehensive overview of the dataset---\dataname.

\newcommand\hys[1]{\textcolor{red}{#1}}
\subsection{Data Engine} 
\label{sec:data_engine}
The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.
Specifically, we first extract multimodal content, including both text and images, from the documents. Based on this extracted content, we then create question-answer pairs. The document content and these constructed pairs are combined to produce conversational-style training data. The format is outlined as: 


\begin{center}
\small
\fbox{
\parbox{0.95\linewidth}{
\texttt{Please read the paper: <paper>, and answer the question: <question> Answer: <answer>}
}
}
\end{center}

\noindent Here, the \texttt{<paper>}, \texttt{<question>}, and \texttt{<answer>} are the placeholder for extracted document content, the generated questions and answer, respectively. In the following, we will provide a detailed explanation of the two key steps: document content extraction and QA pair construction.

\newcommand\ysl[1]{\textcolor{blue}{#1}}

\noindent\textbf{Document Content Extraction.}
In practical applications, different documents have varying page layouts and content types, which poses significant challenges for content extraction. To enhance the efficiency of multimodal models, it is necessary to organize documents into a unified format for streamlined processing.
In this work, we process each document into two formats as follows:

(1) \emph{Interleaved Text-Image Format.} Using the document content extractor MinerU~\cite{wang2024mineru}, we segment the document content into interleaved text and image annotations, for example, 
``\texttt{<text>\textbackslash n<image>\textbackslash n<text>\textbackslash n<image>}''
This format captures the documentâ€™s textual content, making it easier to construct question-answer pairs. 


(2) \emph{Multi-Image Format.} In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page. The structure follows the pattern ``\texttt{<image>\textbackslash n<image>\textbackslash n<image>}''. This format preserves the original layout, enabling the model to learn the overall pagination and visual layout of the document.

After processing the document into contexts in interleaved text-image and paginated image formats, we can not only use these contexts for next-token prediction training but also leverage the document's content, hierarchical structure, and layout features to flexibly and precisely generate high-quality question-answer pairs.


\noindent\textbf{Question-Answer Pairs Construction.}
In this step, we create question-and-answer pairs tailored to the source, content features, and formatting structure of each document. This process is divided into the following main categories:

(1) \emph{For documents with reliable QA annotations,} like the review and reply in OpenReview, we extract the QA pairs and organize them into conversation format.

\begin{figure*}[t!]
    \centering
    {\includegraphics[width=\linewidth]{figure/data.pdf}}
    \caption{
        \textbf{Data distribution of our dataset.}
        The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets.
        \textbf{Left:} Data distribution of {\dataname}.
        \textbf{Right:} Data distribution of our complete SFT training dataset. 
        Note that the number reported in the figure represents the number of samples. ``MT'' is short for multi-turn.
    }
    \label{fig:data_distribution}
    \vspace{-3mm}
\end{figure*}

(2) \emph{For documents with a clear textual structure}, such as well-structured papers from Sci-Hub and Arxiv, we convert them to text and segment them, while the model is instructed to generate contents for each segment, including abstracts, experiment descriptions, and captions for figures and tables. The details of each task for structural papers are illustrated in Table~\ref{tab:task_format}. 

(3) \textit{For other documents}, in addition to using them directly for NTP pretraining, we can input text interspersed with images into MLLMs to obtain QA pairs. To ensure high-quality generated data, we use the state-of-the-art model GPT-4o~\cite{gpt4v}. 


Through our pipeline, most data has been processed into high-quality document-level question-answering data, while the remaining data is converted to plain text and used for next-token prediction tasks. Our pipelines are meticulously designed to ensure high data quality across all generated context. Each LLM-generated sample is explicitly marked in the metadata as model-generated. Across the entire dataset, only 4.8\% of the data is LLM-generated, reinforcing the overall reliability and quality of the dataset.

\begin{table}[]
\small
\begin{tabular}{lp{4.7cm}}
\toprule
Tasks             & \multicolumn{1}{c}{Questions} \\ 
\midrule
Abstract Writing    &  
Read the full text of the paper and provide a concise summary in the form of an abstract.     
\\
\midrule
Paper Titling      &  Based on the provided abstract or introduction of the research paper, please generate a concise and informative title          \\
\midrule
Caption Writing    &  Give the relative texts of the images or tables, please write a caption for each image or table based on the relative texts provided.   \\
\midrule
Experiment Writing &  Please write the "Experiments" section based on the incomplete research paper provided.       \\
\midrule
Translation        &  Please read the full text of the following research paper and translate the Experiments section into Chinese.         \\
\bottomrule
\end{tabular}
\caption{\textbf{Questions format for different tasks.}
For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.
}
\label{tab:task_format}

\vspace{-3mm}

\end{table}

\subsection{Multimodal Document Dataset}

\textbf{Data Source.} 
The composition and distribution of our training data are detailed in Figure~\ref{fig:data_distribution}. Specifically, our dataset predominantly consists of academic papers, which constitute approximately 32.6\% of the total data. The multimodal data, carefully selected to augment our model's learning dimensions, makes up about 88.8\% of our dataset. This strategic distribution is designed to optimize the training process and improve the model's ability to generalize across different types of data inputs.

\noindent\textbf{Dataset Statistics.}
In our {\dataname} dataset, the majority of the data consists of reliably annotated entries, with OpenReview and Arxiv collectively accounting for 75.4\%. The remaining data, sourced from Sci-Hub, is processed using our designed tasks. The overall distribution and number of tasks are shown in Figure~\ref{fig:data_distribution}(a).
Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table~\ref{tab:dataset_analysis}. Compared to previous datasets, {\dataname} contains a larger number of images, with an average of four images per conversation segment. Further comparisons with other datasets are shown in Table~\ref{tab:dataset_comparison}.

\input{table/data_statistics}


\subsection{Data Recipe for Supervised Fine-Tuning}

Although \dataname effectively covers multimodal document QA scenarios, using it directly may lead to model over-fitting on a specific document domain. Therefore, we combine it with several open-source datasets to create a mixed dataset for SFT training.
As shown in Figure \ref{fig:data_distribution}(b), these datasets are organized into 4 categories as follows:

 (1) \emph{For multi-page document QA}, \dataname serves as the core dataset, specifically curated to address complex, multi-page document comprehension. Additional datasets such as MP-Docmatix~\cite{laurenccon2024docmatix}, MP-DocVQA~\cite{mathew2021docvqa}, DUDE~\cite{van2023document}, and Taesiri-ArxivQA~\cite{arxivqa}
offer valuable multi-page scenarios requiring inter-page reasoning and contextual retention across sequences. 

 (2) \emph{For multi-image general QA}, MMDU-45K \cite{liu2024mmdu} offers a comprehensive dataset encompassing diverse real-world scenarios, such as natural environments and everyday contexts. It emphasizes multi-turn dialogues and integration of multiple images, supporting the development of systems capable of generating coherent and accurate responses from complex, lengthy inputs.

(3) \emph{For single-page document QA}, We introduce DocVQA~\cite{mathew2021docvqa}, DocReason \cite{ye2023mplugdocowl}, InfoVQA \cite{mathew2022infographicvqa}, and ChartQA \cite{masry2022chartqa} to further enhance the diversity of the SFT dataset.
These datasets focus on individual pages with complex layouts, rich textual information, and, in some cases, graphical data interpretation.

(4) \emph{For pure-text QA}, we add datasets including LongAlpaca \cite{chen2023longlora}, LongAlpaca-16K-Length \cite{chen2023longlora}, LongQLoRA \cite{yang2023longqlora}, LongCite \cite{zhang2024longcite}, LongAlign \cite{bai2024longalign}, and LongReward \cite{zhang2024longreward} to support the assessment of the model's capabilities in QA tasks requiring long-range dependencies.


This expanded dataset provides a balanced foundation for training and evaluating multimodal document understanding models, enhancing robustness and adaptability across diverse document-related VQA tasks.

\input{table/data_comparison}

\section{Enhanced Baseline for Document-Level Multimodal Understanding}

\subsection{Model Architecture}
Our model architecture leverages the widely-adopted ViT-MLP-LLM structure \cite{liu2023llava, liu2023improved, InternVL2}, consisting of a pre-trained Vision Transformer (ViT), a two-layer MLP projector, and a pre-trained Language Model (LLM). This combination provides a strong baseline for multimodal document analysis, effectively integrating visual and textual information within a unified framework.



\subsection{Optimizing Training Efficiency}

The training efficiency of MLLMs is hindered by two key challenges: (1) \emph{Inconsistent Sample Lengths.} Samples with different context lengths will result in excessive padding and lower training throughput; and (2) \emph{Limited GPU Memory.} 
As the model scale and context length increase, GPU memory consumption becomes increasingly unsustainable. To address these issues, we have implemented the following strategies:

(1) \emph{Multimodal Data Packing.} To balance the computational load between the vision model (ViT) and the language model (LLM) while minimizing resource waste caused by padding, we implement a multimodal data-packing strategy. The key idea is to concatenate multiple samples into long sequences to fully utilize the model's input capacity. 
Specifically, thresholds $T_{\rm img}$ and $T_{\rm tok}$ are set for the number of images and tokens, respectively. Samples are managed using a priority queue, sorted in descending order by the number of images and total tokens. A new sample \( s \) attempts to combine with the sample at the front of the priority queue. If the combination meets the thresholds (\ie, $T_{\rm img}$, $T_{\rm tok}$), the combined sample is pushed back into the priority queue. If \( s \) cannot match with any existing sample in the queue, it is directly added to the queue. When the image number and total token number of the front sample reach one of the thresholds or the number of samples exceeds the maximum limit \( M \), the front sample is dequeued, padded as needed, and sent for training. This strategy optimizes resource utilization and ensures balanced computational workloads. The detailed pseudo-code can be found in the supplementary materials. 


(2) \emph{Ring Attention.} We implement the Ring Attention mechanism~~\cite{liu2023ring} to alleviate memory constraints associated with processing long sequences. By partitioning sequences into blocks and distributing computation across multiple devices, Ring Attention allows the model to accommodate larger contexts. This approach enables overlapping communication between key-value blocks and attention computations, thereby enhancing parallel processing efficiency. Consequently, Ring Attention improves the model's capacity to handle extended context lengths without exceeding memory limits.

(3) \emph{Liger Kernel.} To further improve memory and computational efficiency, we integrate the Liger Kernel~\cite{dai2024ligerkernel}, a specialized kernel library optimized for large-scale model training. The Liger Kernel enhances throughput and reduces memory consumption by employing techniques like kernel fusion, in-place operations, and input chunking. Leveraging the Liger Kernel thus enables higher training throughput and addresses memory limitations, allowing for efficient scaling of large multimodal models.


\input{table/multi_page_qa}

\section{Experiments}

\vspace{-0.5ex}

\subsection{Experimental Setup}
\vspace{-0.5ex}

\noindent\textbf{Training Details.}
Our model is available in two sizes: \modelname-2B and \modelname-8B, both of which are based on the InternVL2~\cite{InternVL2} and fine-tuned for one epoch using the data recipe that includes \dataname. 
The training uses a batch size of 128, the AdamW optimizer with a learning rate of 1e-5, weight decay of 0.01 for the 2B variant, and 0.05 for the 8B variant, along with a cosine learning rate schedule.
To speed up training, we apply multimodal data packing to reduce padding and a dynamic high-resolution strategy~\cite{chen2024far} to enhance OCR for document understanding. 
The maximum number of tiles for multimodal data is limited to 24, and the maximum sequence length is set to 32k tokens.

\noindent\textbf{Baselines.}
We compare our \modelname with a series of open-source document-level MLLMs~\cite{huang2024minimonkey,li2023monkey,liu2024textmonkey,hu2024mplugdocowl2,zhang2024internlm,yao2024minicpm_v,li2024llavaonevision} that supports multi-image input and proprietary MLLMs, including Gemini-1.5-pro~\cite{reid2024gemini1_5}, GPT-4o~\cite{gpt4o}. For comparison with the commonly used RAG method for handling long documents, we selected the latest multimodal RAG methods VisRAG~\cite{yu2024visrag}, InternVL + RAG~\cite{wang2024needle}, and M3DocRAG \cite{cho2024m3docrag}. To compare with more long-context large language models, we use InternVL2-8B as the OCR model to extract texts from the documents and images and feed the parsed documents to long-context LLMs~\cite{dubey2024llama3, cai2024internlm2}.


\input{table/single_page_qa}

\vspace{-1ex}

\subsection{Multi-Page VQA}

\noindent\textbf{Benchmarks.} For the multi-page VQA task, we evaluate our model on three benchmarks:
(1) \textbf{MP-DocVQA} \cite{tito2023mpdocvqa}, 
which is designed to evaluate the ability to handle complex questions across multiple scanned document pages.
(2) \textbf{MMLongbench-Doc} \cite{ma2024mmlong}, a benchmark for evaluating the performance of MLLMs on multi-modal documents.
(3) \textbf{DocGenome} \cite{xia2024docgenome}, a large-scale benchmark for the evaluation of scientific document comprehension.


\noindent\textbf{Results.}
As illustrated in Table~\ref{tab:multi_page_qa}, our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.
Notably, our \modelname-8B surpasses Gemini-1.5-Pro \cite{reid2024gemini1_5} on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o. 
In comparison to RAG-based methods~\cite{yu2024visrag, cho2024m3docrag, wang2024mmniah}, our model demonstrates advantages in multi-page scenarios.
For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our \modelname exhibits a significantly stable improvement compared to the baseline, with \modelname-8B showing an increase of 12.6\% over InternVL2-8B.

\subsection{Interleaved Long-Context QA}

\noindent\textbf{Benchmarks.}
For the interleaved long-context QA task, we evaluate our models on MM-NIAH~\cite{wang2024mmniah}, a benchmark designed for long multimodal document comprehension.

\noindent\textbf{Results.}
The right side of Table~\ref{tab:multi_page_qa} presents the results of MM-NIAH across context lengths ranging from 1K to 64K. We categorize the context lengths into "Short," "Medium," and "Long" based on the context window of InternVL2 (8K) and \modelname (32K). Our \modelname demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations. Notably, for QA tasks with context lengths in the range of $(32K, 64K]$, \modelname-2B outperforms InternVL2-2B by 110\%, and \modelname-8B surpasses InternVL2-8B by 70\%. Furthermore, our model performs comparably to the state-of-the-art multimodal long-context model Gemini-1.5-Pro in contexts longer than 8K, establishing a new state-of-the-art performance among open-source long-context MLLMs.

\subsection{Single-Page VQA}

\noindent\textbf{Benchmarks.}
For single-page VQA tasks, we evaluate our model on three benchmarks:
(1) DocVQA \cite{mathew2021docvqa}, a benchmark for the evaluation of extracting key information from an image of the given document.
(2) ChartQA \cite{masry2022chartqa}, a benchmark for evaluating the reasoning abilities for chart images.
(3) InfoVQA \cite{mathew2022infographicvqa}, a benchmark for infographic image comprehension.

\noindent\textbf{Results.}
As shown in Table~\ref{tab:single_page_qa}, our model achieves 
comparable performance to baseline models. 
Across the three benchmarks, \modelname-2B and InternVL2-2B exhibit comparable results, while \modelname-8B outperforms InternVL2-8B by 0.4 points in DocVQA. These results demonstrate that \dataname effectively enhances the model's long-context modeling capabilities without compromising its performance on shorter documents.


\subsection{Ablation Study}

\noindent\textbf{Effect of \dataname.}
\input{table/data_effect}
We conducted ablation studies on MMLongBench-Doc~\cite{ma2024mmlong} to analyze the impact of our \dataname. We divided \dataname into 3 parts according to the source of the data: (1) Sci-Hub data; (2) Arxiv data; and (3) OpenReview data. We demonstrate the effects of incorporating each part of the data into the SFT process, reported in Table~\ref{tab:data_effect}. 
We observed that with the inclusion of different parts of \dataname, the model's performance improves continuously. Utilizing only open-source data results in an inferior F1 score.


\input{table/latency_analysis}
\noindent\textbf{Latency Analysis.}
To compare the latency in inference between RAG methods and our \modelname, we conducted a latency analysis on MMLongBench-Doc~\cite{ma2024mmlong}, as reported in Table~\ref{tab:latency}. While RAG reduces the document length input to the MLLM, its own time cost remains non-negligible. For instance, InternVL2-2B + RAG is 130\% slower than InternVL2-2B, and VisRAG is 28\% slower than MiniCPM-V2.6. Our \modelname does not require additional processes and therefore has the same inference time as baseline models, making it more suitable for analyzing long documents.



\section{Conclusions}


This work introduced a diverse document-level question-answering dataset that covers complex structures and cross-page dependencies, providing a robust foundation for training and evaluating document understanding models. We also proposed a retrieval-free long-document understanding model that effectively integrates multi-page information, reducing reliance on external retrieval systems. Experimental results show that our model achieves state-of-the-art performance across several document-level QA benchmarks, underscoring its strength in multi-page integration and complex reasoning. Future work will focus on improving computational efficiency, extending the model to larger multimodal tasks, and adapting it to broader applications for enhanced practicality and generalization.

\section*{Acknowledgments}
This project was supported by the National Key R\&D Program of China (No. 2022ZD0161300, 2022ZD0160101), the National Natural Science Foundation of China (No. 62376134, 62372223). Zhe Chen is supported by the Youth PhD Student Research Project under the National Natural Science Foundation (No. 623B2050).

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}


\input{supp}
\end{CJK}
\end{document}


% ===== additional_data_comparison.tex =====
\begin{table*}[t]
\centering
\renewcommand\arraystretch{0.2}
\begin{tabular}{l|c|l|l}
\toprule
Datasets          & \#QA  & Image Types                          & Tasks \\ 
\midrule
MP-DocVQA~\cite{tito2023mpdocvqa}         & 46K   & PDF Documents 
& VQA \\
\midrule
DUDE~\cite{van2023document}              & 41K   & PDF Documents 
& \makecell[tl]{VQA} \\
\midrule
MP-DocStruct1M~\cite{hu2024mplugdocowl2}    & 1M    & PDF Documents 
& \makecell[tl]{Text Parsing, Text Lookup} \\
\midrule
MP-DocReason51K~\cite{hu2024mplugdocowl2}   & 51K   & \makecell[tl]{PDF Documents, \\ Infographics, Webpages, \\ Charts, Natural images}
& VQA \\
\midrule
DocGenome~\cite{xia2024docgenome}         & N/A   & \makecell[tl]{PDF Documents}
& \makecell[tl]{Layout Detection, Document Transformation} \\
\midrule
\dataname (ours)  & 758K  & \makecell[tl]{PDF documents, \\ Charts, Tables} 
& \makecell[tl]{VQA, Abstract Writing, Paper Titling, \\ Caption Writing, Experiment Writing, \\ Translation, Review, Reply }    \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Comparison with other document-level datasets. }}
\label{tab:addi_dataset_comparison}
\end{table*}


% ===== benchmark_metrics.tex =====
\begin{table*}[t]
\centering
\setlength\tabcolsep{4pt}
\begin{tabular}{c|l|l|l}
\toprule
Layout    & Benchmark       & Description                             & Metric                                                                                      \\
\midrule
\multirow{3}{*}{Single-Page} & DocVQA~\cite{mathew2021docvqa}          & VQA on documents.                       & ANLS                                          \\
                             & ChartVQA~\cite{masry2022chartqa}        & VQA on charts.                          & Relaxed EM                                    \\
                             & InfoVQA~\cite{mathew2022infographicvqa}         & VQA on infographics.                    & ANLS                                          \\
\midrule
\multirow{5}{*}{Multi-Page}  & MP-DocVQA~\cite{tito2023mpdocvqa}       & VQA on multi-page documents.            & ANLS                                          \\
                             & MMLongBench-Doc~\cite{ma2024mmlong} & VQA on super-long PDF documents.        & Accuracy, F1 Score                            \\
                             & \multirow{3}{*}[3ex]{DocGenome~\cite{xia2024docgenome}}  & \multirow{3}{*}[3ex]{VQA on multi-page scientific documents.} & Classification Acc, \\
                             &                                  &                                                          & Title ED, Abstract ED, \\
                             &                                  &                                                          & Single-Page Acc,   Multi-Page Acc \\
\midrule
Interleaved                  & MM-NIAH~\cite{wang2024mmniah}         & VQA on natural texts or images.         & Accuracy   \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation benchmarks and metrics for document understanding.}
This table summarizes key benchmarks used in document understanding tasks across three layout types: single-page, multi-page, and interleaved. 
}
\label{tab:benchmark_metrics}
\end{table*}

% ===== data_comparison.tex =====
\begin{table}[t]
\small
\begin{tabular}{lrrr}
\toprule
Dataset         & \#Images  & \#QA Pairs  & \#Tokens         \\ 
\midrule
Docmatix~\cite{laurenccon2024docmatix}                             & 2,444,750 & 9,500,000    & 390,000,000      \\

DocVQA~\cite{clark2017docqa}         & 10,189    & 39,463       & 337,829          \\

TextCaps~\cite{sidorov2020textcaps}  & 21,953    & 21,953       & 389,658          \\

TextVQA~\cite{singh2019textvqa}      & 21,953    & 34,602       & 181,918          \\

ST-VQA~\cite{biten2019stvqa}         & 17,247    & 23,121       & 127,846          \\

OCR-VQA~\cite{mishra2019ocrvqa}      & 165,746   & 801,579      & 6,073,824        \\

VisualMRC~\cite{tanaka2021visualmrc} & 3,027     & 11,988       & 168,828          \\

DUDE~\cite{van2023document}          & 147,597   & 23,716       & 11,341,228       \\ 
\hline
\rowcolor{gray!15}
\dataname (ours)                     & 3,103,494 & 758,000      & 5,200,000,000    \\ 
\bottomrule
\end{tabular}
\caption{
\textbf{Comparison with popular VQA datasets.}
}
\label{tab:dataset_comparison}

\vspace{-3mm}

\end{table}


% ===== data_effect.tex =====
\begin{table}[t]
\centering
\setlength{\tabcolsep}{8pt}
\footnotesize
    \begin{tabular}{lcc}
    \toprule
    Models                                                & Acc    & F1    \\
    \midrule
    Baseline (InternVL2-2B~\cite{InternVL2})              & 10.5   & 10.8  \\
    -- Variant1: SFT using data recipe w/o \dataname      & 18.4   & 9.4   \\
    -- Variant2: Variant1 + our Sci-Hub data              & 18.5   & 15.2  \\
    -- Variant3: Variant2 + our Arxiv data                & 20.5   & 15.5  \\
    \rowcolor{gray!15}
    \modelname-2B: Variant3 + our OpenReview data         & 21.8   & 16.0  \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation study on the data recipe.}
    We evaluate the effectiveness of the training data from different sources on MMLongBench-Doc. Our \dataname can consistently enhance the ability of the model to understand multi-page documents.}
    \label{tab:data_effect}
\end{table}

% ===== data_statistics.tex =====
\begin{table}[]
\small
\centering
\begin{tabular}{lr}
\toprule
Statistics & \multicolumn{1}{c}{Number} \\ \midrule
Total Questions       & 758K   \\
Total Images          & 3.1M   \\
Total Conversations   & 251K   \\ 
\midrule
Multi-Turn Questions  & 87K    \\
Single-Turn Questions & 164K   \\ 
\midrule
Average Text Tokens   & 11245  \\
Average Image Tokens  & 6178   \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Key statistics of the {\dataname} datasets.}
It comprises 758K questions, 3.1M images, and 251K conversations, including 87K multi-turn and 164K single-turn questions. With an average of 11,245 text tokens and 6,178 image tokens, it highlights the datasetâ€™s richness and diversity for multimodal research.
}
\label{tab:dataset_analysis}

\vspace{-3mm}

\end{table}


% ===== hyperparameters.tex =====
\begin{table}[h]
\centering
\small
\setlength\tabcolsep{3.6pt}
\begin{tabular}{c|l|cc}
\toprule
\multicolumn{2}{c|}{Settings}              & \modelname-2B     & \modelname-8B             \\
\midrule
\multirow{2}{*}{\rotatebox{90}{Model}}     & ViT               & InternViT-300M      & InternViT-300M \\
                                           & LLM    & InternLM2-1.8B & InternLM2.5-7B \\

\midrule
\multirow{11}{*}{\rotatebox{90}{Training Hyperparameters}} & Tile Resolution   & \multicolumn{2}{c}{448}                                  \\
                                           & Batch Size        & \multicolumn{2}{c}{128}                                  \\
                                           & Optimizer         & \multicolumn{2}{c}{AdamW}                                \\
                                           & Learning Rate     & \multicolumn{2}{c}{1.00E-05}                             \\
                                           & Warmup Ratio      & \multicolumn{2}{c}{0.03}                                   \\
                                           & LR Scheduler      & \multicolumn{2}{c}{Cosine}                               \\
                                           & Weight Decay      & 0.01                & 0.05           \\
                                           & ViT Drop Path     & \multicolumn{2}{c}{0.1}                                  \\
                                           & Max Tile Number   & \multicolumn{2}{c}{24}                                   \\
                                           & Image Threshold   & \multicolumn{2}{c}{48}                                   \\
                                           & Token Threshold    & \multicolumn{2}{c}{32K}                                  \\
                                           & Epochs   & \multicolumn{2}{c}{1}              \\
\bottomrule
\end{tabular}
\caption{\textbf{Training settings and hyperparameters for \modelname models.} Key configurations for \modelname-2B and \modelname-8B, including model architectures and training parameters.}
\label{tab:hyperparam}
\end{table}

% ===== latency_analysis.tex =====
\begin{table}[t]
\centering
\setlength{\tabcolsep}{10pt}
\footnotesize
    \begin{tabular}{lrrr}
    \toprule
    Models                                    & Latency               & Acc   & F1   \\
    \midrule
    MiniCPM-V2.6~\cite{yao2024minicpm_v}      &  225.4ms              & 16.9  & 15.4 \\
    VisRAG-12B~\cite{yu2024visrag}            &  288.3ms              & 18.8  & 18.3 \\
    \midrule
    InternVL2-2B~\cite{chen2024far}           &   35.9ms              & 10.5  &  10.8 \\
    InternVL2-2B + RAG~\cite{wang2024needle}&   82.9ms              & 17.2  &  16.7 \\
    \rowcolor{gray!15}
    \modelname-2B (ours)                      &   35.9ms              & 21.8  &  16.0 \\
    \midrule
    InternVL2-8B~\cite{chen2024far}           &   81.0ms              & 17.4  &  16.5 \\
    InternVL2-8B + RAG~\cite{wang2024needle}&  113.4ms              & 24.2  &  24.5 \\
    \rowcolor{gray!15}
    \modelname-8B (ours)                      &   81.0ms              & 28.8  &  23.0 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Latency analysis.} We evaluate the average token output latency of model outputs on MMLongBench-Doc~\cite{ma2024mmlong}. 
    RAG-based methods~\cite{wang2024needle, yu2024visrag} exhibit slower processing speeds due to their two-stage inference process, making them less efficient than document MLLMs for handling multimodal long documents.
    }
    \label{tab:latency}
\end{table}

% ===== multi_page_qa.tex =====
\begin{table*}[ht]
\centering
\footnotesize
\tabcolsep=3.3pt
\renewcommand{\arraystretch}{0.9}
    \begin{tabular}{lcccccccccccc}
    \toprule
    \multirow{2}{*}{Models}           
    & MP-Doc         & \multicolumn{2}{c}{MMLong-Doc}            & \multicolumn{5}{c}{DocGenome}
    & \multicolumn{4}{c}{MM-NIAH} \\
    \cmidrule(l){2-2}\cmidrule(l){3-4}\cmidrule(l){5-9}\cmidrule(l){10-13}
    & ANSL$\uparrow$ & Acc$\uparrow$  & F1$\uparrow$          & Class Acc$\uparrow$ & Title ED$\downarrow$ & Abstract ED$\downarrow$  & SP Acc$\uparrow$ & MP Acc$\uparrow$
    & Short    & Medium    & Long    & Overall \\
    \midrule
    \emph{Proprietary Models}\\
    Gemini-1.5-Pro~\cite{reid2024gemini1_5}  
    & --             & 28.2 & 20.6                            & --    & --   & --   & --    & -- 
    & 73.8 & 65.2 & 60.8 & 67.1 \\
    GPT-4o~\cite{gpt4v}       
    & --             & 42.8 & 44.9                            & 97.6 & 9.5 & 6.5 & 71.8 & 67.6 
    & -- & -- & -- & -- \\
    
    \midrule
    \emph{Open-Source Models}\\ 
    MiniMonkey-2B~\cite{huang2024minimonkey} 
    & 70.3           & 10.3 &  8.6                            & 57.4 & 16.5 & 55.0 & 40.3 & 28.9                         
    & 40.9 & 26.9 & 23.5 & 31.0 \\

    InternVL2-2B~\cite{chen2024far} 
    & 71.8           & 10.5 & 10.8                            & 60.8 & 18.4 & 54.3 & 39.4 & 28.9                       
    & 36.6 & 21.2 & 19.4 & 26.4 \\
    InternVL2-2B + RAG~\cite{wang2024needle} 
    & 72.6           & 17.2 & 16.7                            & 60.8 & 18.4 & 54.3 & 39.4 & 28.4 
    & 36.8 & 30.2 & 34.8 & 33.8 \\
    Llama3.2-3B-Instruct$^\dagger$~\cite{dubey2024llama3}
    & --             & 23.7 & 21.2                            & 85.3 & 194.7 & 51.0 & 40.2 & 34.9 
    & 15.5 &  2.2 &  0.5 &  6.6 \\
    \rowcolor{gray!15}
    \modelname-2B (ours)            
    & 76.2           & 21.8 & 16.0                            & 56.2 & 4.5  & 43.6 & 45.1 & 37.4  
    & 58.0 & 46.7 & 40.9 & 49.2 \\
    

    \midrule
    MiniCPM-V2.6-8B~\cite{yao2024minicpm_v} 
    & --             & 16.9 & 15.4                            & 92.8 & 10.2 & 32.6 & 60.0 & 54.2     
    & 49.0 & 15.3 & 0.0 & 23.4 \\

    LLaVA-OneVision-8B~\cite{li2024llavaonevision} 
    & --             & 10.8 &  9.6                            & 85.6 & 49.9 & 77.5 &  9.8 &  7.1 
    & 65.7 & 38.0 & 0.0 & 37.7 \\
    mPLUG-DocOwl2-8B~\cite{hu2024mplugdocowl2} 
    & 69.4           & 13.4 &  8.9                            & --   & --   & --   & --   & -- 
    & 17.9 &  0.1 & 0.0 &  6.6 \\
    M3DocRAG~\cite{cho2024m3docrag}           
    & 84.4           & 21.0 & 22.6                            & --   & --   & --   & --   & -- 
    & -- & -- & -- & -- \\

    VisRAG-8B~\cite{yu2024visrag}           
    & --             & 18.8 & 18.3                            & 92.8 & 10.2 & 32.6 & 60.0 & 50.7 
    & 47.1 & 29.2 & 29.5 & 35.8 \\
    
    InternLM2.5-7B-1M$^\dagger$~\cite{cai2024internlm2}   
    & --             & 28.7 & 25.6                            & 92.7 & 77.6 & 59.3 & 42.7 & 42.5 
    & 40.5 & 37.2 & 35.1 & 37.8 \\

    InternVL2-8B~\cite{chen2024far} 
    & 79.3           & 17.4 & 16.5                            & 90.6 &  8.2 & 39.6 & 56.0 & 46.1 
    & 56.4 & 37.3 & 32.4 & 42.9 \\
    InternVL2-8B + RAG~\cite{wang2024needle} 
    & 78.7           & 24.2 & 24.5                            & 90.6 &  8.2 & 39.6 & 56.0 & 46.0  
    & 55.7 & 43.4 & 45.2 & 48.4 \\

    InternVL2-26B~\cite{chen2024far} 
    & --             & 15.5 & 15.4                            & 87.5 & 16.9 & 23.3 & 49.7 & 42.7 
    & 65.0 & 48.7 & 41.9 & 52.8 \\

    \rowcolor{gray!15}
    \modelname-8B (ours)            
    & 81.3           & 28.8 & 23.0                            & 93.8 & 2.0  & 19.7 & 53.9 & 51.9 
    & 71.2 & 57.4 & 55.3 & 61.8 \\
    \bottomrule
    \end{tabular}
    \caption{
    \textbf{Evaluation on multi-page and interleaved VQA benchmarks.} 
    We report the metrics on MP-DocVQA~\cite{tito2023mpdocvqa} (MP-Doc), MMLongbench-Doc~\cite{ma2024mmlong} (MMLong-Doc), DocGenome~\cite{xia2024docgenome}, and MM-NIAH~\cite{wang2024needle}.
    Our model outperforms document-level MLLMs and multimodal RAG methods on multi-page, medium, and long-context QA. 
    The ``Short'', ``Medium'', and ``Long'' in MM-NIAH refer to input length in $\rm{[0,8k]}$, $\rm{(8k, 32k]}$, $\rm{( 32k, 64k]}$, respectively. ``$\dagger$'' denotes input documents are parsed by OCR models.}
    \label{tab:multi_page_qa}
    \vspace{-3mm}
\end{table*}

% ===== single_page_qa.tex =====
\begin{table}[t!]
\centering
\setlength{\tabcolsep}{7pt}
\footnotesize
    \begin{tabular}{lccc}
    \toprule
    Models & DocVQA & ChartQA & InfoVQA \\
    \midrule
    Gemini-1.5-Pro~\cite{reid2024gemini1_5}  & 93.1    & 87.2    & 81.0    \\
    GPT-4o~\cite{gpt4v}                      & 92.8    & 85.7    & --      \\
    \midrule
    MiniMonkey-2B~\cite{huang2024minimonkey} & 87.4    & 76.5    & 60.1    \\
    InternVL2-2B~\cite{chen2024far}          & 86.9    & 76.2    & 58.9    \\
    \rowcolor{gray!15}
    \modelname-2B (ours)                     & 87.3    & 76.4    & 58.5    \\
    \midrule
    Monkey-8B~\cite{li2023monkey}            & 66.5    & 65.1    & 36.1    \\
    TextMonkey-9B~\cite{liu2024textmonkey}   & 73.0    & 66.9    & 28.6    \\
    mPLUG-DocOwl2-8B~\cite{hu2024mplugdocowl2}& 80.7   & 70.0    & 46.4    \\
    IXC2.5-7B \cite{zhang2024internlm}       & 90.9    & 82.2    & 69.9    \\
    InternVL2-8B~\cite{chen2024far}          & 91.6    & 83.3    & 74.8    \\
    \rowcolor{gray!15}
    \modelname-8B (ours)                     & 92.0    & 83.3    & 73.3    \\
    \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{\textbf{Results on single-page VQA benchmarks.}
    Our \modelname models perform comparably to baselines~\cite{InternVL2}, demonstrating enhanced long-context modeling without loss on shorter tasks.
    }
    \label{tab:single_page_qa}
    \vspace{-3mm}
\end{table}

