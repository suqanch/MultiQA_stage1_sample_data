\begin{table*}[ht]
\centering
\footnotesize
\tabcolsep=3.3pt
\renewcommand{\arraystretch}{0.9}
    \begin{tabular}{lcccccccccccc}
    \toprule
    \multirow{2}{*}{Models}           
    & MP-Doc         & \multicolumn{2}{c}{MMLong-Doc}            & \multicolumn{5}{c}{DocGenome}
    & \multicolumn{4}{c}{MM-NIAH} \\
    \cmidrule(l){2-2}\cmidrule(l){3-4}\cmidrule(l){5-9}\cmidrule(l){10-13}
    & ANSL$\uparrow$ & Acc$\uparrow$  & F1$\uparrow$          & Class Acc$\uparrow$ & Title ED$\downarrow$ & Abstract ED$\downarrow$  & SP Acc$\uparrow$ & MP Acc$\uparrow$
    & Short    & Medium    & Long    & Overall \\
    \midrule
    \emph{Proprietary Models}\\
    Gemini-1.5-Pro~\cite{reid2024gemini1_5}  
    & --             & 28.2 & 20.6                            & --    & --   & --   & --    & -- 
    & 73.8 & 65.2 & 60.8 & 67.1 \\
    GPT-4o~\cite{gpt4v}       
    & --             & 42.8 & 44.9                            & 97.6 & 9.5 & 6.5 & 71.8 & 67.6 
    & -- & -- & -- & -- \\
    
    \midrule
    \emph{Open-Source Models}\\ 
    MiniMonkey-2B~\cite{huang2024minimonkey} 
    & 70.3           & 10.3 &  8.6                            & 57.4 & 16.5 & 55.0 & 40.3 & 28.9                         
    & 40.9 & 26.9 & 23.5 & 31.0 \\

    InternVL2-2B~\cite{chen2024far} 
    & 71.8           & 10.5 & 10.8                            & 60.8 & 18.4 & 54.3 & 39.4 & 28.9                       
    & 36.6 & 21.2 & 19.4 & 26.4 \\
    InternVL2-2B + RAG~\cite{wang2024needle} 
    & 72.6           & 17.2 & 16.7                            & 60.8 & 18.4 & 54.3 & 39.4 & 28.4 
    & 36.8 & 30.2 & 34.8 & 33.8 \\
    Llama3.2-3B-Instruct$^\dagger$~\cite{dubey2024llama3}
    & --             & 23.7 & 21.2                            & 85.3 & 194.7 & 51.0 & 40.2 & 34.9 
    & 15.5 &  2.2 &  0.5 &  6.6 \\
    \rowcolor{gray!15}
    \modelname-2B (ours)            
    & 76.2           & 21.8 & 16.0                            & 56.2 & 4.5  & 43.6 & 45.1 & 37.4  
    & 58.0 & 46.7 & 40.9 & 49.2 \\
    

    \midrule
    MiniCPM-V2.6-8B~\cite{yao2024minicpm_v} 
    & --             & 16.9 & 15.4                            & 92.8 & 10.2 & 32.6 & 60.0 & 54.2     
    & 49.0 & 15.3 & 0.0 & 23.4 \\

    LLaVA-OneVision-8B~\cite{li2024llavaonevision} 
    & --             & 10.8 &  9.6                            & 85.6 & 49.9 & 77.5 &  9.8 &  7.1 
    & 65.7 & 38.0 & 0.0 & 37.7 \\
    mPLUG-DocOwl2-8B~\cite{hu2024mplugdocowl2} 
    & 69.4           & 13.4 &  8.9                            & --   & --   & --   & --   & -- 
    & 17.9 &  0.1 & 0.0 &  6.6 \\
    M3DocRAG~\cite{cho2024m3docrag}           
    & 84.4           & 21.0 & 22.6                            & --   & --   & --   & --   & -- 
    & -- & -- & -- & -- \\

    VisRAG-8B~\cite{yu2024visrag}           
    & --             & 18.8 & 18.3                            & 92.8 & 10.2 & 32.6 & 60.0 & 50.7 
    & 47.1 & 29.2 & 29.5 & 35.8 \\
    
    InternLM2.5-7B-1M$^\dagger$~\cite{cai2024internlm2}   
    & --             & 28.7 & 25.6                            & 92.7 & 77.6 & 59.3 & 42.7 & 42.5 
    & 40.5 & 37.2 & 35.1 & 37.8 \\

    InternVL2-8B~\cite{chen2024far} 
    & 79.3           & 17.4 & 16.5                            & 90.6 &  8.2 & 39.6 & 56.0 & 46.1 
    & 56.4 & 37.3 & 32.4 & 42.9 \\
    InternVL2-8B + RAG~\cite{wang2024needle} 
    & 78.7           & 24.2 & 24.5                            & 90.6 &  8.2 & 39.6 & 56.0 & 46.0  
    & 55.7 & 43.4 & 45.2 & 48.4 \\

    InternVL2-26B~\cite{chen2024far} 
    & --             & 15.5 & 15.4                            & 87.5 & 16.9 & 23.3 & 49.7 & 42.7 
    & 65.0 & 48.7 & 41.9 & 52.8 \\

    \rowcolor{gray!15}
    \modelname-8B (ours)            
    & 81.3           & 28.8 & 23.0                            & 93.8 & 2.0  & 19.7 & 53.9 & 51.9 
    & 71.2 & 57.4 & 55.3 & 61.8 \\
    \bottomrule
    \end{tabular}
    \caption{
    \textbf{Evaluation on multi-page and interleaved VQA benchmarks.} 
    We report the metrics on MP-DocVQA~\cite{tito2023mpdocvqa} (MP-Doc), MMLongbench-Doc~\cite{ma2024mmlong} (MMLong-Doc), DocGenome~\cite{xia2024docgenome}, and MM-NIAH~\cite{wang2024needle}.
    Our model outperforms document-level MLLMs and multimodal RAG methods on multi-page, medium, and long-context QA. 
    The ``Short'', ``Medium'', and ``Long'' in MM-NIAH refer to input length in $\rm{[0,8k]}$, $\rm{(8k, 32k]}$, $\rm{( 32k, 64k]}$, respectively. ``$\dagger$'' denotes input documents are parsed by OCR models.}
    \label{tab:multi_page_qa}
    \vspace{-3mm}
\end{table*}