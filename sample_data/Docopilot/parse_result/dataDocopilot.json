[{
  "caption": "Figure 1. Accuracy v.s inference latency on MM-NIAH. The proposed Docopilot-8B shows a notable improvement over baseline models [73], achieving a +19.9% accuracy gain compared to InternVL2-8B and surpassing InternVL2-26B with less than 31% of the inference latency. Additionally, Docopilot-2B uses fewer parameters (less than 10%) while exhibiting comparable performance to the 10× larger InternVL2-26B. These results suggest that our Docopilot strikes a reasonable balance between latency, model size, and performance.",
  "captionBoundary": {
    "x1": 317.25,
    "x2": 553.499755859375,
    "y1": 387.341796875,
    "y2": 480.41595458984375
  },
  "figType": "Figure",
  "imageText": ["9%", "+1", "9.", "y", "ur", "ac", "A", "cc", "Docopilot-8B", "Diameter", "2B", "8B", "26B", "InternVL2-26B", "Docopilot-2B", "InternVL2-8B", "+RAG", "InternVL2-2B", "+RAG", "InternVL2-8B", "LLaVA-OV-8B", "InternVL2-2B", "8%", "+2", "2.", "y", "ur", "ac", "A", "cc", "Accuracy", "+", "11.5%<", "31%", "Latency", "<", "31%", "#Param", "Docopilot", "(ours)", "InternVL2", "LLaVA-OV", "InternVL2-RAG", "Inference", "Latency", "(s)", "(%", ")", "ac", "y", "cc", "ur", "H", "A", "-N", "IA", "M", "M", "0", "5", "10", "15", "20", "25", "30", "60.0", "55.0", "50.0", "45.0", "40.0", "35.0", "30.0", "25.0", "20.0"],
  "name": "1",
  "page": 0,
  "regionBoundary": {
    "x1": 317.76,
    "x2": 553.1999999999999,
    "y1": 222.95999999999998,
    "y2": 374.15999999999997
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Figure1-1.png"
}, {
  "caption": "Table 3. Comparison with popular VQA datasets.",
  "captionBoundary": {
    "x1": 82.56900024414062,
    "x2": 270.6828918457031,
    "y1": 205.31875610351562,
    "y2": 210.72100830078125
  },
  "figType": "Table",
  "imageText": ["Docmatix", "[33]", "2,444,750", "9,500,000", "390,000,000", "DocVQA", "[15]", "10,189", "39,463", "337,829", "TextCaps", "[64]", "21,953", "21,953", "389,658", "TextVQA", "[65]", "21,953", "34,602", "181,918", "ST-VQA", "[8]", "17,247", "23,121", "127,846", "OCR-VQA", "[55]", "165,746", "801,579", "6,073,824", "VisualMRC", "[70]", "3,027", "11,988", "168,828", "DUDE", "[78]", "147,597", "23,716", "11,341,228", "Doc-750K", "(ours)", "3,103,494", "758,000", "5,200,000,000", "Dataset", "#Images", "#QA", "Pairs", "#Tokens"],
  "name": "3",
  "page": 5,
  "regionBoundary": {
    "x1": 57.839999999999996,
    "x2": 293.03999999999996,
    "y1": 71.75999999999999,
    "y2": 193.2
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table3-1.png"
}, {
  "caption": "Table 4. Evaluation on multi-page and interleaved VQA benchmarks. We report the metrics on MP-DocVQA [76] (MP-Doc), MMLongbench-Doc [51] (MMLong-Doc), DocGenome [89], and MM-NIAH [85]. Our model outperforms document-level MLLMs and multimodal RAG methods on multi-page, medium, and long-context QA. The “Short”, “Medium”, and “Long” in MM-NIAH refer to input length in [0, 8k], (8k, 32k], (32k, 64k], respectively. “†” denotes input documents are parsed by OCR models.",
  "captionBoundary": {
    "x1": 58.5,
    "x2": 553.49951171875,
    "y1": 291.01873779296875,
    "y2": 329.2969970703125
  },
  "figType": "Table",
  "imageText": ["MiniCPM-V2.6-8B", "[92]", "–", "16.9", "15.4", "92.8", "10.2", "32.6", "60.0", "54.2", "49.0", "15.3", "0.0", "23.4", "LLaVA-OneVision-8B", "[36]", "–", "10.8", "9.6", "85.6", "49.9", "77.5", "9.8", "7.1", "65.7", "38.0", "0.0", "37.7", "mPLUG-DocOwl2-8B", "[27]", "69.4", "13.4", "8.9", "–", "–", "–", "–", "–", "17.9", "0.1", "0.0", "6.6", "M3DocRAG", "[14]", "84.4", "21.0", "22.6", "–", "–", "–", "–", "–", "–", "–", "–", "–", "VisRAG-8B", "[95]", "–", "18.8", "18.3", "92.8", "10.2", "32.6", "60.0", "50.7", "47.1", "29.2", "29.5", "35.8", "InternLM2.5-7B-1M†", "[9]", "–", "28.7", "25.6", "92.7", "77.6", "59.3", "42.7", "42.5", "40.5", "37.2", "35.1", "37.8", "InternVL2-8B", "[13]", "79.3", "17.4", "16.5", "90.6", "8.2", "39.6", "56.0", "46.1", "56.4", "37.3", "32.4", "42.9", "InternVL2-8B", "+", "RAG", "[85]", "78.7", "24.2", "24.5", "90.6", "8.2", "39.6", "56.0", "46.0", "55.7", "43.4", "45.2", "48.4", "InternVL2-26B", "[13]", "–", "15.5", "15.4", "87.5", "16.9", "23.3", "49.7", "42.7", "65.0", "48.7", "41.9", "52.8", "Docopilot-8B", "(ours)", "81.3", "28.8", "23.0", "93.8", "2.0", "19.7", "53.9", "51.9", "71.2", "57.4", "55.3", "61.8", "Open-Source", "Models", "MiniMonkey-2B", "[28]", "70.3", "10.3", "8.6", "57.4", "16.5", "55.0", "40.3", "28.9", "40.9", "26.9", "23.5", "31.0", "InternVL2-2B", "[13]", "71.8", "10.5", "10.8", "60.8", "18.4", "54.3", "39.4", "28.9", "36.6", "21.2", "19.4", "26.4", "InternVL2-2B", "+", "RAG", "[85]", "72.6", "17.2", "16.7", "60.8", "18.4", "54.3", "39.4", "28.4", "36.8", "30.2", "34.8", "33.8", "Llama3.2-3B-Instruct†", "[21]", "–", "23.7", "21.2", "85.3", "194.7", "51.0", "40.2", "34.9", "15.5", "2.2", "0.5", "6.6", "Docopilot-2B", "(ours)", "76.2", "21.8", "16.0", "56.2", "4.5", "43.6", "45.1", "37.4", "58.0", "46.7", "40.9", "49.2", "Proprietary", "Models", "Gemini-1.5-Pro", "[60]", "–", "28.2", "20.6", "–", "–", "–", "–", "–", "73.8", "65.2", "60.8", "67.1", "GPT-4o", "[56]", "–", "42.8", "44.9", "97.6", "9.5", "6.5", "71.8", "67.6", "–", "–", "–", "–", "ANSL↑", "Acc↑", "F1↑", "Class", "Acc↑", "Title", "ED↓", "Abstract", "ED↓", "SP", "Acc↑", "MP", "Acc↑", "Short", "Medium", "Long", "Overall", "Models", "MP-Doc", "MMLong-Doc", "DocGenome", "MM-NIAH"],
  "name": "4",
  "page": 6,
  "regionBoundary": {
    "x1": 57.839999999999996,
    "x2": 556.0799999999999,
    "y1": 71.75999999999999,
    "y2": 279.12
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table4-1.png"
}, {
  "caption": "Table 5. Results on single-page VQA benchmarks. Our Docopilot models perform comparably to baselines [73], demonstrating enhanced long-context modeling without loss on shorter tasks.",
  "captionBoundary": {
    "x1": 317.25,
    "x2": 553.4986572265625,
    "y1": 488.08074951171875,
    "y2": 515.4010009765625
  },
  "figType": "Table",
  "imageText": ["Monkey-8B", "[40]", "66.5", "65.1", "36.1", "TextMonkey-9B", "[46]", "73.0", "66.9", "28.6", "mPLUG-DocOwl2-8B", "[27]", "80.7", "70.0", "46.4", "IXC2.5-7B", "[98]", "90.9", "82.2", "69.9", "InternVL2-8B", "[13]", "91.6", "83.3", "74.8", "Docopilot-8B", "(ours)", "92.0", "83.3", "73.3", "MiniMonkey-2B", "[28]", "87.4", "76.5", "60.1", "InternVL2-2B", "[13]", "86.9", "76.2", "58.9", "Docopilot-2B", "(ours)", "87.3", "76.4", "58.5", "Gemini-1.5-Pro", "[60]", "93.1", "87.2", "81.0", "GPT-4o", "[56]", "92.8", "85.7", "–", "Models", "DocVQA", "ChartQA", "InfoVQA"],
  "name": "5",
  "page": 6,
  "regionBoundary": {
    "x1": 316.8,
    "x2": 552.0,
    "y1": 343.91999999999996,
    "y2": 479.03999999999996
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table5-1.png"
}, {
  "caption": "Figure 2. Multimodal document dataset generation pipeline. This pipeline involves three main stages: (1) Raw Data Collection: Documents are gathered from sources like Sci-Hub, arXiv, and OpenReview, available in PDF and HTML formats. (2) Document Content Extraction: Multimodal content is processed in two formats: interleaved text-image format and multi-image format. (3) Question-Answer Pairs Construction: QA pairs are generated based on the document structure or constructed using GPT-4o.",
  "captionBoundary": {
    "x1": 58.5,
    "x2": 553.4993286132812,
    "y1": 255.06576538085938,
    "y2": 293.344970703125
  },
  "figType": "Figure",
  "imageText": ["Question:", "Please", "read", "the", "paper:", "<paper>,", "and", "answer", "the", "question:", "<question>", "Answer:", "<answer>", "(3)", "Other", "Documents", "Question-Answer", "Pairs", "Construction", "Question:", "Please", "write", "an", "abstract", "/", "intro-", "duction", "section", "for", "this", "paper:", "<paper>", "Answer:", "<answer>", "(2)", "Documents", "with", "a", "Clear", "Textual", "Structure", "Question:", "Please", "write", "a", "review", "for", "the", "provide", "paper:", "<paper>", "Answer:", "<answer>", "(1)", "Documents", "with", "Reliable", "QA", "Annotations", "Document", "Content", "Extraction", "...", "Driven", "by", "the", "significance", "of", "depth,", "a", "question", "arises:", "Is", "learning", "better", "networks", "as", "easy", "as", "stacking", "more", "layers?", "...", "Deeper", "neural", "networks", "are", "more", "difficult", "to", "train.", "We", "present", "a", "residual", "learning", "framework", "...", "(1)", "Interleaved", "Text-Image", "Format", "(2)", "Multi-Image", "Format", "Raw", "Data", "CollectionData", "Source"],
  "name": "2",
  "page": 2,
  "regionBoundary": {
    "x1": 57.839999999999996,
    "x2": 554.16,
    "y1": 72.0,
    "y2": 243.12
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Figure2-1.png"
}, {
  "caption": "Table 7. Latency analysis. We evaluate the average token output latency of model outputs on MMLongBench-Doc [51]. RAGbased methods [85, 95] exhibit slower processing speeds due to their two-stage inference process, making them less efficient than document MLLMs for handling multimodal long documents.",
  "captionBoundary": {
    "x1": 317.25,
    "x2": 553.498046875,
    "y1": 323.8007507324219,
    "y2": 373.0389404296875
  },
  "figType": "Table",
  "imageText": ["InternVL2-8B", "[13]", "81.0ms", "17.4", "16.5", "InternVL2-8B", "+", "RAG", "[85]", "113.4ms", "24.2", "24.5", "Docopilot-8B", "(ours)", "81.0ms", "28.8", "23.0", "InternVL2-2B", "[13]", "35.9ms", "10.5", "10.8", "InternVL2-2B", "+", "RAG", "[85]", "82.9ms", "17.2", "16.7", "Docopilot-2B", "(ours)", "35.9ms", "21.8", "16.0", "MiniCPM-V2.6", "[92]", "225.4ms", "16.9", "15.4", "VisRAG-12B", "[95]", "288.3ms", "18.8", "18.3", "Models", "Latency", "Acc", "F1"],
  "name": "7",
  "page": 7,
  "regionBoundary": {
    "x1": 324.96,
    "x2": 546.0,
    "y1": 204.95999999999998,
    "y2": 312.0
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table7-1.png"
}, {
  "caption": "Table 6. Ablation study on the data recipe. We evaluate the effectiveness of the training data from different sources on MMLongBench-Doc. Our Doc-750K can consistently enhance the ability of the model to understand multi-page documents.",
  "captionBoundary": {
    "x1": 317.25,
    "x2": 553.5,
    "y1": 152.11874389648438,
    "y2": 190.39697265625
  },
  "figType": "Table",
  "imageText": ["Baseline", "(InternVL2-2B", "[73])", "10.5", "10.8", "–", "Variant1:", "SFT", "using", "data", "recipe", "w/o", "Doc-750K", "18.4", "9.4", "–", "Variant2:", "Variant1", "+", "our", "Sci-Hub", "data", "18.5", "15.2", "–", "Variant3:", "Variant2", "+", "our", "Arxiv", "data", "20.5", "15.5", "Docopilot-2B:", "Variant3", "+", "our", "OpenReview", "data", "21.8", "16.0", "Models", "Acc", "F1"],
  "name": "6",
  "page": 7,
  "regionBoundary": {
    "x1": 318.96,
    "x2": 552.0,
    "y1": 71.75999999999999,
    "y2": 140.16
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table6-1.png"
}, {
  "caption": "Figure 3. Visualization of an example from Doc-750K. The left side presents the interleaved text-image format data obtained through Document Content Extraction, while the right side showcases the annotations generated via Question-Answer Pairs Construction.",
  "captionBoundary": {
    "x1": 58.5,
    "x2": 553.5000610351562,
    "y1": 442.9547424316406,
    "y2": 459.31597900390625
  },
  "figType": "Figure",
  "imageText": ["…", "…", "According", "to", "the", "paper,", "‘The", "regression", "diagnostics", "offered", "little", "guidance", "on", "improving", "the", "structure", "of", "the", "model.", "The", "addition", "of", "a", "dummy", "variable", "for", "the", "years", "1971-1973", "did", "little", "to", "improve", "the", "residuals", "in", "subsequent", "years.", "Adding", "a", "set", "of", "dummy", "variables", "to", "account", "for", "each", "unstable", "period", "did", "not", "seem", "reasonable.’", "Image-3", "presents", "a", "comparative", "analysis", "of", "electricity", "cross-price", "elasticity", "studies.", "This", "table", "showcases", "various", "studies,", "their", "sample", "sectors,", "and", "reported", "elasticities", "for", "electricity,", "natural", "gas,", "oil,", "and", "coal", "across", "different", "regions", "and", "time", "periods.", "The", "study’s", "own", "findings,", "-0.078", "for", "short-run", "own-price", "elasticity", "of", "electricity,", "align", "closely", "with", "prior", "research,", "confirming", "its", "place", "within", "established", "literature.", "The", "context", "provided", "by", "other", "studies", "helps", "to", "validate", "the", "paper’s", "conclusions", "regarding", "the", "robustness", "of", "its", "econometric", "model", "and", "the", "effectiveness", "of", "conservation", "programs.", "Analyze", "the", "importance", "of", "Image-3", "in", "comparing", "the", "study’s", "findings", "with", "previous", "empirical", "research", "on", "electricity", "price", "elasticities.", "According", "to", "the", "paper,", "‘In", "1993,", "the", "39", "companies", "in", "the", "sample", "accounted", "for", "19%", "of", "total", "C&I", "electricity", "sales", "nationwide", "and", "40%", "of", "all", "utility", "conservation", "program", "impacts,", "including", "residential", "impacts", "(Energy", "Information", "Administration", "1995).’", "The", "table", "in", "Image-1", "provides", "detailed", "data", "on", "the", "1993", "C&I", "sales", "and", "C&I", "conservation", "program", "impacts", "for", "various", "utilities.", "This", "image", "shows", "the", "sales", "in", "GWh", "and", "conservation", "impacts", "both", "in", "GWh", "and", "as", "a", "percentage", "of", "sales.", "For", "example,", "California", "utilities", "such", "as", "Southern", "California", "Edison", "reported", "high", "sales", "and", "high", "conservation", "percentages,", "which", "is", "a", "significant", "outlier.", "These", "data", "illustrate", "the", "varying", "levels", "of", "conservation", "achievements", "across", "utilities,", "underlining", "the", "study’s", "aim", "to", "verify", "the", "effectiveness", "and", "accuracy", "of", "reported", "conservation", "impacts.", "Larger", "utilities", "tend", "to", "report", "greater", "conservation", "impacts", "relative", "to", "sales,", "and", "this", "detail", "supports", "the", "analysis", "presented", "in", "the", "paper.", "<paper>", "Please", "read", "the", "paper", "and", "answer", "the", "question:", "According", "to", "this", "paper,", "how", "does", "Image-1", "illustrate", "the", "conservation", "impacts", "of", "various", "utilities", "in", "1993?", "Discuss", "the", "findings", "and", "the", "significance", "of", "these", "data", "in", "the", "context", "of", "the", "research.", "According", "to", "California", "regulators,", "SCE’s", "reported", "impacts", "for", "service", "programs", "have", "always", "been", "viewed", "with", "skepticism", "during", "regulatory", "reviews,", "although", "its", "impacts", "for", "other", "types", "of", "conservation", "programs", "have", "not.", "In", "California,", "service", "program", "impacts", "have", "historically", "been", "viewed", "as", "unverifiable,", "so", "utilities", "have", "been", "largely", "ineligible", "for", "incentive", "payments", "related", "to", "service", "program", "expenditures.", "Regulators", "in", "California", "acknowledge", "that", "they", "believe", "SCE’s", "reported", "conservation", "impacts", "overstate", "the", "actual", "savings", "achieved", "by", "their", "programs", "and", "have", "communicated", "this", "skepticism", "to", "the", "utility.", "But,", "according", "to", "the", "rate", "regulation", "…", "Early", "estimation", "attempts", "with", "individual", "utilities", "revealed", "that", "there", "was", "insufficient", "variation", "in", "the", "data", "series", "to", "produce", "reliable", "coefficient", "estimates", "for", "each", "utility", "taken", "separately.", "To", "increase", "the", "statistical", "power", "of", "the", "analysis,", "…", "…", "Finally,", "since", "no", "incentives", "are", "offered,", "it", "becomes", "more", "difficult", "to", "attribute", "customer", "investment", "decisions", "to", "the", "utility", "programs", "as", "opposed", "to", "other", "influences.", "How", "Many", "Kilowatts", "are", "in", "a", "Negawatt?", "Verifying", "“Ex", "Post”", "Estimates", "of", "Utility", "Conservation", "Impacts", "at", "the", "Regional", "Level", "The", "objective", "consideration", "of", "conservation", "policy", "under", "restructuring", "is", "proving", "to", "be", "a", "difficult", "task.", "One", "of", "the", "greatest", "obstacles", "has", "been", "the", "persistent", "uncertainty", "among", "utility", "planners", "regarding", "the", "true", "resource-", "effectiveness", "and", "cost-", "effectiveness", "of", "conservation", "relative", "…", "These", "figures", "indicate", "that", "larger", "utilities", "tend", "to", "report", "greater", "…"],
  "name": "3",
  "page": 3,
  "regionBoundary": {
    "x1": 57.839999999999996,
    "x2": 554.16,
    "y1": 72.0,
    "y2": 431.03999999999996
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Figure3-1.png"
}, {
  "caption": "Figure 4. Data distribution of our dataset. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Left: Data distribution of Doc-750K. Right: Data distribution of our complete SFT training dataset. Note that the number reported in the figure represents the number of samples. “MT” is short for multi-turn.",
  "captionBoundary": {
    "x1": 58.5,
    "x2": 553.5040893554688,
    "y1": 206.35073852539062,
    "y2": 233.67095947265625
  },
  "figType": "Figure",
  "imageText": ["LongAlign", "(5.9K)", "LongCite", "(13.5K)", "LongReward", "(10K)", "LongAlpaca", "(12K)", "LongQLoRA", "(38.7K)", "Pure-Text", "QA", "(6.8%)", "DocVQA", "(56K)", "DocReason", "(25.8K)", "InfoVQA", "(25.5K)", "ChartQA", "(30.2K)", "Single-Page", "Document", "QA", "(10.8%)", "(a)", "Data", "Distribution", "of", "Doc-750K", "(b)", "Data", "Recipe", "for", "Supervised", "Fine-tuning", "MP-DocVQA", "(51K)", "Doc-750K", "(750K)", "(ours)", "Docmatix", "(141K)", "ArxivQA", "(31K)", "DUDE", "(27K)", "Multi-Image", "General", "QA", "(3.6%)", "MMDU", "(45K)", "Multi-Page", "Document", "QA", "(78.8%)", "Review", "(147.6K)", "Reply", "(77.7K)", "Source:", "OpenReview", "(29.8%)", "MT-QA", "(image)", "(49.8K)", "Translation", "(3K)", "MT-QA", "(interleave)", "(49.8K)", "Caption", "Writing", "(2K)", "Experiment", "Writing", "(2K)", "Abstract", "Writing", "(2K)", "Paper", "Titling", "(2K)", "MT-QA", "(interleave)", "(211K)", "MT-QA", "(image)", "(211K)", "Source:", "Arxiv", "(55.8%)", "Source:", "Sci-Hub", "(14.4%)"],
  "name": "4",
  "page": 4,
  "regionBoundary": {
    "x1": 60.0,
    "x2": 550.0799999999999,
    "y1": 73.92,
    "y2": 191.04
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Figure4-1.png"
}, {
  "caption": "Table 1. Questions format for different tasks. For documents with a clear textual structure, we design several proxy tasks. All tasks leverage the inherent structure of the documents, with answers directly sourced from the original text.",
  "captionBoundary": {
    "x1": 58.49998474121094,
    "x2": 294.7466125488281,
    "y1": 488.979736328125,
    "y2": 527.2589721679688
  },
  "figType": "Table",
  "imageText": ["Translation", "Please", "read", "the", "full", "text", "of", "the", "follow-", "ing", "research", "paper", "and", "translate", "the", "Experiments", "section", "into", "Chinese.", "Experiment", "Writing", "Please", "write", "the", "”Experiments”", "sec-", "tion", "based", "on", "the", "incomplete", "research", "paper", "provided.", "Caption", "Writing", "Give", "the", "relative", "texts", "of", "the", "images", "or", "tables,", "please", "write", "a", "caption", "for", "each", "image", "or", "table", "based", "on", "the", "rel-", "ative", "texts", "provided.", "Paper", "Titling", "Based", "on", "the", "provided", "abstract", "or", "introduction", "of", "the", "research", "paper,", "please", "generate", "a", "concise", "and", "infor-", "mative", "title", "Abstract", "Writing", "Read", "the", "full", "text", "of", "the", "paper", "and", "provide", "a", "concise", "summary", "in", "the", "form", "of", "an", "abstract.", "Tasks", "Questions"],
  "name": "1",
  "page": 4,
  "regionBoundary": {
    "x1": 57.839999999999996,
    "x2": 288.0,
    "y1": 247.92,
    "y2": 477.12
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table1-1.png"
}, {
  "caption": "Table 2. Key statistics of the Doc-750K datasets. It comprises 758K questions, 3.1M images, and 251K conversations, including 87K multi-turn and 164K single-turn questions. With an average of 11,245 text tokens and 6,178 image tokens, it highlights the dataset’s richness and diversity for multimodal research.",
  "captionBoundary": {
    "x1": 317.25,
    "x2": 553.5045776367188,
    "y1": 369.3867492675781,
    "y2": 418.62493896484375
  },
  "figType": "Table",
  "imageText": ["Average", "Text", "Tokens", "11245", "Average", "Image", "Tokens", "6178", "Multi-Turn", "Questions", "87K", "Single-Turn", "Questions", "164K", "Total", "Questions", "758K", "Total", "Images", "3.1M", "Total", "Conversations", "251K", "Statistics", "Number"],
  "name": "2",
  "page": 4,
  "regionBoundary": {
    "x1": 366.96,
    "x2": 504.0,
    "y1": 247.92,
    "y2": 357.12
  },
  "renderDpi": 300,
  "renderURL": "sample_data/Docopilot/parse_result/figDocopilot-Table2-1.png"
}]