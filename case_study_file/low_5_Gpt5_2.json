[
    {
        "question": "Outline the two-stage training of M-DocSum-7B: summarize the goals and settings of Stage 1 (instruction tuning) and the construction, filtering, objective, and training settings of Stage 2 (DPO).",
        "evidence": "For each image, we set the maximum pixels equal to the minimum pixels.",
        "score": 0.2209385633468628
    },
    {
        "question": "Outline the two-stage training of M-DocSum-7B: summarize the goals and settings of Stage 1 (instruction tuning) and the construction, filtering, objective, and training settings of Stage 2 (DPO).",
        "evidence": "A preference data pair is considered standard if it satisfies the following formula: \\begin{equation} Δ TS > 0  and  Δ IS > 0  and  Δ TS + Δ IS > δ, \\end{equation} where the $Δ TS$ and $Δ IS$ represent the differences in TS and IS between $y$ and $y$, and $δ$ is an adjustable threshold that controls the margin.",
        "score": 0.26071202754974365
    },
    {
        "question": "What datasets and evaluation metrics were used for ChuLo, and how were zero-shot prompts applied to large language models during experiments?",
        "evidence": "1) HP (Hyperpartisan News Detection) evaluates the classification of news articles based on hyperpartisan argumentation <cit.>. We use the ‘byarticle’ version, which contains 238 hyperpartisan and 407 non-hyperpartisan articles. The same train-test split as in <cit.> is adopted.",
        "score": 0.30517005920410156
    },
    {
        "question": "Outline the two-stage training of M-DocSum-7B: summarize the goals and settings of Stage 1 (instruction tuning) and the construction, filtering, objective, and training settings of Stage 2 (DPO).",
        "evidence": "The modifications include: (1) Shuffling Image Order: Altering the order of images without providing image indices. (2) Adding Constraints: Requiring each paragraph to include an image reference, disallowing no selection. (3) Reducing Information: Providing only the abstract and introduction paragraphs of the original article.",
        "score": 0.32534879446029663
    },
    {
        "question": "What datasets and evaluation metrics were used for ChuLo, and how were zero-shot prompts applied to large language models during experiments?",
        "evidence": "3) Eurlex57k consists of 47,000 English EU legislative documents with 4,271 EUROVOC concepts. We also include a simulated Inverted-Eurlex57k version, where the header and recitals are moved to the end, compelling the model to read the entire document for key information.",
        "score": 0.3470931649208069
    }
]