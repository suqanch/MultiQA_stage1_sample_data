[
    {
        "question": "How does the M-DocSum-Bench benchmark address the limitations of previous document understanding benchmarks in evaluating models' ability to comprehend and summarize interleaved image-text in long scientific documents?",
        "evidence": "Through these efforts, M-DocSum-Bench offers two key advantages over previous work: (1) Clarity and intuitiveness: The reference-based multimodal generation output format intuitively reflects the model's overall understanding of interleaved image-text information. ... (2) Comprehensive coverage: The summary encompasses all critical key points from the beginning to the end of the document, including both images and text. This rigorously evaluates the comprehensive capabilities of the models such as understanding, reasoning, locating and summarization, enabling in-depth analysis of its performance in handling long-range dependencies, multimodal integration, and global coherence.",
        "score": 0.8265659213066101
    },
    {
        "question": "How does the M-DocSum-Bench benchmark address the limitations of previous document understanding benchmarks in evaluating models' ability to comprehend and summarize interleaved image-text in long scientific documents?",
        "evidence": "To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark (M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents.",
        "score": 0.800190269947052
    },
    {
        "question": "How does ChuLo’s chunk-based representation perform in token-level tasks such as Named Entity Recognition (NER) for long documents, especially compared to state-of-the-art Transformer baselines and Large Language Models? Summarize key findings with reference to experimental results and visual analyses.",
        "evidence": "To further demonstrate the effectiveness of our chunk representation method, we evaluated it on a token-level classification task—specifically, Named Entity Recognition (NER) using long documents. We compared our model against two state-of-the-art long-document pre-trained models, Longformer <cit.> and BigBird <cit.>, as well as newly released large language models, GPT-4o and Gemini 1.5 Pro. As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models. To leverage the broader context captured by our chunk representation, we integrate a BERT-decoder module that utilizes the enhanced chunk embeddings to predict token labels more accurately. This configuration allows the model to maintain a global understanding of the document while focusing on the local dependencies necessary for precise token classification.",
        "score": 0.7974730730056763
    },
    {
        "question": "How does the M-DocSum-Bench benchmark address the limitations of previous document understanding benchmarks in evaluating models' ability to comprehend and summarize interleaved image-text in long scientific documents?",
        "evidence": "Most benchmarks have been limited to single-page documents... Our proposed DocSum output format addresses this by reflecting the model's overall understanding of interleaved image-text information and comprehensively covers all critical information.",
        "score": 0.7891762256622314
    },
    {
        "question": "What are the main processes and design considerations in generating the Doc-750K dataset for document-level multimodal understanding, including data extraction formats, QA pair construction, task coverage, and quality controls?",
        "evidence": "We provide additional comparisons of Doc-750K with various document-level datasets in terms of image types and task coverage, as shown in Table [Ref id=\"tab:addi_dataset_comparison\"]. Compared to these datasets, Doc-750K exhibits greater diversity in proxy tasks and ranks among the largest datasets in terms of QA pair count.",
        "score": 0.7836042642593384
    }
]