[
    {
        "question": "How does ChuLo's chunk representation method address the limitations of existing approaches like truncation and sparse attention in handling long documents, and what evidence supports its effectiveness?",
        "evidence": "Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies.",
        "score": 0.4516259431838989
    },
    {
        "question": "How does ChuLo's chunk representation method address the limitations of existing approaches like truncation and sparse attention in handling long documents, and what evidence supports its effectiveness?",
        "evidence": "We propose ChuLo, a novel chunk representation method that enhances long document understanding by reducing input length while preserving semantic content. Unlike existing approaches like truncation and standard chunking, ChuLo minimizes information loss and maintains contextual dependencies.",
        "score": 0.8228877782821655
    },
    {
        "question": "How does ChuLo's chunk representation method address the limitations of existing approaches like truncation and sparse attention in handling long documents, and what evidence supports its effectiveness?",
        "evidence": "Our approach mitigates these issues by segmenting the document into non-overlapping chunks before feeding them into the model. It enables complete self-attention among chunks, ensuring that all information is retained and enabling the model to process larger portions of the document context.",
        "score": 0.6267322301864624
    },
    {
        "question": "How does ChuLo's chunk representation method address the limitations of existing approaches like truncation and sparse attention in handling long documents, and what evidence supports its effectiveness?",
        "evidence": "Overall, the results confirm that ChuLo consistently outperforms standard PLM baselines and existing chunking methods in long document classification tasks. Its ability to retain and emphasize key semantic content, while efficiently handling long inputs, makes it a robust solution for various document classification challenges.",
        "score": 0.7055131196975708
    },
    {
        "question": "What are the key components of ChuLo's framework, and how do they contribute to its performance in both document-level and token-level tasks?",
        "evidence": "The fundamental reason for extracting keyphrases from the chunks, as defined in the document chunking step, is to maintain the integrity of the document's semantic content while reducing input length. By identifying and highlighting critical phrases within these chunks, ChuLo ensures that the most relevant information is preserved and emphasized.",
        "score": 0.501162588596344
    },
    {
        "question": "What are the key components of ChuLo's framework, and how do they contribute to its performance in both document-level and token-level tasks?",
        "evidence": "After extracting the semantically significant keyphrases, we construct a chunk representation that preserves and highlights this key information, ensuring that the chunk retains the core semantic content of the document. By assigning a higher weight to keyphrase tokens, we ensure that the resulting chunk representation emphasizes the most critical information while maintaining a compact input length.",
        "score": 0.4419632852077484
    },
    {
        "question": "What are the key components of ChuLo's framework, and how do they contribute to its performance in both document-level and token-level tasks?",
        "evidence": "In this final step, we train a Transformer-based model using our keyphrase-enhanced chunk representations to effectively incorporate the core semantic content of the document. By emphasizing key information in the chunk embeddings, we ensure that the model can focus on the most relevant aspects of the text.",
        "score": 0.4716044068336487
    },
    {
        "question": "What are the key components of ChuLo's framework, and how do they contribute to its performance in both document-level and token-level tasks?",
        "evidence": "We introduced ChuLo, a novel chunk representation method that enhances the performance of Transformer-based models on long document classification and token-level classification tasks. By utilizing unsupervised keyphrase extraction, ChuLo effectively reduces input length while preserving critical information, addressing the limitations of truncation and sparse attention.",
        "score": 0.6447268128395081
    },
    {
        "question": "How does ChuLo's performance compare to other models like Longformer and BigBird on Named Entity Recognition (NER) tasks with long documents, and what explains its superior performance?",
        "evidence": "As shown in Table [Ref id=\"tab:overal_performance_token_cls_comparison\"], our model consistently outperforms Longformer, BigBird and LLM models on the NER tasks, particularly on the CoNLL, where document lengths often exceed the input limitations of these baseline models.",
        "score": 0.7578468322753906
    },
    {
        "question": "How does ChuLo's performance compare to other models like Longformer and BigBird on Named Entity Recognition (NER) tasks with long documents, and what explains its superior performance?",
        "evidence": "For the CoNLL, as document lengths exceed the maximum input capacities of Longformer and BigBird, both models exhibit significant performance drops to 31.56% and 29.78%, respectively. In contrast, our model maintains high performance across all length intervals, as visualized in Figures [Ref id=\"fig:conll_results\"] and [Ref id=\"fig:gum_results\"].",
        "score": 0.6839559078216553
    },
    {
        "question": "How does ChuLo's performance compare to other models like Longformer and BigBird on Named Entity Recognition (NER) tasks with long documents, and what explains its superior performance?",
        "evidence": "This discrepancy suggests that the uneven distribution of document lengths in their pretraining corpora may lead to inconsistent performance on longer sequences. In contrast, our model’s ability to compress the entire document into 512-length chunks for the decoder enables it to leverage complete contextual information, resulting in better stability and accuracy even on longer documents.",
        "score": 0.5568066835403442
    },
    {
        "question": "How does ChuLo's performance compare to other models like Longformer and BigBird on Named Entity Recognition (NER) tasks with long documents, and what explains its superior performance?",
        "evidence": "Table [Ref id=\"tab:Accuracy length intervals\"] shows that LLMs outperform Longformer in the document classification task with zero-shot prompt tuning. However, their performance drops significantly in the NER task in Table [Ref id=\"tab:Conll Microf1 length intervals\"] and Table [Ref id=\"tab:Gum Microf1 length intervals\"].",
        "score": 0.6397486925125122
    },
    {
        "question": "What are the key challenges faced by current multimodal large language models (MLLMs) in document-level understanding, and how does Docopilot address these challenges?",
        "evidence": "Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets.",
        "score": 0.6250016093254089
    },
    {
        "question": "What are the key challenges faced by current multimodal large language models (MLLMs) in document-level understanding, and how does Docopilot address these challenges?",
        "evidence": "Retrieval-augmented generation (RAG) methods attempt to address this by retrieving key information to fit within the limited context windows of MLLMs, but they still encounter the following challenges in document-level tasks. (1) Fragmented Retrieval Contexts. Retrieved information is fragmented, lacking the overall structure of the document; (2) Multi-Stage Error Accumulation. Incorrect retrieval results can affect subsequent responses, leading to errors or omissions of critical details, especially in multi-turn or complex tasks; (3) Extra Time Costs. The retrieval step increases the latency of the QA system, limiting the scalability of RAG in time-sensitive scenarios.",
        "score": 0.5446193814277649
    },
    {
        "question": "What are the key challenges faced by current multimodal large language models (MLLMs) in document-level understanding, and how does Docopilot address these challenges?",
        "evidence": "Building upon this dataset, we developed a native baseline model for document-level multimodal understanding–Docopilot. Unlike existing approaches that rely on RAG, our model achieves efficient document-level training and testing through simple engineering optimizations, such as multimodal data packing, Ring Attention, and Liger Kernel.",
        "score": 0.7723426818847656
    },
    {
        "question": "What are the key challenges faced by current multimodal large language models (MLLMs) in document-level understanding, and how does Docopilot address these challenges?",
        "evidence": "Results. As illustrated in Table [Ref id=\"tab:multi_page_qa\"], our model achieves consistent improvements on multi-page QA benchmarks, outperforming previous document-level MLLMs.",
        "score": 0.6330100297927856
    },
    {
        "question": "How was the Doc-750K dataset constructed, and what are its key features that make it suitable for training document-level multimodal models?",
        "evidence": "The data engine operates primarily through two steps: document content extraction and question-answer (QA) pair construction.",
        "score": 0.3918132185935974
    },
    {
        "question": "How was the Doc-750K dataset constructed, and what are its key features that make it suitable for training document-level multimodal models?",
        "evidence": "In this work, we process each document into two formats as follows: (1) Interleaved Text-Image Format. Using the document content extractor MinerU, we segment the document content into interleaved text and image annotations; (2) Multi-Image Format. In this format, a document with $n$ pages is rendered as $n$ images, with each image corresponding to a single page.",
        "score": 0.5419381260871887
    },
    {
        "question": "How was the Doc-750K dataset constructed, and what are its key features that make it suitable for training document-level multimodal models?",
        "evidence": "Our dataset ultimately consists of 251K conversations, comprising a total of 758K questions. Additional statistical details are provided in Table [Ref id=\"tab:dataset_analysis\"]. Compared to previous datasets, {Doc-750K} contains a larger number of images, with an average of four images per conversation segment.",
        "score": 0.5317138433456421
    },
    {
        "question": "How was the Doc-750K dataset constructed, and what are its key features that make it suitable for training document-level multimodal models?",
        "evidence": "This work introduced a diverse document-level question-answering dataset that covers complex structures and cross-page dependencies, providing a robust foundation for training and evaluating document understanding models.",
        "score": 0.6265621185302734
    },
    {
        "question": "What are the performance gains of Docopilot over existing models, particularly in multi-page and interleaved long-context QA tasks?",
        "evidence": "Notably, our Docopilot-8B surpasses Gemini-1.5-Pro on MMLongBench-Doc, positioning it as the closest open-source model to GPT-4o.",
        "score": 0.6946597695350647
    },
    {
        "question": "What are the performance gains of Docopilot over existing models, particularly in multi-page and interleaved long-context QA tasks?",
        "evidence": "For example, in the Multi-Page QA of DocGenome benchmark, the RAG method shows a performance decline due to the disruption of document continuity while our Docopilot exhibits a significantly stable improvement compared to the baseline, with Docopilot-8B showing an increase of 12.6.",
        "score": 0.7241858243942261
    },
    {
        "question": "What are the performance gains of Docopilot over existing models, particularly in multi-page and interleaved long-context QA tasks?",
        "evidence": "The right side of Table [Ref id=\"tab:multi_page_qa\"] presents the results of MM-NIAH across context lengths ranging from 1K to 64K. Our Docopilot demonstrates exceptional performance in both medium- and long-context scenarios, while maintaining high accuracy in short-context situations.",
        "score": 0.7341892123222351
    },
    {
        "question": "What are the performance gains of Docopilot over existing models, particularly in multi-page and interleaved long-context QA tasks?",
        "evidence": "Across the three benchmarks, Docopilot-2B and InternVL2-2B exhibit comparable results, while Docopilot-8B outperforms InternVL2-8B by 0.4 points in DocVQA.",
        "score": 0.7011242508888245
    },
    {
        "question": "What limitations does the Doc-750K dataset have, and how do these limitations affect the generalizability of models trained on it?",
        "evidence": "A key limitation of Doc-750K is its current domain restriction to academic documents.",
        "score": 0.6572431921958923
    },
    {
        "question": "What limitations does the Doc-750K dataset have, and how do these limitations affect the generalizability of models trained on it?",
        "evidence": "We plan to expand the dataset’s coverage to a broader range of document types and enhance the generalizability of proxy tasks, ensuring wider applicability across diverse domains.",
        "score": 0.5015599727630615
    },
    {
        "question": "What limitations does the Doc-750K dataset have, and how do these limitations affect the generalizability of models trained on it?",
        "evidence": "Future work will focus on improving computational efficiency, extending the model to larger multimodal tasks, and adapting it to broader applications for enhanced practicality and generalization.",
        "score": 0.4253237545490265
    },
    {
        "question": "What are the key components of the M-DocSum-Bench dataset, and how does it address gaps in existing benchmarks for evaluating interleaved image-text summarization?",
        "evidence": "Existing benchmarks primarily feature text-only outputs and focus on Visual Question-Answering (VQA) tasks, limiting their applicability in complex scenarios and failing to sufficiently assess the capability of interleaved image-text processing.",
        "score": 0.6034257411956787
    },
    {
        "question": "What are the key components of the M-DocSum-Bench dataset, and how does it address gaps in existing benchmarks for evaluating interleaved image-text summarization?",
        "evidence": "To investigate this critical issue and bridge this gap, we introduce a novel and challenging Multimodal Document Summarization Benchmark (M-DocSum-Bench), a reference-based generation task, which requires generating interleaved image-text summaries with reference-based images directly from long documents.",
        "score": 0.8010984063148499
    },
    {
        "question": "What are the key components of the M-DocSum-Bench dataset, and how does it address gaps in existing benchmarks for evaluating interleaved image-text summarization?",
        "evidence": "The input comprises two modalities, first, the textual information of the scientific article is parsed into complex markdown format, preserving not only the main body of the text but also critical elements like tables and image captions. Second, the accompanying images within the article are parsed into image format, retaining their original visual information.",
        "score": 0.554189920425415
    },
    {
        "question": "What are the key components of the M-DocSum-Bench dataset, and how does it address gaps in existing benchmarks for evaluating interleaved image-text summarization?",
        "evidence": "To support our evaluation benchmark, we process high-quality multimodal documents from the publicly accessible arXiv website, primarily focusing on six domains: Artificial Intelligence, and AI-related fields such as Engineering, Computer Science, Physics, Mathematics, and Medicine.",
        "score": 0.4671931862831116
    },
    {
        "question": "What are the key components of the M-DocSum-Bench dataset, and how does it address gaps in existing benchmarks for evaluating interleaved image-text summarization?",
        "evidence": "For image referencing, we provide the extracted key points, original images, and image captions to a powerful LVLM. The model first determines if an image is necessary to aid understanding of the summary paragraph.",
        "score": 0.5304186344146729
    },
    {
        "question": "How does the M-DocEval evaluation method assess the performance of models in generating interleaved summaries, and what metrics are used?",
        "evidence": "To provide a comprehensive and objective assessment of the performance in generating interleaved summaries, we design a multifaceted set of evaluation metrics. These metrics cover textual quality, image referencing appropriateness, and instruction-following capability, ensuring a holistic evaluation of the model abilities.",
        "score": 0.8017677068710327
    },
    {
        "question": "How does the M-DocEval evaluation method assess the performance of models in generating interleaved summaries, and what metrics are used?",
        "evidence": "Textual Content Evaluation. The text summary evaluation focuses on two primary aspects: (1) Completeness (Com) measures the proportion of essential information captured from the original text using a benchmark set of key points, generated and verified by human experts. (2) Accuracy (Acc) assesses the correctness of the summary by comparing each sentence to the original content.",
        "score": 0.5994701385498047
    },
    {
        "question": "How does the M-DocEval evaluation method assess the performance of models in generating interleaved summaries, and what metrics are used?",
        "evidence": "Visual Reference Evaluation. For image referencing, we evaluate the model's ability to determine image necessity and select appropriate images correctly. (1) None Accuracy (NonAcc) assesses the correct identification of paragraphs needing no image. (2) Image Accuracy (ImgAcc) measures precise image matching for paragraphs requiring images.",
        "score": 0.48765426874160767
    },
    {
        "question": "How does the M-DocEval evaluation method assess the performance of models in generating interleaved summaries, and what metrics are used?",
        "evidence": "Instruction Following Capability. Given that open-source multimodal models may not always generate content that fully adheres to the given instructions, we introduce an Instruction Following Capability (IF) metric.",
        "score": 0.5068981647491455
    },
    {
        "question": "How does the M-DocEval evaluation method assess the performance of models in generating interleaved summaries, and what metrics are used?",
        "evidence": "To obtain a final, comprehensive evaluation of each sample, we integrate the TS, IS, and IF using a weighted average: Total = α * IF + β * TS + γ * IS, where α, β, γ are 0.1, 0.45, 0.45, respectively.",
        "score": 0.48193132877349854
    },
    {
        "question": "What are the stages involved in training the M-DocSum-7B model, and how do they contribute to its performance?",
        "evidence": "After establishing the InterSum-Bench, as shown in Figure [Ref id=\"fig-main\"](b), we initialize M-DocSum-7B with the weights from Qwen2-VL and adopt a two-stage training strategy.",
        "score": 0.8014642596244812
    },
    {
        "question": "What are the stages involved in training the M-DocSum-7B model, and how do they contribute to its performance?",
        "evidence": "During this stage, our primary goal is to enhance the accuracy and comprehensiveness of the model in extracting key points for summary generation, while ensuring that the referenced images are appropriate and well-structured.",
        "score": 0.4411126375198364
    },
    {
        "question": "What are the stages involved in training the M-DocSum-7B model, and how do they contribute to its performance?",
        "evidence": "In this stage, we introduce a novel method for constructing preference data. By conducting a second-stage DPO training on this data, we can further enhance the performance and robustness of the model.",
        "score": 0.5190746784210205
    },
    {
        "question": "What are the stages involved in training the M-DocSum-7B model, and how do they contribute to its performance?",
        "evidence": "As shown in Table [Ref id=\"tab:main\"], M-DocSum-7B outperforms all open-source models in both text and image overall evaluations. Notably, in image referencing, M-DocSum-7B even surpasses advanced closed-source models, with its Image Score exceeding Gemini-Pro by 24.",
        "score": 0.6859607696533203
    },
    {
        "question": "What are the stages involved in training the M-DocSum-7B model, and how do they contribute to its performance?",
        "evidence": "Finally, a strong summarization baseline, M-DocSum-7B, is developed using a progressive two-stage training approach (instruction-tuning and DPO). This model achieves state-of-the-art performance among various models, demonstrating the potential of LVLMs for enhanced interleaved image-text understanding.",
        "score": 0.5687879323959351
    }
]